{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The purpose of this script is to deploy the models to Gradio. The two models used are the best deep learning model found from the Grid Search CV and the best Gradient Boosting model from the Randomized Search CV."
      ],
      "metadata": {
        "id": "7CydGbMubSH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC5NjITr54eR",
        "outputId": "ff7379ed-61d2-4b71-bc4c-bf55b3f5da53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpTAl3ED5Ecj",
        "outputId": "4bb6ad68-2185-470c-8168-2de7896c7b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [22:57:44] WARNING: /workspace/src/gbm/gbtree.cc:388: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [22:57:44] WARNING: /workspace/src/context.cc:43: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [22:57:44] WARNING: /workspace/src/context.cc:196: XGBoost is not compiled with CUDA support.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "# load in the models\n",
        "from joblib import load\n",
        "\n",
        "DL_model = load(\"/content/drive/MyDrive/ML_FinalProject/deep_learning_model.pkl\")\n",
        "\n",
        "xgb_model = load(\"/content/drive/MyDrive/ML_FinalProject/best_xgb_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "MvS5AReOPYk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "# Define a mapping for one-hot encoding nucleotides (using same mapping scheme as before)\n",
        "NUCLEOTIDE_MAP = {\n",
        "    \"A\": [1, 0, 0, 0],\n",
        "    \"T\": [0, 1, 0, 0],\n",
        "    \"G\": [0, 0, 1, 0],\n",
        "    \"C\": [0, 0, 0, 1]\n",
        "}\n",
        "\n",
        "def one_hot_encode_sequence(seq, target_length=1000):\n",
        "    \"\"\"\n",
        "    One-hot encode a DNA sequence and ensure it matches the target length.\n",
        "    - seq: String containing the nucleotide sequence (e.g., \"AATTCG\").\n",
        "    - target_length: Length to pad or truncate the sequence to (default: 1000).\n",
        "    Returns:\n",
        "        - A NumPy array of shape (target_length, 4) representing the one-hot encoding.\n",
        "    \"\"\"\n",
        "    one_hot_encoded = np.array([NUCLEOTIDE_MAP.get(nucleotide, [0, 0, 0, 0]) for nucleotide in seq])\n",
        "\n",
        "    # Pad or truncate the sequence\n",
        "    if len(one_hot_encoded) < target_length:\n",
        "        # Pad with zeros to reach target length\n",
        "        padding = np.zeros((target_length - len(one_hot_encoded), 4))\n",
        "        one_hot_encoded = np.vstack([one_hot_encoded, padding])\n",
        "    elif len(one_hot_encoded) > target_length:\n",
        "        # Truncate to the target length\n",
        "        one_hot_encoded = one_hot_encoded[:target_length]\n",
        "\n",
        "    return one_hot_encoded\n",
        "\n",
        "# One-hot encode tissue input\n",
        "def one_hot_encode_tissue(tissue, num_classes=4):\n",
        "    tissue_id = int(tissue)\n",
        "    if tissue_id >= num_classes:\n",
        "        raise ValueError(f\"Tissue ID {tissue_id} is out of range for {num_classes} classes.\")\n",
        "    one_hot_encoded = np.zeros((1, num_classes))\n",
        "    one_hot_encoded[0, tissue_id] = 1\n",
        "    return one_hot_encoded\n",
        "\n",
        "def preprocess_inputs(seq, tissue):\n",
        "    \"\"\"\n",
        "    Preprocess inputs for both models.\n",
        "    - seq: Sequence input as a regular string\n",
        "    - tissue: Tissue input as an integer string\n",
        "    \"\"\"\n",
        "    seq_array = one_hot_encode_sequence(seq).reshape(1, -1, 4)  # Reshape sequence to (1, length, 4)\n",
        "    tissue_array = one_hot_encode_tissue(tissue)  # One-hot encode tissue ID to (0, 3)\n",
        "    return seq_array, tissue_array\n",
        "\n",
        "# Prediction functions\n",
        "def predict_with_deep_model(seq, tissue):\n",
        "    \"\"\"\n",
        "    Predict using the deep learning model.\n",
        "    - seq: Sequence input as a regular string.\n",
        "    - tissue: Tissue input as an integer string.\n",
        "    \"\"\"\n",
        "    seq_array, tissue_array = preprocess_inputs(seq, tissue)\n",
        "    prediction = DL_model.predict([seq_array, tissue_array])\n",
        "    return float(prediction[0][0])\n",
        "\n",
        "def predict_with_xgb(seq, tissue):\n",
        "    \"\"\"\n",
        "    Predict using the XGB model.\n",
        "    - seq: Sequence input as a regular string.\n",
        "    - tissue: Tissue input as an integer string.\n",
        "    \"\"\"\n",
        "    seq_array, tissue_array = preprocess_inputs(seq, tissue)\n",
        "    combined_features = np.hstack([seq_array.flatten(), tissue_array.flatten()])\n",
        "    prediction = xgb_model.predict([combined_features])\n",
        "\n",
        "    return prediction[0]\n",
        "\n",
        "# Gradio interface\n",
        "def gradio_interface(seq, tissue):\n",
        "    \"\"\"\n",
        "    Combine predictions from both models and display results.\n",
        "    - seq: Regular DNA sequence string.\n",
        "    - tissue: Integer tissue identifier.\n",
        "    \"\"\"\n",
        "    deep_pred = predict_with_deep_model(seq, tissue)\n",
        "    xgb_pred = predict_with_xgb(seq, tissue)\n",
        "    return {\n",
        "        \"Deep Learning Model Prediction\": deep_pred,\n",
        "        \"XGBoost Model Prediction\": xgb_pred,\n",
        "    }\n",
        "\n",
        "# Define Gradio inputs\n",
        "seq_input = gr.Textbox(label=\"Sequence Input (Regular DNA sequence)\")\n",
        "tissue_input = gr.Textbox(label=\"Tissue Input (Integer 0-3)\")\n",
        "\n",
        "# Define Gradio output\n",
        "output = gr.JSON(label=\"Model Predictions\")\n",
        "\n",
        "# Launch Gradio app\n",
        "gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[seq_input, tissue_input],\n",
        "    outputs=output,\n",
        "    title=\"Gene Expression Prediction\",\n",
        "    description=\"Enter a DNA sequence and a tissue identifier to get predictions from both the Deep Learning model and the Gradient Boosting model.\"\n",
        ").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "M0JwQ3oc7tfn",
        "outputId": "ff6af575-7168-4d30-f665-8f30ac28d6c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://54212779f63c139cda.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://54212779f63c139cda.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7864 <> https://54212779f63c139cda.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}