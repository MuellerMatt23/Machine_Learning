{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"10Dz7SzfRiahfaCmnAv0LNSruXt3ZR5D4","authorship_tag":"ABX9TyOVzHF8N0RHes5LnxgQkrh/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Exercise 5: Explore text generation pipeline using RNN"],"metadata":{"id":"NHRa2IYgHH4k"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"l2PZfjDv8ABn","executionInfo":{"status":"ok","timestamp":1743102990960,"user_tz":300,"elapsed":597,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"outputs":[],"source":["# load ascii text and covert to lowercase\n","filename = \"/content/drive/MyDrive/CSCI5930/Exercise_05/ML_reading_comments.csv\"\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()"]},{"cell_type":"code","source":["def step2_clean_text(raw_text):\n","   import re\n","   import string\n","   import nltk\n","   from nltk.stem import WordNetLemmatizer\n","\n","   # (1) Initialize the lemmatizer\n","   nltk.download('wordnet')\n","   lemmatizer = WordNetLemmatizer()\n","\n","   # (2) Define a list of stopwords (single letters)\n","   stopwords = set(['b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n","\n","   # (3) Define a regular expression pattern to match unwanted characters (e.g., digits and special symbols)\n","   unwanted_chars_pattern = re.compile(r'[^a-zA-Z\\s.,!?]')\n","\n","   data_text = []\n","   lines = raw_text.strip().split('\\n')\n","\n","   # prepare regex for char filtering\n","   re_print = re.compile('[^%s]' % re.escape(string.printable))\n","\n","   for comment in lines:\n","       # Replace unwanted characters with a space\n","       comment = re.sub(unwanted_chars_pattern, ' ', comment)\n","\n","       # tokenize on white space\n","       comment = comment.split()\n","\n","       ## convert comment to lowercase\n","       comment = [word.lower() for word in comment]\n","\n","       # remove non-printable chars form each token\n","       comment = [re_print.sub('', w) for w in comment]\n","\n","       ### lemmatize word\n","       comment = [lemmatizer.lemmatize(word) for word in comment]\n","\n","       # remove tokens with numbers in them\n","       comment = [word for word in comment if word not in stopwords ]\n","\n","       if len(comment) < 200: # filter out short comment\n","           continue\n","\n","       # store as string\n","       comment = ' '.join(comment)\n","\n","       # Replace multiple spaces with a single space\n","       comment = re.sub(r'\\s+', ' ', comment)\n","\n","       data_text.append(comment)\n","\n","   cleaned_text = '\\n'.join(data_text)\n","   return cleaned_text"],"metadata":{"id":"N_eFpnDm95v9","executionInfo":{"status":"ok","timestamp":1743102990968,"user_tz":300,"elapsed":26,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["cleaned_text = step2_clean_text(raw_text)\n","cleaned_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"id":"tie--in29-SQ","executionInfo":{"status":"ok","timestamp":1743103008125,"user_tz":300,"elapsed":17139,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"791cac82-1380-46c3-c40d-843f1a512ff6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["'after going through the abstract of the paper, i found that the paper brings up some key issue like how ml model are treated a black box and restrict their use in meteorology and this paper us some of ml model like linear regression, logistic regression, decision trees, na ve bayes, gradient boosted decision tree and svm. this proposed paper us meteorological example to show how these model work and also the process and best approach to apply these model in different datasets and domain. most of the ml model opaque nature prevents meeting one of the three requirement like consistency prior to user knowledge, accuracy, and benefit. most of the paper model satisfy the last two condition but fail to satisfy the condition and this make the model lose trustworthiness among the users. this section majorly covered about what is ml model and how many type these are sub categorized like supervised and unsupervised and showcased how these differ and work differently. then it presented the workflow of how ml model training go and also explained about type of supervised like classification and regression model and on what type of target data and type of prediction we can apply these on different datasets. the paper aim to work on the thunderstorm and this us sevir dataset and this paper us ml model like classification and regression are used to find if there is a thunderstorm in the image and estimate lightning is present in the image and how many lightning flash are present in the image. every pixel in image can be considered a predictor but feature all the pixel of a single image is impractical and they used a set of statistic to shorten the feature pixel based on certain factor for each image and to create label the no of lightning flash are summed and for task if it contains at least one lightning flash in five minute it is classified a thunderstorm and for task the sum of lightning flash are used for the regression target the paper described the importance of training, validation and test data in a better way like the training data of original data is used to train the ml model and the validation data of original data is used to validate data after hyper parameter tuning and at last the test data is used to evaluate the model with the unseen data and check performance. they used time division to split the data a the adjacent data are correlated and to avoid biased output. the main question arises like how much data is needed to train the ml model and also removal of spurious data is important because it may make model overfit, underfit, show bias to some feature and may not work a intended too. so prior sending data to model it must be preprocessed and cleaned to avoid bias in model decision like missing value filling removal, duplicate data removal, removal of unnecessary feature and a lot to remove the spurious data the author used accuracy the standard metric to evaluate model , which indicate a good model and other metric used is common area under auc and this is associated with roc curve and this is calculated with pofd ad pod based on contingency table tp,fp,tn,fn pofd fp tp fp pod tp tp fn their all model showed auc nearly . which indicate best performance. the other metric used is performance diagram and this time pofd go on axis and sr go on axis. sr tp tp fp and at last the calculating csi without considering true negatives. by using these three metric they analyzed the result like the all model have almost same accuracy and auc and the differ in performance diagram , all the tree based method have . frequency bias and other model like svm, na ve bayes, logistic regression have frequency bias and having same csi a tree model these model are found to be best performing. to improve ml explainibility like how these model are making prediction the author used to method to check on what feature these model are making decision and by using permutation fold and multi fold the gained some insight and then by using accumulated local effect ale to see if ml model are learned to see plausible trend within input features. a the number of image with no lightning are of dataset , to increase performance they used the classification model output and then apply the regression to identify the number of lightning in the image. by observing the one to one plot, linear regression wa not able to perform well and it overpredicted and unpredicted some flash in the image and also linear model like svm and linear regression showed bias of and flash and tree based model showed a better performance. after reading the summary and future work of this paper, we can learn how to apply ml model on different datasets and work with them like how to collect data, engineering data, selecting features, splitting data and evaluate models. the paper majorly help in grasping all the above information a this paper used servir dataset to apply ml model like classification and regression on meteorology to find if a image ha a thunderstorm and how many lightning storm are present in a image. this paper helped me a lot in grasping new information like how to split data into training, validation and testing data and their importance in training, evaluating and testing the model and also how we can use time to split data if adjacent feature are depandant and avoid random splitting to prevent bias in predictions. the next part is how we can use different performance metric to check the best performing model and at last is how to apply different ml model on particular task and make predictions.\\nthe ml in meteorology is the main topic of the paper. the pupil curriculum did not cover it, though, because they had to first study about ml. the report also briefly summarizes some of what people opinion were on blackbox. the major objective is to make machine learning method in meteorology less obscure. this study aim to reduce the resistance of meteorologist to using machine learning technology in their work.the biggest problem wa that because the ml is not well known by all people, it is difficult to get their trust. due to it opacity, it doesn meet all the requirement that could prevent people from believing in ml models.in the lecture, we talked about topic like classification and regression, which show similarity with the article that demonstrates their distinctions. additionally, it talk about predetermined feature that are covered in class in relation to various input and outputs.the paper discusses the use of machine learning technique to solve two specific issue related to thunderstorm identifying a thunderstorm in a photo and counting the number of lightning strikes. in order to make the technology more approachable and applicable to a wider range of weather sensing instrument and applications, the article appears to attempt to bridge the gap between machine learning and meteorology.the input characteristic are satellite and radar data, and the output is geostationary lightning data. the data contains image that describe the current metrological condition at a point. with the help of this input data, they can foresee or understand lightning flash glm data , which are massive energy burst that can be seen from space a lightning hits. the objective is to relate climatic variable input to the occurrence and pattern of lightning output in that region using machine learning. according to meteorological circumstances, this help in understanding and forecasting lightning strikes.the section discusses how to partition the data into test, train, and validate because failure to do so could lead to overfitting and even the use of hyperparameters in various situations.since we need to manage a large amount of data and even organize them into some sort of document that we can access in order to develop an ml model on reading the data, there are many problem when you take the data by yourself. there may be inconsistency that may have an impact on the model. we need to pay attention to such minute detail when using a csv file.in order to evaluate how well their machine learning algorithm predicted the weather, the author used a range of metrics. all of the model were found to be fairly accurate and able to distinguish between various weather conditions. however, certain model were more effective when dealing with uneven data. finally, they found model for their meteorological prediction that were both balanced and precise.by using technique like permutation significance, which determines which input quality are most important, and ale accumulated local effect , which show how change in input affect model predictions, the author solves the issue of black box machine learning models. they begin with simple model to see if more complicated one can pick up on realistic link and then they look into tree model decision making processes. the most important input are determined via importance testing, and the learned relationship are revealed by examining how prediction change with inputs. it ensures that the model make sense and give a clearer understanding of what the black box model have learned by comparing the model output to accepted scientific knowledge.in paragraph .a, the author us linear regression to solve a meteorological issue. he start by selecting key characteristic like temperature and humidity. to efficiently train and assess the model, they separated the data into training, validation, and testing sets. these feature are used to train the linear regression model, which is then adjusted using the validation set. they then assess the model performance using metric like mse and conduct coefficient analysis to determine the impact of each feature. this approach is contrasted with more intricate model that predict lightning flash by utilizing all feature and a variety of evaluation techniques.this paper provides simple explanation of complex concept to help newcomer to machine learning. it illustrates how these idea are applied realistically by using instance from the actual world, such lightning detection. the handbook simplifies the learning process by providing a step by step methodology from data selection through evaluation. it is a useful resource for anyone unfamiliar with machine learning because it also includes reference for further research.i learned new abilities, particularly in classification and regression, through evaluating machine learning models. i gained knowledge of auc, performance diagram, mae, and rmse for model evaluation. the importance of data purification before using training, validating, and testing data wa another discovery. i had never heard of the sevir dataset featuring storm photographs. the article also introduced regularization method for linear models, such a ridge and lasso regression, and suggested adjusting model setting hyper parameter . my comprehension of model evaluation and improvement wa boosted by these additions.\\nthe ml in meteorology is the main topic of the paper. the pupil curriculum did not cover it, though, because they had to first study about ml. the report also briefly summarizes some of what people opinion were on blackbox. the major objective is to make machine learning method in meteorology less obscure. this study aim to reduce the resistance of meteorologist to using machine learning technology in their work.the biggest problem wa that because the ml is not well known by all people, it is difficult to get their trust. due to it opacity, it doesn meet all the requirement that could prevent people from believing in ml models.in the lecture, we talked about topic like classification and regression, which show similarity with the article that demonstrates their distinctions. additionally, it talk about predetermined feature that are covered in class in relation to various input and outputs.the paper discusses the use of machine learning technique to solve two specific issue related to thunderstorm identifying a thunderstorm in a photo and counting the number of lightning strikes. in order to make the technology more approachable and applicable to a wider range of weather sensing instrument and applications, the article appears to attempt to bridge the gap between machine learning and meteorology.the input characteristic are satellite and radar data, and the output is geostationary lightning data. the data contains image that describe the current metrological condition at a point. with the help of this input data, they can foresee or understand lightning flash glm data , which are massive energy burst that can be seen from space a lightning hits. the objective is to relate climatic variable input to the occurrence and pattern of lightning output in that region using machine learning. according to meteorological circumstances, this help in understanding and forecasting lightning strikes.the section discusses how to partition the data into test, train, and validate because failure to do so could lead to overfitting and even the use of hyperparameters in various situations.since we need to manage a large amount of data and even organize them into some sort of document that we can access in order to develop an ml model on reading the data, there are many problem when you take the data by yourself. there may be inconsistency that may have an impact on the model. we need to pay attention to such minute detail when using a csv file.in order to evaluate how well their machine learning algorithm predicted the weather, the author used a range of metrics. all of the model were found to be fairly accurate and able to distinguish between various weather conditions. however, certain model were more effective when dealing with uneven data. finally, they found model for their meteorological prediction that were both balanced and precise.by using technique like permutation significance, which determines which input quality are most important, and ale accumulated local effect , which show how change in input affect model predictions, the author solves the issue of black box machine learning models. they begin with simple model to see if more complicated one can pick up on realistic link and then they look into tree model decision making processes. the most important input are determined via importance testing, and the learned relationship are revealed by examining how prediction change with inputs. it ensures that the model make sense and give a clearer understanding of what the black box model have learned by comparing the model output to accepted scientific knowledge.in paragraph .a, the author us linear regression to solve a meteorological issue. he start by selecting key characteristic like temperature and humidity. to efficiently train and assess the model, they separated the data into training, validation, and testing sets. these feature are used to train the linear regression model, which is then adjusted using the validation set. they then assess the model performance using metric like mse and conduct coefficient analysis to determine the impact of each feature. this approach is contrasted with more intricate model that predict lightning flash by utilizing all feature and a variety of evaluation techniques.this paper provides simple explanation of complex concept to help newcomer to machine learning. it illustrates how these idea are applied realistically by using instance from the actual world, such lightning detection. the handbook simplifies the learning process by providing a step by step methodology from data selection through evaluation. it is a useful resource for anyone unfamiliar with machine learning because it also includes reference for further research.i learned new abilities, particularly in classification and regression, through evaluating machine learning models. i gained knowledge of auc, performance diagram, mae, and rmse for model evaluation. the importance of data purification before using training, validating, and testing data wa another discovery. i had never heard of the sevir dataset featuring storm photographs. the article also introduced regularization method for linear models, such a ridge and lasso regression, and suggested adjusting model setting hyper parameter . my comprehension of model evaluation and improvement wa boosted by these additions.\\n.the main theme of this abstract is to illustrate how machine learning grows in the industry with their model to predict the future based on the previous data a well a , due to shortage of providing enough machine learning skill to end user result in a black box manner. in this some of the ml model like decision trees, random forest ,svm , naive bayes, etc are explored and also can apply these model to their own perspective data. .in the introduction part, mainly focus on how to overcome with the trustworthiness of end user after they learn from the correct one to apply in real world industrybecause of their opaqueness and interpretability difficulties, whitebox and blackbox machine learning ml model cause doubt with respect to the reliability of ml in meteorological studies.black box simply , it is a something can get result without being explanation of ml model . for example we can take neural network which is more complex to interpret.white box white box model are more interpretable to explain how we got the result form the data . example we can use decision trees, logistic regression, etc are ml model to interpret on data . .ml is an empirical method which we fit data on training to learn and test on test data for predicting with supervised and unsupervised learning techniques.supervised learning which ha both input feature and an output feature either numerical or categorical or both.for example , predict the spam or ham message after ml model train and test on data.supervised learning ha category a. classificationb. regressiona. classification problem can be used when we have an predicted output label and use some ml model based on the task we have taken to predict more accurately.b. regression model are used when we have continuous data a in output variable to predict next day weather temperature like that and we can experiment several ml regression model based on different task .the prediction are more accurate , if the error rate is low and the probabilistic is high. on other side unsupervised learning, here we don have output label . for example, clustering method the similar data will group together. .the main goal of this proposed paper is to predict the no.of thunderstorm counted based on the given input data. the sevir dataset is used for this project and the use case is thunderstorms. . author acquires the sevir data to predict how many no.of time are lightning a an output after having image of thunderstorm which is an input data. we can use different machine learning technique like svm, decision trees, naive bayes, etc.. . after the data collection, data cleaning , data processing and later this data go for training, testing and validation our data.training data can learn from the training parameter to generate stronger prediction or described some pattern relationship among the features. the training dataset is in range between to of dataset to train our data,rest to of data go for both validation and test data. some time training performance is good but not generalized well because of overfitting.validation the trained data will get validated based on the given parameter during training whether it is predicting or validating on trained data good are not.test set after training, they can test on test data sample which are not trained whether to check predicted correctly are not. . the raw data be a messy type of data and unnecessary data exists and no concern on this.after data exists, the next step is cleaning the data or preprocessing the data, feature engineering exists, in this step usually every member will take most of the time to get some quality of data for training and testing our data. . here, the author us for evaluating the model performance for classification are accuracy, area under the curve auc , roc curve, critical success index which ha no true negative , successor ratio sr .for regression, the evaluation metric be mean absolute error mae , mean squared error mse , root mean squared error rmse , squared . author generally make use of a variety of strategy and approach in order to solve the issue with black box machine learning ml model and enhance ml explainability.analyzing the usefulness of attribute is a typical approach. this involves recognizing the input feature variable that have the greatest impact on the prediction made by the model.sometimes, the interpretability of an ensemble can be improved by combining several black box models. a balance between accuracy and transparency, for instance, can be achieved by making use of an ensemble of decision tree such a random forest and then interpreting the ensemble predictions. . a widely used approach for modeling relationship between a dependent variable the target and one or more independent variable feature is linear regression, which combine machine learning with statistical methods. a. data first we need to collect the data which is related to weather with some features.b. feature selection choose those relevant independent variable property that you think might be correlated to the characteristic of thunderstorms.c. data cleaning which handle missing values, scaling the features, etc. d. we can apply linear regression to our data. we will calculate the link between an individual independent variable and the thunderstorm variable using simple linear regression. you can model a relationship between different independent variable and thunderstorm using multiple linear regression.e. we will evaluate the model, limitation of the model, and evaluate the model to predict.f. a basic grasp of the relation between different weather related variable and thunderstorm characteristic can be gained by meteorological study using linear regression. however, when dealing with complex and nonlinear meteorological phenomenon like thunderstorms, it crucial that you understand it limitation and take into account advance modeling techniques. . generally , this tutorial benefit the beginner how to predict the future outcome through ml predictive modelling technique like decision trees, regression, naive bayes, support vector machine, gradient boosting, etc.also learns the modelling workflow or end to end pipeline and follows all necessary step taken for training the data. . regularizarion, a different evaluation metric for classification and regression, ridge, lasso and auc, roc curve are the three new thing i have learnt and it a very useful for the future while modelling.\\n.the main theme of this abstract is to illustrate how machine learning grows in the industry with their model to predict the future based on the previous data a well a , due to shortage of providing enough machine learning skill to end user result in a black box manner. in this some of the ml model like decision trees, random forest ,svm , naive bayes, etc are explored and also can apply these model to their own perspective data. .in the introduction part, mainly focus on how to overcome with the trustworthiness of end user after they learn from the correct one to apply in real world industrybecause of their opaqueness and interpretability difficulties, whitebox and blackbox machine learning ml model cause doubt with respect to the reliability of ml in meteorological studies.black box simply , it is a something can get result without being explanation of ml model . for example we can take neural network which is more complex to interpret.white box white box model are more interpretable to explain how we got the result form the data . example we can use decision trees, logistic regression, etc are ml model to interpret on data . .ml is an empirical method which we fit data on training to learn and test on test data for predicting with supervised and unsupervised learning techniques.supervised learning which ha both input feature and an output feature either numerical or categorical or both.for example , predict the spam or ham message after ml model train and test on data.supervised learning ha category a. classificationb. regressiona. classification problem can be used when we have an predicted output label and use some ml model based on the task we have taken to predict more accurately.b. regression model are used when we have continuous data a in output variable to predict next day weather temperature like that and we can experiment several ml regression model based on different task .the prediction are more accurate , if the error rate is low and the probabilistic is high. on other side unsupervised learning, here we don have output label . for example, clustering method the similar data will group together. .the main goal of this proposed paper is to predict the no.of thunderstorm counted based on the given input data. the sevir dataset is used for this project and the use case is thunderstorms. . author acquires the sevir data to predict how many no.of time are lightning a an output after having image of thunderstorm which is an input data. we can use different machine learning technique like svm, decision trees, naive bayes, etc.. . after the data collection, data cleaning , data processing and later this data go for training, testing and validation our data.training data can learn from the training parameter to generate stronger prediction or described some pattern relationship among the features. the training dataset is in range between to of dataset to train our data,rest to of data go for both validation and test data. some time training performance is good but not generalized well because of overfitting.validation the trained data will get validated based on the given parameter during training whether it is predicting or validating on trained data good are not.test set after training, they can test on test data sample which are not trained whether to check predicted correctly are not. . the raw data be a messy type of data and unnecessary data exists and no concern on this.after data exists, the next step is cleaning the data or preprocessing the data, feature engineering exists, in this step usually every member will take most of the time to get some quality of data for training and testing our data. . here, the author us for evaluating the model performance for classification are accuracy, area under the curve auc , roc curve, critical success index which ha no true negative , successor ratio sr .for regression, the evaluation metric be mean absolute error mae , mean squared error mse , root mean squared error rmse , squared . author generally make use of a variety of strategy and approach in order to solve the issue with blackbox machine learning ml model and enhance ml explainability.analyzing the usefulness of attribute is a typical approach. this involves recognizing the input feature variable that have the greatest impact on the prediction made by the model.sometimes, the interpretability of an ensemble can be improved by combining several blackbox models. a balance between accuracy and transparency, for instance, can be achieved by making use of an ensemble of decision tree such a random forest and then interpreting the ensemble predictions. . a widely used approach for modeling relationship between a dependent variable the target and one or more independent variable feature is linear regression, which combine machine learning with statistical methods. a. data first we need to collect the data which is related to weather with some features.b. feature selection choose those relevant independent variable property that you think might be correlated to the characteristic of thunderstorms.c. data cleaning which handle missing values, scaling the features,etc. d. we can apply linear regression to our data. we will calculate the link between an individual independent variable and the thunderstorm variable using simple linear regression. you can model a relationship between different independent variable and thunderstorm using multiple linear regression.e. we will evaluate the model, lmitations of the model, and evaluate the model to predict.f. a basic grasp of the relation between different weather related variable and thunderstorm characteristic can be gained by meteorological study using linear regression. however, when dealing with complex and nonlinear meteorological phenomenon like thunderstorms, it crucial that you understand it limitation and take into account advance modeling techniques. . generally , this tutorial benefit the beginner how to predict the future outcome through ml predictive modelling technique like decision trees, regeression, naive bayes, support vector machine, gradient boosting, etc.also learns the modelling workflow or end to end pipeline and follows all necessary step taken for training the data. . regularizarion, a different evaluation metric for classification and regression, ridge, lasso and auc, roc curve are the three new thing i have learnt and it a very useful for the future while modelling.\\n.the paper aim to solve the detection of thunderstorms. the application are trained using the storm event imagery dataset sevir ,sevir consists of an example storm event , and measured variables.the first task is to determine whether an image ha a thunderstorm and the second is to guess the number of lightning flash present in an image.the paper aim to use ml to classify image for the presence of thunderstorm and estimate the number of flashes. .the feature input are infrared brightness temperature,water vapor brightness temperature etc.and the output feature are if the picture contain a thunderstorm and how many lightning flash are in the image. .as the model need to learn there need to be a very large dataset generally the training set is larger than the testing dataset and ghthe validation dataset is used to assess and finetune the model to correctly give result when using the training dataset and finally the testing dataset is used to check if the model is correctly trained or not and provide an unbiased evaluation. . .for the classification task the author us evaluation metric which include accuracy curve etc.in addition they also meanto use performance diagram this contains the pod and sr. they also use model comparision and feature importance. for the regression task they use mean bias,mean absolute error,root mean squared error and coeffiient of determination. .the concern were about the quality of the meterological datasets of both the satellite and radar data the quality issue effect the reliability i.e it make the dataset less reliable and inacurrate.the data may also contain noise that may give inaccurate output. the author proposed quality control and cleaning and data manipulation and feature engineering and percentiles. .the author use permutation importance ,which quantifies the relative importance of input feature by assessing how randomizing a feature would affect the performance metric such a the accuracy curve.the author address the concern by employing technique that increase the understanding of their models. .predicting the number of lightning flash within an image this is a regression task therefore we can use linear regression is one of the main ml learning techniques.in the paper linear regression is applied using the default parameter . the model is trained to learn the relationship between the selected feature and the target variable.the paper us mean bias ,mae,rmse,coefficient of determination.the linear regreession serf a a intial model to address the research problem of predicting lightning flashes. .overall the paper give a step by step approach to applying ml in real world problems,it discusses from the beginning i.e how data is collected and to the ending i.e the evaluation of the model.since the paper us simple term and non technical explaination it is easy for a beginner to understand the machine learning method and application .the paper introduced me to some new evaluation metric such a the pod and also introduced me to technique such a the permutation importance and accumulated local effect ale .i already know about evaluation metric such a auc,mae,rmse,and square.and the final thing a the hyper parameter tuning.althought the paper explaing hyperparameter tuning to some extent i think ill still need some more time before i understand it completely.\\nthe background application to be resolved are radar and satellite storm images. section is a discussion of potential pitfall and the importance of having a high quality dataset.q the author acquires data. the amount of data needed depends on the project and the amount of data available for use. the dataset also needed to include image off place when a storm wa not present so that the machine learning program is able to differentiate between a storm and a non storm.q after the data ha been thoroughly quality controlled, the next step are training, validating and testing the datasets. training is done with a large dataset in order to generalize a wide variety of examples, validating is done with a small dataset to assure the program doesn overfit and the test dataset is also small and performed at the end after the other step have concluded. these step are important to ensure that the program is returning the most accurate result possible.q concern about the raw data relate to how the data is broken up for the testing steps. it is important to understand the data being used so that you can know if randomly separating the data into chunk will be beneficial for the machine learning program being tested. for the sevir dataset, breaking up the dataset randomly would be lead to bias in the program.\\nthe author is using a collection of image of storm event acquired from the sevir data set. for each image, the following were extracted magnitude of reflectance in the visible channel, the coldness of the brightness temperature in the water vapor and clean infrared channels, and the amount of vertically integrated water there is. the paper further explains that these value were characterized a the following percentile , , , , , , , , and . because all of these variable are classified using specific percentile categories, they re all categorical variables. regarding the output, for the first problem, the image from the data set are classified a either being a thunderstorm or not being a thunderstorm based on the frequency of lightning strikes. because this feature only ha two possible values, we can think of it a categorical. for the second problem, lightning flash are summed, giving a numeric value for this feature. the author explains the importance of these category by discussing the problem of overfitting. essentially, it is possible that a machine learning model could perform well on a specific set of training data, but could then perform poorly on more general data. in this case, we would say that the machine learning model is overfit to that training data. by splitting the data into the three category mentioned, the training data can be used to tune the machine learning model, then the validation set can be used to evaluate if the model is overfit, and can also be used to adjust some of the model parameters. finally, the testing dataset can be used to evaluate the performance of the model. the key difference highlighted by the paper is that the validation phase still involves tweaking and adjusting model parameters, whereas the testing phase is purely for judging the performance of the model. because the entire machine learning model is being trained, validated, then finally tested on the dataset being used, the quality of the machine learning model is largely dependent on the quality of the dataset. problem with the data itself will almost certainly have an effect on the performance of the final model, either due to bias or other issue that could develop. one way this can happen is if the dataset contains spurious data that could affect the model ability to make identifications. in this specific case, this could happen if any of the radar image in the dataset contain satellite artifact or radar ground clutter. although the author doe mention that the sevir dataset being used ha already been subjected to quality control. another issue that can come up is if the data is improperly divided into the training, validating, and testing phases. essentially, we want the three data subset to be a independent a possible. the paper discusses that it common practice to divide the data into it subset randomly, but that this approach isn ideal for meteorology because of data correlation based on location and time.\\nin section a and the author of this paper get both satellite and radar data via sevir nexrad and lightning flash and flash frequency over time for regression via the glm. essentially the sevir and nexrad data input are huge raster that tell about the current meteorology within an area and the glm output or predicted variable is a point on a grid where large energy discharge flash are detected from space. the section about creating good training, validation, and testing set for your data is engaged with in a way to not only introduce a big problem in machine learning overfitting but also a a way to make more sense of what it actually mean to train a machine learning model. by splitting the data into these set then talking about parameter and then hyperparameters i think the paper doe a great job compartmentalizing an extremally complex topic wrapped up in a section about splitting up data. i know firsthand the problem with data management a the majority of time i have spent engaging with ml ha been wrangling excel file and csv file into the correct form. this is especially true in regard to the idea of garbage in garbage out . data that is professionally curated is one thing but actually collecting the data yourself can lead to inconsistency in the dataset which are not usually that cumbersome, but attach a inconsistency rate on a file with record and you have a lot of minor problem to work through.\\nquestion in this passages, the author take data from sevir which ha data from to .it say that it ha data feature from go and nexrad instruments. the different task discussed in this paper are knowing whether an image ha a classification data related to thunderstorm, finding out the total number of lightning flash in a particular image which is regression problem. it is also noticed that glm observation are unavailable, which are necessary for use of other meteorological measurement a features. for input features, the author try to include different variable like visible channel reflectance, water vapor brightness temperature, infrared brightness temperature, and many others. initially, author think to take use of every pixel in image a a predictor, but a sevir image ha high pixel count, the author opts for extracting key statistic from each variable which include percentile to differentiate distribution of variable within each image. in case of output variables, the author take the no of lightning flash in an image. this output variable help in know both classification and regression problem in this issue. question the author give much significance for creating distinct training, testing and validation datasets in this problem so that to ensure it is robust and generalize the ml models. while the training dataset ha large portion of data allowing model to learn patterns, sometimes it can lead to overfit model, and on other side the validation dataset let assess model performance during parameter tuning, finally the testing dataset give a evaluation of model true skill for unseen data and it prediction and reliability for that problem. this careful subset separation of dataset help in achieving best ml solution and trusty result in case of any solutions. question the author majorly concern with the raw data in the dataset which easily can inject noise. in order to get rid of it, the author show importance of data quality control a a preprocessing step. even though sevir dataset ha undergone quality control, it is not pristine yet. so, along with this data quality control, cleaning and manipulating the data is considered and this step is important in preventing the scenario called garbage in garbage out , laying best example for reliable and accurate ml model outcome in meteorological applications.\\nquestion the author acknowledge the importance of data quality and preprocessing. they highlight two common issue in meteorological datasets satellite artifact and radar ground clutter. to address these concerns, they perform the following preprocessing step quality control the sevir dataset ha already undergone rigorous quality control. however, they caution that this is not always the case with raw meteorological datasets, and extensive cleaning is often necessary to remove spurious data.feature engineering they mention the importance of extracting relevant feature from the data. in their case, they select statistic from each image and variable, such a percentile , , , , , , , , and , which are used a input feature for their machine learning models.label creation they create label by summing the number of lightning flash in the image. for problem statement , an image is classified a containing a thunderstorm if it ha at least one flash in the last five minutes. for problem statement , they use the sum of all lightning flash in the past five minute a the regression target.in summary, the author highlight the critical importance of data quality, feature engineering, and careful selection of training, validation, and testing set to ensure the success of their machine learning application in meteorology. they emphasize the need to address data concern and preprocess the data to achieve a high quality dataset, which is essential for building successful machine learning models.\\nthe application of ml wa used on radar and satellite thunderstorm image datasets from over storms. the goal wa to see if an image contained a thunderstorm, and how many lightning strike were in the image. these datasets came from the storm event imagery dataset. the image data contained the five following measurement red channel visible reflectance, midtropospheric water vapor brightness temperature, clean infrared window brightness temperature, vertically integrated liquid, and glm measured lightning flashes. the lightning flash task required classification, whether there were lightning flashes, and regression to find the number of lightning flashes. no storm image were also included to diversify the training set. training, validation, and testing set are important for machine learning. training set are the largest of these sets, and train the model. validation help fine tune the model and prevent overfitting. testing set are unseen data and used to test the performance of the ml model. bad data were removed a a part of quality control. this is done to make sure that input are consistent and represent the data well. this is also to make sure that there isn extra noise in the training set that would impact the quality of the model. there are many classification models, the idea is to try them all and find the best fit by analyzing performance metric of the models. the author address concern of the ml black box problem by applying permutation performance to see which input feature are the most important to decision making. linear regression model are compared to find best fit to a line for counting lightning strikes.\\nfor evaluating the classification problem, the author started by running various machine learning model using sickit learn, default hyper parameters, and a single input feature, in this case the minimum infrared brightness temperature. the author go on to explain that all of the machine learning method tested achieved on accuracy with this variable on the validation dataset, but that accuracy isn always the best measurement. the author then go on to discus auc, or area under the curve. in this approach, a receiver operating characteristic curve is derived from the relationship between the probability of detection and probability of false detection. in this instance, the model still all had relatively similar performance, so another method of evaluating machine learning model is discussed, the performance diagram. the difference here is that the axis represents a success ratio rather than the probability of false detection. the idea here is that the performance diagram graph show which algorithm achieved higher success ratio alongside a higher probability of detection. this evaluation doesn consider the impact of true negative results, which can be useful in meteorology because, a the author explains, large impact meteorological event are often rare. a similar approach is used for evaluating the regression problem of counting lightning flashes, wherein a simple scenario is considered with the only input variable being the minimum infrared brightness temperature. the model are then quantitatively compared using the mean bias, mean absolute error, root mean squared error, and coefficient determination. after these more simple test case are done, the same test and evaluation were performed using all of the available input feature for both the classification and the regression problem, and the resulting data wa included. for the regression problem specifically, the model didn perform quite a well, so some time is also spent discussing the tuning of hyper parameters. the author attempt to address the black box issue by spending some time discussing how the data can be interpreted, by asking key question like what input feature most influenced this decision and do the pattern learned by the machine learning model follow meteorological expectation . the author then brings up permutation importance and accumulated local effect a two technique that can address the question mentioned. alongside siting relevant sources, the author briefly discusses these two techniques, with permutation importance evaluating the impact of a given variable by judging how much the evaluation metric of the model change when the given input variable is shuffled. accumulated local effect ale on the other hand serve to quantify how minor change in input feature correspond to change in the model output. linear regression is specifically used in attempting to predict the number of lightning flash inside of an image. it is mentioned a being a useful starting point due to it simplicity a well a it history of use in meteorology a a discipline. because of it simplicity, it form a good point of comparison for newer, and potentially more complicated regression machine learning algorithms.\\nquestion in the above passages, the paper had two task , one related to classification and other related to regression. so in both the cases, the author implemented a comprehensive approach. in the first task of classification, they initially train the multiple model , then try using them by giving different feature and finding out different metric like accuracy, auc and then once after that, they impose different technique related to interpretation called a accumulated local effects, permutation importance. thus these all evaluation procedure having multiple level ensures a thorough understanding of the performance of the model and find error if any occurs to give a best accurate result. in the same way, to the regression task, the author first start with a simple model and one predictor. later it will try assessing the performance of the model with different metric like rmse, , bias etc and also can be seen via help of plot and graphs. then they implement the same to all the other predictor and analyze the different performance of various models. finally after this procedure, they conduct hyperparameter tuning and final testing using testing dataset. thereby, the author ensures that the used ml is robust and accurate. question the author address the black box ml concern by implementing different technique like use of permutation importance quantifying the significance of input feature by finding the metric when feature are randomly shuffled thus giving which feature is most influential in this prediction and ale help in knowing the relation between input and output by binning data and replacing the value with bin edge and finding mean difference thus knowing how change in input affect the model output and know the learned pattern are compatible with the expectation of the output .\\nquestion the author identifies concern with raw meteorological data, such a noise, missing values, imbalanced data, high dimensionality, and location specific trends. to address these, the author proposes preprocessing step like data cleaning, imputation for missing values, data balancing, feature engineering, normalization, and seasonal adjustments. these step aim to improve data quality, making it more suitable for training reliable and effective machine learning model in meteorological applications. question i found that the author us quantitative metric such a accuracy and precision for initial evaluation and employ fold cross validation for robustness. result are compared with traditional meteorological model to gauge efficacy. confidence interval measure prediction uncertainty, and real world testing assesses model reliability. interpretability is also emphasized for understanding prediction rationale. question according to the author, use of interpretable algorithm like decision tree and random forests.feature importance score to identify influential variables.linear model for straightforward interpretation.techniques like lime for neural network explainability. question in subsection .a, the author us linear regression a a straightforward, interpretable method for tackling a meteorological problem. they start by selecting relevant feature like temperature and humidity. the data is then split into training, validation, and testing set for effective model training and evaluation. the linear regression model is trained on these features, and it parameter are fine tuned using the validation set. finally, the model performance is evaluated using metric like mse, and the coefficient are analyzed to understand the influence of each feature. question according to my learning i found this book is useful in giving,foundational knowledge provides a strong base in machine learning concepts.step by step guidance walk through the entire machine learning pipeline.contextual relevance us meteorological example for direct applicability.interpretability focus address the importance of model transparency and trust.hands on experience offer code for practical, hand on learning.resource optimization discusses practical utility in resource allocation.future direction give insight into upcoming challenge and opportunities. question the concept that i found new and challenging arethe concept of feature importance, particularly in the context of complex model like random forest and gradient boosting.various data preprocessing step like imputation, data balancing, and normalization and the concept of fold cross validation for robust model evaluation\\nquestion the process to evaluate and validate is through a comprehensive approach that includes several step a below performance metric the author use various performance metric to assess the result of their machine learning models. for classification task , they consider accuracy, the area under the roc curve auc , and performance diagrams, which provide insight into model performance beyond accuracy. for regression tasks, they use metric such a mean bias, mean absolute error mae , root mean squared error rmse , and coefficient of determination to quantify the accuracy and predictive power of their models.one to one plot for the regression task, the author employ one to one plot to visually assess the model performance. these plot show the correspondence between predicted and true values. a perfect model would have all data point aligned along the diagonal.hyper parameter tuning in the regression task, the author perform hyper parameter tuning on the random forest model. they systematically vary parameter such a the maximum depth of tree and the number of tree in the forest. they evaluate the impact of these hyper parameter on model performance using the defined metrics.interpretability technique to address the black box nature of machine learning models, the author employ interpretability technique like permutation importance and accumulated local effect ale . these technique help them understand which input feature are most important for model prediction and investigate whether the model pattern align with meteorological expectations.model comparison throughout the evaluation, the author compare the performance of different machine learning methods, such a logistic regression, random forest, gradient boosted trees, and support vector machines. they identify the best performing model based on various metrics.discussion and interpretation the author interpret the result in the context of meteorological knowledge. they assess whether the important predictor identified by the model align with known meteorological principle or if there are discrepancy that require further investigation.in summary, the author employ a combination of quantitative metrics, visualization tools, hyper parameter tuning, and interpretability technique to rigorously evaluate and validate the result of their machine learning algorithms.\\nquestion the author address the concern of black box machine learning by employing several technique to improve ml explainability permutation importance the author use permutation importance to understand the importance of input feature in their machine learning models. by shuffling feature value and observing the impact on model performance, they identify which feature are most influential in making predictions. this help provide transparency into feature importance.accumulated local effect ale ale is another technique used to investigate the relationship between input feature and model outputs. it quantifies how small change in input feature affect model predictions. ale allows the author to gain insight into how the model make decision based on specific feature values.feature interpretation through permutation importance and ale, the author interpret the significance of input feature and assess whether the model pattern align with meteorological knowledge. they analyze whether the most important predictor make sense in a meteorological context.model comparison the author compare the performance and interpretability of different machine learning methods, such a logistic regression, random forest, gradient boosted trees, and support vector machines. they select model that not only perform well but also offer insight that align with meteorological expectations.by employing these techniques, the author strive to make their machine learning model more interpretable and transparent, addressing the black box issue and ensuring that model prediction can be understood in the context of meteorological science.\\nquestion the linear regression approach is applied to the regression task of predicting the number of lightning flash within an image. here how the linear regression approach is used single feature model initially, the author consider a simple scenario where they use the minimum infrared brightness temperature th a the sole predictor for the regression task. th serf a a proxy for the depth of storms, which is related to lightning formation.assessment of single feature model the author assess the performance of this single feature linear regression model. they evaluate how well it predicts the number of lightning flash within image based on th. they use metric such a mean bias, mean absolute error mae , root mean squared error rmse , and coefficient of determination to quantify the model accuracy and predictive power.challenges with single feature model they observe that the single feature linear regression model face challenges, especially for image with fewer than flashes. linear methods, in this case, tend to exhibit strong over predictions.complex scenario with all feature to improve model performance, the author transition to a more complex scenario where they use all available feature a predictor for the regression task. this expanded feature set aim to provide more information to the model and enhance prediction accuracy.evaluation of complex model using all available features, the author evaluate the performance of various regression methods, including linear regression, decision tree, random forest, gradient boosted trees, and support vector machines. they compare these model using metric like mae, rmse, and to determine which method performs best.hyper parameter tuning in the case of the random forest model, the author perform hyper parameter tuning by systematically varying parameter like the maximum depth of tree and the number of tree in the forest. they evaluate how different configuration impact model performance.in summary, the linear regression approach is initially applied to create a baseline model using a single feature th and is then compared to more complex model using all available feature to predict the number of lightning flash in images. the author use a range of evaluation metric and technique to assess the effectiveness of linear regression and other regression method in this context.\\nthe abstract introduces the theme and the problem the author are trying to address. the paper will introduce several novel hypothesis and argument in machine learning education a it applies to meteorology students. the paper demonstrates several regression model and statistical method and attempt to educate meteorology student on them even providing the python script used. machine learning in some way run counter to the predictive and demonstrative nature of meteorology. machine learning regarding forecast can be reliable and accurate interestingly, but they are not always consistent with training data. meteorologist cannot necessarily trust black box model that don take into account their prior knowledge, so the paper attempt to make machine learning more accessible to them. the paper essentially distills the first few lecture down to the key point needed, such a feature and label and how they are used in supervised learning. the paper explains the optimization paradigm and how parameter are used to limit a predefined loss function. the paper explains how those label and feature are used to train towards a target, and how it mathematically can be represented a a vector. for meteorologists, the biggest thing for them to know are that they can have either regression or classification model used to forecast the weather.the datasets being used come from radar and satellite datasets in particular the storm event imagery dataset and the nexrad. the application aim to observe sevir used to train. a model to determine thunderstorm and how many lightning flash are in an image potentially for use on satellite without lightning. the author use storm image data from sevir a well a nexrad and glm lightning mapped data. the author aim to use a feature input of storm characteristic such a magnitude of reflectance and coldness of brightness temperature in the water vapor, and these can be used to generate output statistics. the author describes the training dataset a the largest subset of data that will be generated and this is so that the model can generalize to a wide variety of examples. the cleaning of a dataset will take up the majority of time because spurious or noisy data make a poor ml model. the test dataset used to validate will be set aside until after all model have been trained, and the validation data will be incredibly specific in order to train the dataset. a method that can be used is fold cross validation which is a resampling methods. that can be used in the event of less robust statistics. the author concern included the composition of sevir data, and how exactly to split it. they also wanted to avoid introducing bias in the data with image that only contained thunderstorms. they decided to do a resampling method known a fold cross validation because the data could potentially be skewed. they got a very large training data set, and then smaller validation and testing set a well. they evaluated the model by using area under curve auc and receiver operating characteristic curve roc methods. they used the roc and obtained the metric by comparing the probability of false detection and the probability of detection to determine the overall true to false positive ratio of the dataset. they also used a threshold for prediction accuracy. another way wa to not use the true negative which can heavily bias machine learning model in meteorology. they used smaller datasets to make them easier to interpret, used extremely specific default hyper parameter to evaluate the algorithms, and they also used the backward permutation importance test to make the most important metric more apparent. linear regression wa used because it is simple and efficient, and it wa used in conjunction with others such a decision tree and random forest to compare how each variable wa observed to behave. linear regression actually performed near the worst with single feature with bias in how it perceived flashes.this paper wa an example of applied machine learning where specialist outside computing can actually understand it. it also demonstrates theoretically how machine learning is done, and it explains how datasets need to treated, how to validate and test at a very basic level, some basic statistic used and involved in the model, and finally wa able to be used to classify a novel type of data from a large dataset so new user can observe how machine learning can benefit them. this paper wa fairly well written and somewhat straightforward, but i am still confused a to how all the different regression model behave and what exactly they are comparing. i am also somewhat unsure of how to use many of the test they demonstrate in the paper to do analyze these models, and hope that i can build on a statistic background and understand them intuitively later on.\\ni knew about how training datasets worked and that you would need a testing set, but i did not know about validation set or rather the difference between the two. that is very interesting to me because sectioning the total data must be a bit of a grueling task, to ensure that each set of data is quality and unbiased. in addition, can there be multiple validation sets? would there be a need? this way of thinking is new for me.true negatives, or simply data that could dominate the training set is something i didn ever think of or know wa possible.not necessarily new but something that popped out to me. i understood ml application could use image a input and i assumed it wa pixel and their arrangement that identified things. in this case, the pixel count for each image would have been incredibly large and consequently inefficient for the application. using statistic on the pixel within the image to determine lightning flash and the number of them wa a cool idea i hadn thought of and make me wonder about what other type of ml apps can be simplified beyond just inputting something raw and outputting what you want. this tutorial did a great job at expanding the process deeper than high level.\\nafter going through the abstract of the paper, i found that the paper brings up some key issue like how ml model are treated a black box and restrict their use in meteorology and this paper us some of ml model like linear regression, logistic regression, decision trees, na ve bayes, gradient boosted decision tree and svm. this proposed paper us meteorological example to show how these model work and also the process and best approach to apply these model in different datasets and domain. most of the ml model opaque nature prevents meeting one of the three requirement like consistency prior to user knowledge, accuracy, and benefit. most of the paper model satisfy the last two condition but fail to satisfy the condition and this make the model lose trustworthiness among the users. this section majorly covered about what is ml model and how many type these are sub categorized like supervised and unsupervised and showcased how these differ and work differently. then it presented the workflow of how ml model training go and also explained about type of supervised like classification and regression model and on what type of target data and type of prediction we can apply these on different datasets. the paper aim to work on the thunderstorm and this us sevir dataset and this paper us ml model like classification and regression are used to find if there is a thunderstorm in the image and estimate lightning is present in the image and how many lightning flash are present in the image. every pixel in image can be considered a predictor but feature all the pixel of a single image is impractical and they used a set of statistic to shorten the feature pixel based on certain factor for each image and to create label the no of lightning flash are summed and for task if it contains at least one lightning flash in five minute it is classified a thunderstorm and for task the sum of lightning flash are used for the regression target the paper described the importance of training, validation and test data in a better way like the training data of original data is used to train the ml model and the validation data of original data is used to validate data after hyper parameter tuning and at last the test data is used to evaluate the model with the unseen data and check performance. they used time division to split the data a the adjacent data are correlated and to avoid biased output. the main question arises like how much data is needed to train the ml model and also removal of spurious data is important because it may make model overfit, underfit, show bias to some feature and may not work a intended too. so prior sending data to model it must be preprocessed and cleaned to avoid bias in model decision like missing value filling removal, duplicate data removal, removal of unnecessary feature and a lot to remove the spurious data the author used accuracy the standard metric to evaluate model , which indicate a good model and other metric used is common area under auc and this is associated with roc curve and this is calculated with pofd ad pod based on contingency table tp,fp,tn,fn pofd fp tp fp pod tp tp fn their all model showed auc nearly . which indicate best performance. the other metric used is performance diagram and this time pofd go on axis and sr go on axis. sr tp tp fp and at last the calculating csi without considering true negatives. by using these three metric they analyzed the result like the all model have almost same accuracy and auc and the differ in performance diagram , all the tree based method have . frequency bias and other model like svm, na ve bayes, logistic regression have frequency bias and having same csi a tree model these model are found to be best performing. to improve ml explainibility like how these model are making prediction the author used to method to check on what feature these model are making decision and by using permutation fold and multi fold the gained some insight and then by using accumulated local effect ale to see if ml model are learned to see plausible trend within input features. a the number of image with no lightning are of dataset , to increase performance they used the classification model output and then apply the regression to identify the number of lightning in the image. by observing the one to one plot, linear regression wa not able to perform well and it overpredicted and unpredicted some flash in the image and also linear model like svm and linear regression showed bias of and flash and tree based model showed a better performance. after reading the summary and future work of this paper, we can learn how to apply ml model on different datasets and work with them like how to collect data, engineering data, selecting features, splitting data and evaluate models. the paper majorly help in grasping all the above information a this paper used servir dataset to apply ml model like classification and regression on meteorology to find if a image ha a thunderstorm and how many lightning storm are present in a image. this paper helped me a lot in grasping new information like how to split data into training, validation and testing data and their importance in training, evaluating and testing the model and also how we can use time to split data if adjacent feature are depandant and avoid random splitting to prevent bias in predictions. the next part is how we can use different performance metric to check the best performing model and at last is how to apply different ml model on particular task and make predictions.\\n.the main theme of this abstract is to illustrate how machine learning grows in the industry with their model to predict the future based on the previous data a well a , due to shortage of providing enough machine learning skill to end user result in a black box manner. in this some of the ml model like decision trees, random forest ,svm , naive bayes, etc are explored and also can apply these model to their own perspective data. .in the introduction part, mainly focus on how to overcome with the trustworthiness of end user after they learn from the correct one to apply in real world industrybecause of their opaqueness and interpretability difficulties, whitebox and blackbox machine learning ml model cause doubt with respect to the reliability of ml in meteorological studies.black box simply , it is a something can get result without being explanation of ml model . for example we can take neural network which is more complex to interpret.white box white box model are more interpretable to explain how we got the result form the data . example we can use decision trees, logistic regression, etc are ml model to interpret on data . .ml is an empirical method which we fit data on training to learn and test on test data for predicting with supervised and unsupervised learning techniques.supervised learning which ha both input feature and an output feature either numerical or categorical or both.for example , predict the spam or ham message after ml model train and test on data.supervised learning ha category a. classificationb. regressiona. classification problem can be used when we have an predicted output label and use some ml model based on the task we have taken to predict more accurately.b. regression model are used when we have continuous data a in output variable to predict next day weather temperature like that and we can experiment several ml regression model based on different task .the prediction are more accurate , if the error rate is low and the probabilistic is high. on other side unsupervised learning, here we don have output label . for example, clustering method the similar data will group together. .the main goal of this proposed paper is to predict the no.of thunderstorm counted based on the given input data. the sevir dataset is used for this project and the use case is thunderstorms. . author acquires the sevir data to predict how many no.of time are lightning a an output after having image of thunderstorm which is an input data. we can use different machine learning technique like svm, decision trees, naive bayes, etc.. . after the data collection, data cleaning , data processing and later this data go for training, testing and validation our data.training data can learn from the training parameter to generate stronger prediction or described some pattern relationship among the features. the training dataset is in range between to of dataset to train our data,rest to of data go for both validation and test data. some time training performance is good but not generalized well because of overfitting.validation the trained data will get validated based on the given parameter during training whether it is predicting or validating on trained data good are not.test set after training, they can test on test data sample which are not trained whether to check predicted correctly are not. . the raw data be a messy type of data and unnecessary data exists and no concern on this.after data exists, the next step is cleaning the data or preprocessing the data, feature engineering exists, in this step usually every member will take most of the time to get some quality of data for training and testing our data. . here, the author us for evaluating the model performance for classification are accuracy, area under the curve auc , roc curve, critical success index which ha no true negative , successor ratio sr .for regression, the evaluation metric be mean absolute error mae , mean squared error mse , root mean squared error rmse , squared . author generally make use of a variety of strategy and approach in order to solve the issue with black box machine learning ml model and enhance ml explainability.analyzing the usefulness of attribute is a typical approach. this involves recognizing the input feature variable that have the greatest impact on the prediction made by the model.sometimes, the interpretability of an ensemble can be improved by combining several black box models. a balance between accuracy and transparency, for instance, can be achieved by making use of an ensemble of decision tree such a random forest and then interpreting the ensemble predictions. . a widely used approach for modeling relationship between a dependent variable the target and one or more independent variable feature is linear regression, which combine machine learning with statistical methods. a. data first we need to collect the data which is related to weather with some features.b. feature selection choose those relevant independent variable property that you think might be correlated to the characteristic of thunderstorms.c. data cleaning which handle missing values, scaling the features, etc. d. we can apply linear regression to our data. we will calculate the link between an individual independent variable and the thunderstorm variable using simple linear regression. you can model a relationship between different independent variable and thunderstorm using multiple linear regression.e. we will evaluate the model, limitation of the model, and evaluate the model to predict.f. a basic grasp of the relation between different weather related variable and thunderstorm characteristic can be gained by meteorological study using linear regression. however, when dealing with complex and nonlinear meteorological phenomenon like thunderstorms, it crucial that you understand it limitation and take into account advance modeling techniques. . generally , this tutorial benefit the beginner how to predict the future outcome through ml predictive modelling technique like decision trees, regression, naive bayes, support vector machine, gradient boosting, etc.also learns the modelling workflow or end to end pipeline and follows all necessary step taken for training the data. . regularizarion, a different evaluation metric for classification and regression, ridge, lasso and auc, roc curve are the three new thing i have learnt and it a very useful for the future while modelling.\\n.the main theme of this abstract is to illustrate how machine learning grows in the industry with their model to predict the future based on the previous data a well a , due to shortage of providing enough machine learning skill to end user result in a black box manner. in this some of the ml model like decision trees, random forest ,svm , naive bayes, etc are explored and also can apply these model to their own perspective data. .in the introduction part, mainly focus on how to overcome with the trustworthiness of end user after they learn from the correct one to apply in real world industrybecause of their opaqueness and interpretability difficulties, whitebox and blackbox machine learning ml model cause doubt with respect to the reliability of ml in meteorological studies.black box simply , it is a something can get result without being explanation of ml model . for example we can take neural network which is more complex to interpret.white box white box model are more interpretable to explain how we got the result form the data . example we can use decision trees, logistic regression, etc are ml model to interpret on data . .ml is an empirical method which we fit data on training to learn and test on test data for predicting with supervised and unsupervised learning techniques.supervised learning which ha both input feature and an output feature either numerical or categorical or both.for example , predict the spam or ham message after ml model train and test on data.supervised learning ha category a. classificationb. regressiona. classification problem can be used when we have an predicted output label and use some ml model based on the task we have taken to predict more accurately.b. regression model are used when we have continuous data a in output variable to predict next day weather temperature like that and we can experiment several ml regression model based on different task .the prediction are more accurate , if the error rate is low and the probabilistic is high. on other side unsupervised learning, here we don have output label . for example, clustering method the similar data will group together. .the main goal of this proposed paper is to predict the no.of thunderstorm counted based on the given input data. the sevir dataset is used for this project and the use case is thunderstorms. . author acquires the sevir data to predict how many no.of time are lightning a an output after having image of thunderstorm which is an input data. we can use different machine learning technique like svm, decision trees, naive bayes, etc.. . after the data collection, data cleaning , data processing and later this data go for training, testing and validation our data.training data can learn from the training parameter to generate stronger prediction or described some pattern relationship among the features. the training dataset is in range between to of dataset to train our data,rest to of data go for both validation and test data. some time training performance is good but not generalized well because of overfitting.validation the trained data will get validated based on the given parameter during training whether it is predicting or validating on trained data good are not.test set after training, they can test on test data sample which are not trained whether to check predicted correctly are not. . the raw data be a messy type of data and unnecessary data exists and no concern on this.after data exists, the next step is cleaning the data or preprocessing the data, feature engineering exists, in this step usually every member will take most of the time to get some quality of data for training and testing our data. . here, the author us for evaluating the model performance for classification are accuracy, area under the curve auc , roc curve, critical success index which ha no true negative , successor ratio sr .for regression, the evaluation metric be mean absolute error mae , mean squared error mse , root mean squared error rmse , squared . author generally make use of a variety of strategy and approach in order to solve the issue with blackbox machine learning ml model and enhance ml explainability.analyzing the usefulness of attribute is a typical approach. this involves recognizing the input feature variable that have the greatest impact on the prediction made by the model.sometimes, the interpretability of an ensemble can be improved by combining several blackbox models. a balance between accuracy and transparency, for instance, can be achieved by making use of an ensemble of decision tree such a random forest and then interpreting the ensemble predictions. . a widely used approach for modeling relationship between a dependent variable the target and one or more independent variable feature is linear regression, which combine machine learning with statistical methods. a. data first we need to collect the data which is related to weather with some features.b. feature selection choose those relevant independent variable property that you think might be correlated to the characteristic of thunderstorms.c. data cleaning which handle missing values, scaling the features,etc. d. we can apply linear regression to our data. we will calculate the link between an individual independent variable and the thunderstorm variable using simple linear regression. you can model a relationship between different independent variable and thunderstorm using multiple linear regression.e. we will evaluate the model, lmitations of the model, and evaluate the model to predict.f. a basic grasp of the relation between different weather related variable and thunderstorm characteristic can be gained by meteorological study using linear regression. however, when dealing with complex and nonlinear meteorological phenomenon like thunderstorms, it crucial that you understand it limitation and take into account advance modeling techniques. . generally , this tutorial benefit the beginner how to predict the future outcome through ml predictive modelling technique like decision trees, regeression, naive bayes, support vector machine, gradient boosting, etc.also learns the modelling workflow or end to end pipeline and follows all necessary step taken for training the data. . regularizarion, a different evaluation metric for classification and regression, ridge, lasso and auc, roc curve are the three new thing i have learnt and it a very useful for the future while modelling.\\n.the paper aim to solve the detection of thunderstorms. the application are trained using the storm event imagery dataset sevir ,sevir consists of an example storm event , and measured variables.the first task is to determine whether an image ha a thunderstorm and the second is to guess the number of lightning flash present in an image.the paper aim to use ml to classify image for the presence of thunderstorm and estimate the number of flashes. .the feature input are infrared brightness temperature,water vapor brightness temperature etc.and the output feature are if the picture contain a thunderstorm and how many lightning flash are in the image. .as the model need to learn there need to be a very large dataset generally the training set is larger than the testing dataset and ghthe validation dataset is used to assess and finetune the model to correctly give result when using the training dataset and finally the testing dataset is used to check if the model is correctly trained or not and provide an unbiased evaluation. . .for the classification task the author us evaluation metric which include accuracy curve etc.in addition they also meanto use performance diagram this contains the pod and sr. they also use model comparision and feature importance. for the regression task they use mean bias,mean absolute error,root mean squared error and coeffiient of determination. .the concern were about the quality of the meterological datasets of both the satellite and radar data the quality issue effect the reliability i.e it make the dataset less reliable and inacurrate.the data may also contain noise that may give inaccurate output. the author proposed quality control and cleaning and data manipulation and feature engineering and percentiles. .the author use permutation importance ,which quantifies the relative importance of input feature by assessing how randomizing a feature would affect the performance metric such a the accuracy curve.the author address the concern by employing technique that increase the understanding of their models. .predicting the number of lightning flash within an image this is a regression task therefore we can use linear regression is one of the main ml learning techniques.in the paper linear regression is applied using the default parameter . the model is trained to learn the relationship between the selected feature and the target variable.the paper us mean bias ,mae,rmse,coefficient of determination.the linear regreession serf a a intial model to address the research problem of predicting lightning flashes. .overall the paper give a step by step approach to applying ml in real world problems,it discusses from the beginning i.e how data is collected and to the ending i.e the evaluation of the model.since the paper us simple term and non technical explaination it is easy for a beginner to understand the machine learning method and application .the paper introduced me to some new evaluation metric such a the pod and also introduced me to technique such a the permutation importance and accumulated local effect ale .i already know about evaluation metric such a auc,mae,rmse,and square.and the final thing a the hyper parameter tuning.althought the paper explaing hyperparameter tuning to some extent i think ill still need some more time before i understand it completely.\\nthe background application to be resolved are radar and satellite storm images. section is a discussion of potential pitfall and the importance of having a high quality dataset.q the author acquires data. the amount of data needed depends on the project and the amount of data available for use. the dataset also needed to include image off place when a storm wa not present so that the machine learning program is able to differentiate between a storm and a non storm.q after the data ha been thoroughly quality controlled, the next step are training, validating and testing the datasets. training is done with a large dataset in order to generalize a wide variety of examples, validating is done with a small dataset to assure the program doesn overfit and the test dataset is also small and performed at the end after the other step have concluded. these step are important to ensure that the program is returning the most accurate result possible.q concern about the raw data relate to how the data is broken up for the testing steps. it is important to understand the data being used so that you can know if randomly separating the data into chunk will be beneficial for the machine learning program being tested. for the sevir dataset, breaking up the dataset randomly would be lead to bias in the program.\\nthe author is using a collection of image of storm event acquired from the sevir data set. for each image, the following were extracted magnitude of reflectance in the visible channel, the coldness of the brightness temperature in the water vapor and clean infrared channels, and the amount of vertically integrated water there is. the paper further explains that these value were characterized a the following percentile , , , , , , , , and . because all of these variable are classified using specific percentile categories, they re all categorical variables. regarding the output, for the first problem, the image from the data set are classified a either being a thunderstorm or not being a thunderstorm based on the frequency of lightning strikes. because this feature only ha two possible values, we can think of it a categorical. for the second problem, lightning flash are summed, giving a numeric value for this feature. the author explains the importance of these category by discussing the problem of overfitting. essentially, it is possible that a machine learning model could perform well on a specific set of training data, but could then perform poorly on more general data. in this case, we would say that the machine learning model is overfit to that training data. by splitting the data into the three category mentioned, the training data can be used to tune the machine learning model, then the validation set can be used to evaluate if the model is overfit, and can also be used to adjust some of the model parameters. finally, the testing dataset can be used to evaluate the performance of the model. the key difference highlighted by the paper is that the validation phase still involves tweaking and adjusting model parameters, whereas the testing phase is purely for judging the performance of the model. because the entire machine learning model is being trained, validated, then finally tested on the dataset being used, the quality of the machine learning model is largely dependent on the quality of the dataset. problem with the data itself will almost certainly have an effect on the performance of the final model, either due to bias or other issue that could develop. one way this can happen is if the dataset contains spurious data that could affect the model ability to make identifications. in this specific case, this could happen if any of the radar image in the dataset contain satellite artifact or radar ground clutter. although the author doe mention that the sevir dataset being used ha already been subjected to quality control. another issue that can come up is if the data is improperly divided into the training, validating, and testing phases. essentially, we want the three data subset to be a independent a possible. the paper discusses that it common practice to divide the data into it subset randomly, but that this approach isn ideal for meteorology because of data correlation based on location and time.\\nin section a and the author of this paper get both satellite and radar data via sevir nexrad and lightning flash and flash frequency over time for regression via the glm. essentially the sevir and nexrad data input are huge raster that tell about the current meteorology within an area and the glm output or predicted variable is a point on a grid where large energy discharge flash are detected from space. the section about creating good training, validation, and testing set for your data is engaged with in a way to not only introduce a big problem in machine learning overfitting but also a a way to make more sense of what it actually mean to train a machine learning model. by splitting the data into these set then talking about parameter and then hyperparameters i think the paper doe a great job compartmentalizing an extremally complex topic wrapped up in a section about splitting up data. i know firsthand the problem with data management a the majority of time i have spent engaging with ml ha been wrangling excel file and csv file into the correct form. this is especially true in regard to the idea of garbage in garbage out . data that is professionally curated is one thing but actually collecting the data yourself can lead to inconsistency in the dataset which are not usually that cumbersome, but attach a inconsistency rate on a file with record and you have a lot of minor problem to work through.\\nquestion in this passages, the author take data from sevir which ha data from to .it say that it ha data feature from go and nexrad instruments. the different task discussed in this paper are knowing whether an image ha a classification data related to thunderstorm, finding out the total number of lightning flash in a particular image which is regression problem. it is also noticed that glm observation are unavailable, which are necessary for use of other meteorological measurement a features. for input features, the author try to include different variable like visible channel reflectance, water vapor brightness temperature, infrared brightness temperature, and many others. initially, author think to take use of every pixel in image a a predictor, but a sevir image ha high pixel count, the author opts for extracting key statistic from each variable which include percentile to differentiate distribution of variable within each image. in case of output variables, the author take the no of lightning flash in an image. this output variable help in know both classification and regression problem in this issue. question the author give much significance for creating distinct training, testing and validation datasets in this problem so that to ensure it is robust and generalize the ml models. while the training dataset ha large portion of data allowing model to learn patterns, sometimes it can lead to overfit model, and on other side the validation dataset let assess model performance during parameter tuning, finally the testing dataset give a evaluation of model true skill for unseen data and it prediction and reliability for that problem. this careful subset separation of dataset help in achieving best ml solution and trusty result in case of any solutions. question the author majorly concern with the raw data in the dataset which easily can inject noise. in order to get rid of it, the author show importance of data quality control a a preprocessing step. even though sevir dataset ha undergone quality control, it is not pristine yet. so, along with this data quality control, cleaning and manipulating the data is considered and this step is important in preventing the scenario called garbage in garbage out , laying best example for reliable and accurate ml model outcome in meteorological applications.\\nquestion the author acknowledge the importance of data quality and preprocessing. they highlight two common issue in meteorological datasets satellite artifact and radar ground clutter. to address these concerns, they perform the following preprocessing step quality control the sevir dataset ha already undergone rigorous quality control. however, they caution that this is not always the case with raw meteorological datasets, and extensive cleaning is often necessary to remove spurious data.feature engineering they mention the importance of extracting relevant feature from the data. in their case, they select statistic from each image and variable, such a percentile , , , , , , , , and , which are used a input feature for their machine learning models.label creation they create label by summing the number of lightning flash in the image. for problem statement , an image is classified a containing a thunderstorm if it ha at least one flash in the last five minutes. for problem statement , they use the sum of all lightning flash in the past five minute a the regression target.in summary, the author highlight the critical importance of data quality, feature engineering, and careful selection of training, validation, and testing set to ensure the success of their machine learning application in meteorology. they emphasize the need to address data concern and preprocess the data to achieve a high quality dataset, which is essential for building successful machine learning models.\\nthe application of ml wa used on radar and satellite thunderstorm image datasets from over storms. the goal wa to see if an image contained a thunderstorm, and how many lightning strike were in the image. these datasets came from the storm event imagery dataset. the image data contained the five following measurement red channel visible reflectance, midtropospheric water vapor brightness temperature, clean infrared window brightness temperature, vertically integrated liquid, and glm measured lightning flashes. the lightning flash task required classification, whether there were lightning flashes, and regression to find the number of lightning flashes. no storm image were also included to diversify the training set. training, validation, and testing set are important for machine learning. training set are the largest of these sets, and train the model. validation help fine tune the model and prevent overfitting. testing set are unseen data and used to test the performance of the ml model. bad data were removed a a part of quality control. this is done to make sure that input are consistent and represent the data well. this is also to make sure that there isn extra noise in the training set that would impact the quality of the model. there are many classification models, the idea is to try them all and find the best fit by analyzing performance metric of the models. the author address concern of the ml black box problem by applying permutation performance to see which input feature are the most important to decision making. linear regression model are compared to find best fit to a line for counting lightning strikes.\\nfor evaluating the classification problem, the author started by running various machine learning model using sickit learn, default hyper parameters, and a single input feature, in this case the minimum infrared brightness temperature. the author go on to explain that all of the machine learning method tested achieved on accuracy with this variable on the validation dataset, but that accuracy isn always the best measurement. the author then go on to discus auc, or area under the curve. in this approach, a receiver operating characteristic curve is derived from the relationship between the probability of detection and probability of false detection. in this instance, the model still all had relatively similar performance, so another method of evaluating machine learning model is discussed, the performance diagram. the difference here is that the axis represents a success ratio rather than the probability of false detection. the idea here is that the performance diagram graph show which algorithm achieved higher success ratio alongside a higher probability of detection. this evaluation doesn consider the impact of true negative results, which can be useful in meteorology because, a the author explains, large impact meteorological event are often rare. a similar approach is used for evaluating the regression problem of counting lightning flashes, wherein a simple scenario is considered with the only input variable being the minimum infrared brightness temperature. the model are then quantitatively compared using the mean bias, mean absolute error, root mean squared error, and coefficient determination. after these more simple test case are done, the same test and evaluation were performed using all of the available input feature for both the classification and the regression problem, and the resulting data wa included. for the regression problem specifically, the model didn perform quite a well, so some time is also spent discussing the tuning of hyper parameters. the author attempt to address the black box issue by spending some time discussing how the data can be interpreted, by asking key question like what input feature most influenced this decision and do the pattern learned by the machine learning model follow meteorological expectation . the author then brings up permutation importance and accumulated local effect a two technique that can address the question mentioned. alongside siting relevant sources, the author briefly discusses these two techniques, with permutation importance evaluating the impact of a given variable by judging how much the evaluation metric of the model change when the given input variable is shuffled. accumulated local effect ale on the other hand serve to quantify how minor change in input feature correspond to change in the model output. linear regression is specifically used in attempting to predict the number of lightning flash inside of an image. it is mentioned a being a useful starting point due to it simplicity a well a it history of use in meteorology a a discipline. because of it simplicity, it form a good point of comparison for newer, and potentially more complicated regression machine learning algorithms.\\nquestion in the above passages, the paper had two task , one related to classification and other related to regression. so in both the cases, the author implemented a comprehensive approach. in the first task of classification, they initially train the multiple model , then try using them by giving different feature and finding out different metric like accuracy, auc and then once after that, they impose different technique related to interpretation called a accumulated local effects, permutation importance. thus these all evaluation procedure having multiple level ensures a thorough understanding of the performance of the model and find error if any occurs to give a best accurate result. in the same way, to the regression task, the author first start with a simple model and one predictor. later it will try assessing the performance of the model with different metric like rmse, , bias etc and also can be seen via help of plot and graphs. then they implement the same to all the other predictor and analyze the different performance of various models. finally after this procedure, they conduct hyperparameter tuning and final testing using testing dataset. thereby, the author ensures that the used ml is robust and accurate. question the author address the black box ml concern by implementing different technique like use of permutation importance quantifying the significance of input feature by finding the metric when feature are randomly shuffled thus giving which feature is most influential in this prediction and ale help in knowing the relation between input and output by binning data and replacing the value with bin edge and finding mean difference thus knowing how change in input affect the model output and know the learned pattern are compatible with the expectation of the output .\\nquestion the author identifies concern with raw meteorological data, such a noise, missing values, imbalanced data, high dimensionality, and location specific trends. to address these, the author proposes preprocessing step like data cleaning, imputation for missing values, data balancing, feature engineering, normalization, and seasonal adjustments. these step aim to improve data quality, making it more suitable for training reliable and effective machine learning model in meteorological applications. question i found that the author us quantitative metric such a accuracy and precision for initial evaluation and employ fold cross validation for robustness. result are compared with traditional meteorological model to gauge efficacy. confidence interval measure prediction uncertainty, and real world testing assesses model reliability. interpretability is also emphasized for understanding prediction rationale. question according to the author, use of interpretable algorithm like decision tree and random forests.feature importance score to identify influential variables.linear model for straightforward interpretation.techniques like lime for neural network explainability. question in subsection .a, the author us linear regression a a straightforward, interpretable method for tackling a meteorological problem. they start by selecting relevant feature like temperature and humidity. the data is then split into training, validation, and testing set for effective model training and evaluation. the linear regression model is trained on these features, and it parameter are fine tuned using the validation set. finally, the model performance is evaluated using metric like mse, and the coefficient are analyzed to understand the influence of each feature. question according to my learning i found this book is useful in giving,foundational knowledge provides a strong base in machine learning concepts.step by step guidance walk through the entire machine learning pipeline.contextual relevance us meteorological example for direct applicability.interpretability focus address the importance of model transparency and trust.hands on experience offer code for practical, hand on learning.resource optimization discusses practical utility in resource allocation.future direction give insight into upcoming challenge and opportunities. question the concept that i found new and challenging arethe concept of feature importance, particularly in the context of complex model like random forest and gradient boosting.various data preprocessing step like imputation, data balancing, and normalization and the concept of fold cross validation for robust model evaluation\\nquestion the process to evaluate and validate is through a comprehensive approach that includes several step a below performance metric the author use various performance metric to assess the result of their machine learning models. for classification task , they consider accuracy, the area under the roc curve auc , and performance diagrams, which provide insight into model performance beyond accuracy. for regression tasks, they use metric such a mean bias, mean absolute error mae , root mean squared error rmse , and coefficient of determination to quantify the accuracy and predictive power of their models.one to one plot for the regression task, the author employ one to one plot to visually assess the model performance. these plot show the correspondence between predicted and true values. a perfect model would have all data point aligned along the diagonal.hyper parameter tuning in the regression task, the author perform hyper parameter tuning on the random forest model. they systematically vary parameter such a the maximum depth of tree and the number of tree in the forest. they evaluate the impact of these hyper parameter on model performance using the defined metrics.interpretability technique to address the black box nature of machine learning models, the author employ interpretability technique like permutation importance and accumulated local effect ale . these technique help them understand which input feature are most important for model prediction and investigate whether the model pattern align with meteorological expectations.model comparison throughout the evaluation, the author compare the performance of different machine learning methods, such a logistic regression, random forest, gradient boosted trees, and support vector machines. they identify the best performing model based on various metrics.discussion and interpretation the author interpret the result in the context of meteorological knowledge. they assess whether the important predictor identified by the model align with known meteorological principle or if there are discrepancy that require further investigation.in summary, the author employ a combination of quantitative metrics, visualization tools, hyper parameter tuning, and interpretability technique to rigorously evaluate and validate the result of their machine learning algorithms.\\nquestion the author address the concern of black box machine learning by employing several technique to improve ml explainability permutation importance the author use permutation importance to understand the importance of input feature in their machine learning models. by shuffling feature value and observing the impact on model performance, they identify which feature are most influential in making predictions. this help provide transparency into feature importance.accumulated local effect ale ale is another technique used to investigate the relationship between input feature and model outputs. it quantifies how small change in input feature affect model predictions. ale allows the author to gain insight into how the model make decision based on specific feature values.feature interpretation through permutation importance and ale, the author interpret the significance of input feature and assess whether the model pattern align with meteorological knowledge. they analyze whether the most important predictor make sense in a meteorological context.model comparison the author compare the performance and interpretability of different machine learning methods, such a logistic regression, random forest, gradient boosted trees, and support vector machines. they select model that not only perform well but also offer insight that align with meteorological expectations.by employing these techniques, the author strive to make their machine learning model more interpretable and transparent, addressing the black box issue and ensuring that model prediction can be understood in the context of meteorological science.\\nquestion the linear regression approach is applied to the regression task of predicting the number of lightning flash within an image. here how the linear regression approach is used single feature model initially, the author consider a simple scenario where they use the minimum infrared brightness temperature th a the sole predictor for the regression task. th serf a a proxy for the depth of storms, which is related to lightning formation.assessment of single feature model the author assess the performance of this single feature linear regression model. they evaluate how well it predicts the number of lightning flash within image based on th. they use metric such a mean bias, mean absolute error mae , root mean squared error rmse , and coefficient of determination to quantify the model accuracy and predictive power.challenges with single feature model they observe that the single feature linear regression model face challenges, especially for image with fewer than flashes. linear methods, in this case, tend to exhibit strong over predictions.complex scenario with all feature to improve model performance, the author transition to a more complex scenario where they use all available feature a predictor for the regression task. this expanded feature set aim to provide more information to the model and enhance prediction accuracy.evaluation of complex model using all available features, the author evaluate the performance of various regression methods, including linear regression, decision tree, random forest, gradient boosted trees, and support vector machines. they compare these model using metric like mae, rmse, and to determine which method performs best.hyper parameter tuning in the case of the random forest model, the author perform hyper parameter tuning by systematically varying parameter like the maximum depth of tree and the number of tree in the forest. they evaluate how different configuration impact model performance.in summary, the linear regression approach is initially applied to create a baseline model using a single feature th and is then compared to more complex model using all available feature to predict the number of lightning flash in images. the author use a range of evaluation metric and technique to assess the effectiveness of linear regression and other regression method in this context.\\nthe abstract introduces the theme and the problem the author are trying to address. the paper will introduce several novel hypothesis and argument in machine learning education a it applies to meteorology students. the paper demonstrates several regression model and statistical method and attempt to educate meteorology student on them even providing the python script used. machine learning in some way run counter to the predictive and demonstrative nature of meteorology. machine learning regarding forecast can be reliable and accurate interestingly, but they are not always consistent with training data. meteorologist cannot necessarily trust black box model that don take into account their prior knowledge, so the paper attempt to make machine learning more accessible to them. the paper essentially distills the first few lecture down to the key point needed, such a feature and label and how they are used in supervised learning. the paper explains the optimization paradigm and how parameter are used to limit a predefined loss function. the paper explains how those label and feature are used to train towards a target, and how it mathematically can be represented a a vector. for meteorologists, the biggest thing for them to know are that they can have either regression or classification model used to forecast the weather.the datasets being used come from radar and satellite datasets in particular the storm event imagery dataset and the nexrad. the application aim to observe sevir used to train. a model to determine thunderstorm and how many lightning flash are in an image potentially for use on satellite without lightning. the author use storm image data from sevir a well a nexrad and glm lightning mapped data. the author aim to use a feature input of storm characteristic such a magnitude of reflectance and coldness of brightness temperature in the water vapor, and these can be used to generate output statistics. the author describes the training dataset a the largest subset of data that will be generated and this is so that the model can generalize to a wide variety of examples. the cleaning of a dataset will take up the majority of time because spurious or noisy data make a poor ml model. the test dataset used to validate will be set aside until after all model have been trained, and the validation data will be incredibly specific in order to train the dataset. a method that can be used is fold cross validation which is a resampling methods. that can be used in the event of less robust statistics. the author concern included the composition of sevir data, and how exactly to split it. they also wanted to avoid introducing bias in the data with image that only contained thunderstorms. they decided to do a resampling method known a fold cross validation because the data could potentially be skewed. they got a very large training data set, and then smaller validation and testing set a well. they evaluated the model by using area under curve auc and receiver operating characteristic curve roc methods. they used the roc and obtained the metric by comparing the probability of false detection and the probability of detection to determine the overall true to false positive ratio of the dataset. they also used a threshold for prediction accuracy. another way wa to not use the true negative which can heavily bias machine learning model in meteorology. they used smaller datasets to make them easier to interpret, used extremely specific default hyper parameter to evaluate the algorithms, and they also used the backward permutation importance test to make the most important metric more apparent. linear regression wa used because it is simple and efficient, and it wa used in conjunction with others such a decision tree and random forest to compare how each variable wa observed to behave. linear regression actually performed near the worst with single feature with bias in how it perceived flashes.this paper wa an example of applied machine learning where specialist outside computing can actually understand it. it also demonstrates theoretically how machine learning is done, and it explains how datasets need to treated, how to validate and test at a very basic level, some basic statistic used and involved in the model, and finally wa able to be used to classify a novel type of data from a large dataset so new user can observe how machine learning can benefit them. this paper wa fairly well written and somewhat straightforward, but i am still confused a to how all the different regression model behave and what exactly they are comparing. i am also somewhat unsure of how to use many of the test they demonstrate in the paper to do analyze these models, and hope that i can build on a statistic background and understand them intuitively later on.\\ni knew about how training datasets worked and that you would need a testing set, but i did not know about validation set or rather the difference between the two. that is very interesting to me because sectioning the total data must be a bit of a grueling task, to ensure that each set of data is quality and unbiased. in addition, can there be multiple validation sets? would there be a need? this way of thinking is new for me.true negatives, or simply data that could dominate the training set is something i didn ever think of or know wa possible.not necessarily new but something that popped out to me. i understood ml application could use image a input and i assumed it wa pixel and their arrangement that identified things. in this case, the pixel count for each image would have been incredibly large and consequently inefficient for the application. using statistic on the pixel within the image to determine lightning flash and the number of them wa a cool idea i hadn thought of and make me wonder about what other type of ml apps can be simplified beyond just inputting something raw and outputting what you want. this tutorial did a great job at expanding the process deeper than high level.\\nwhen trying to optimize a linear regression model with training date, the goal is broadly to find a value for theta such that the rmse or mse since they lead to the same result is minimized. this can be done by using a closed form equation, the normal equation, to solve for the optimal theta value, which give our bias and coefficients.q gradient descent is another approach to optimizing linear regression model and it work by starting with random theta values, then selects the value with the smallest cost function. this process continues, with the algorithm taking iterative baby step toward smaller and smaller cost function value until a minimum is reached.q with the learning rate, the size of the learning rate is critically important because if the learning rate is too small, then the overall efficiency of the algorithm will be negatively effected, because a large number of iterative step will have to be taken to arrive at the minimum value. however, a visualized with figure , if the learning rate is too large, it actually possible for the algorithm to begin to diverge, moving in the wrong direction. likewise, feature scaling can also reduce the number of step that must be taken, a shown in figure . in this case, even without using feature scaling, the minimum will still be reached, but it will take more pass to get there.q batch gradient descent operates by computing a gradient vector which contains the partial derivative of the cost function, with this gradient vector then indicating which direction the algorithm should proceed in. batch gradient descent is slow because it us the entire training dataset for every step. stochastic gradient descent is the other extreme, and for each step, it only operates on a single, randomly selected instance of training data. this make stochastic gradient descent faster to execute, but it make the path toward the minimum much bumpier, rather than the consistent, steady descent seen with batch gradient descent. mini batch gradient descent essentially seek to compromise between these two extreme approaches. for each step, it operates on a set of randomly selected instance of training data, rather than just a single one like stochastic gradient descent.q linear regression us the standard mse formula a it cost function, whereas in contract, ridge regression add an extra regularization term to the mse while training, with the intent of improving fitting to the data, and also trying to minimize model weights. lasso regression also add a regularization term like ridge regression. however, with lasso regression, but the regularization term us a different formula for calculation in order to try and ensure that the optimal regularization value are independent of the size of the training data. elastic net regression attempt to combine the regularization term from ridge regression and lasso regression. elastic net regression feature a regularization term which is a blending of the ride and lasso regularization terms, where the weight of the blending can be controlled by an value. logistic regression us a different approach, with it being used to estimate the probability that an element is a member of a certain class. in other words, it can be used for categorical classification rather than just numeric estimation. because of this, it us a unique piecewise function with the goal being to find a theta such that high probability correspond to positive instance and low probability correspond to negative instances. finally, where logistic regression focused on binary classifiers, softmax regression is an attempt to generalize this approach so that more than two class can be supported directly.q according to the next, if there is a large gap between the training and validation error, this is an indication that the data is overfit, because it performing significantly better on the training data. one way this can be fixed is by feeding the model more training data. reducing the complexity of a model can also reduce overfitting by reducing variance, although this is a trade off because less complex model may have a higher bias. finally, overfitting can also be addressed by attempting to regularize the model. there are various way this can be done, but the text mention that one simple approach to regularize a polynomial model is to reduce the number of polynomial degrees.\\nthe linear regression can be trained on dataset to find the optimal parameter by finding the suitable value for model parameter vector bias term and feature weight so a to minimize the cost function mse or rmse is by using the normal equation xt.x .xt.y and singular value decomposition technique svd . the goal of gradient descent algorithm is to minimize the cost function by finding the gradient by randomly initializing the model parameter and taking step along with the steep until it converges to the minimum by tweaking the model parameter iteratively and find the optimal parameters. the size of step defined by the learning rate hyper parameter is the important parameter of the gradient descent because if the learning rate is too small, the model ha to go many iteration to converge to minimum point and take a lot of time and if the learning rate is too high , it may jump across the valley and might reach to extreme height than you were before and thereby failing to find the optimal solution. so, therefore the choosing the better learning rate that is not too low and too high is necessary. the feature scaling will also effect the performance of gradient descent a the time taken by model will be high and therefore all the feature need to be scaled similar using standard scaler before using it. instead of calculating partial derivative individually like in gradient descent, in batch gradient descent we can find them in one go like gradient vector for each model parameter by considering entire training set at each gradient step so that why it is called batch gradient descent. it is very slow on large datasets a time to find gradient will be high a we take entire data at each step.unlike batch gradient descent, the stochastic gradient descent us an instance of training set to calculate the gradient at each step which make it better to work faster with larger datasets since one instance need to be in memory at each iteration. the final parameter are good but not optimal a due to random nature once the algorithm stop it may bounce back to never settling down.unlike stochastic gd, the mini batch gd use a batch of random set of instance to compute gradient at each step which help in performance boost while using gpu and the algorithm progress parameter space is less erratic when compared to stochastic gd the difference between all the type of regression is like on using regularization term on the cost function to generalize the solution and faster. the linear regression doe not include any regularization for cost function, ridge regression us regularization, lasso us regularization, elastic regression us a mix ratio of and regularization, logistic regression us probability of instance parameter and log loss function and at last softmax regression us cross entropy function for multiclass classification and us either or regularization technique with respective cost function. these regularization should be given while training the model. generally, the gap between training error and validating error bias, variance and trade off define how well the model performs on training and validation data. if the gap is too small and both curve are too close probably the model is underfitted and any additional adding of data will not help instead we need to use complex and better feature and if the gap is too large then the model is overfitted like it doe well on training but doe not perform well on validation data and to make it better we need to serve more data a input until it reach the training error.the three way mentioned are use regularization technique early stopping increase input data\\nto train a linear regression model you have to set it parameter so that the model is best fit to the training set. the first step to being able to do this is measuring how well the model fit the data, and a commonly used method of measurement of this is the root mean square error. you must find the value of theta that minimizes the rmse. the equation to calculate this value is called the normal equation, and in it, is the vector of target value containing to . the bias term is included to factor in if the data distribution is centered around a value offset from the origin, and the coefficient multiply the predictor values. since they are multipliers, they give you the size of the effect the variable ha on the dependent variable, and the sign on the coefficient tell you direction. the goal of gradient descent is to iteratively adjust parameter so that you can minimize a cost function. you can do this by finding the local gradient of the error function in term of the parameter vector theta and going in the direction of the descending gradient until you reach zero the minimum . initially, you fill the theta value with random value random initialization and then you gradually improve it with each baby step being equivalent to an attempt to decrease the cost function until the algorithm converges to a minimum. gradient descent is guaranteed to approach arbitrarily closely the global minimum. the learning rate hyperparameter determines the size of the steps, and if it is too small the algorithm will have too many iteration to go through to converge. if it is too high, you risk the algorithm diverging and failing to find a good solution by overshooting. cost function can have a variety of look also, with holes, ridges, plateaus, etc. this can make converging to a minimum difficult. if the feature have drastically different scales, the bowl that is the cost function can become elongated. feature scaling cut down the time it take to reach the minimum by allowing the algorithm to take a direct path. batch gradient descent utilizes the whole dataset which result in an accurate result but at an expensive cost. stochastic gradient descent only us a single point which make for a fast result, but one that is a lot less accurate. mini batch gradient descent take smaller group of subset of the dataset which allows for a quicker result but also cheaper than considering the dataset in it entirety, and it ha pretty good accuracy. mini batch is sort of the middle of the road in between batch and stochastic, it combine the efficiency with the accuracy at a low cost. linear regression minimizes the squared difference between predicted vs. actual values. ridge regression us the linear regression cost function but add a regularization term. lasso regression considers the size of the number included in prediction making, taking out one that aren significant so that they don have input on the result and leaving only the valuable variables. elastic regression ha feature of both ridge and lasso in the way that it considers insignificant variables, setting them to zero or simply reducing them a necessary. logistic regression take variable that are categorical and determines the probability of a certain outcome binary and try to minimize error in classification. softmax regression us a function to predict the class with highest estimated probability score , this is similar to the logistic regression cost function.\\nwe can use rmse to find if a model is the best fit for the data or not. we have to find the best value for the parameter theta i.e. coefficient and bias. we can find the best value for the parameter using two method i.e. closed form equation and gredient descent method. the first method computes the best value for the parameter and the gradient descent change the value of the parameter to minimize the cost functionq the goal of gradient descent is to find the best value for the parameter so that that the cost function is minimal. this is achieved by changing the value of the parameter in such a way that the cost function is minimal. the cost function can be for example mse. gradient descent employ a learning rate, the learning rate decides the step size.q learning rate is too small then algo will go through many iteration and will take a long time.if the learning rate is too high it might make the algorithm diverge with larger values. when both feature have the same scale the gradient descent reach the global minimum very quickly but when the feature have varying scale then the gradient descent will take a very long time to reach the global minimum thereby increasing the length of the optimization process.q in batch gradient descent the algorithm us the entire batch of training data at every step of the process this make it slow on very large training sets. the stochastic gradient descent pick a random i.e. stochastic instance in the training set and generates a gradient, although this work very well on large training sets, since it is random, the generated parameter will always be good but not the best. the mini batch gradient descent neither take a single instance a in stochastic gradient descent nor take the whole training set a in batch gradient , but it take small random set of instance called mini batches.q difference among linear regression, ridge regression, lasso regression, elastic regression, logistic regression, softmax regression in term of cost functionlinear regression us mse typically, ridge regression add a regularization term to mse so that the learning algorithm will not only fit the data but also keep the model weight a small a possible in lasso regression the norm is multiplied by a.in elastic regression the regularization term is a weighted sum of both ridge and lasso regularization terms. in logistic regression, the cost function of a single training instance give high probability for positive instance and low probability for negative instances. softmax regression us cross entropy a it cost function.q reducing the complexity of the model, this can be achieved by decreasing the degree of the polynomial. by applying the regularization technique such a and . increasing the training set size\\nquestion linear regression involves modeling the relationship between a dependent variable and independent variable using a linear equation. the process seek the best fit parameter by defining a cost function, typically the mean squared error, to measure the model accuracy. optimization techniques, such a the normal equation or gradient descent, are then employed to minimize this cost function, resulting in the optimal coefficient and bias for the model.question the goal of gradient descent is to iteratively adjust model parameter to minimize a cost function, typically representing the difference between predicted and actual values. in the context of linear regression, the cost function often used is the mean squared error mse . gradient descent work by computing the gradient of the cost function with respect to each parameter and then updating the parameter in the direction that reduces the cost function. this process is repeated until the cost function converges to a minimum value. by doing so, gradient descent find the optimal parameter coefficient and bias that best fit the training data for the linear regression model.question linear regression us the mean squared error mse to measure the average squared difference between prediction and actual values. ridge regression add an regularization term to the mse, penalizing large coefficient to prevent overfitting. lasso regression incorporates an regularization term, which can lead to some coefficient being exactly zero. elastic regression combine both and regularization terms. logistic regression, designed for binary classification, employ a log loss cost function based on predicted probabilities. softmax regression, suitable for multi class classification, us a cross entropy loss to compare the predicted probability distribution across class with the actual distribution.\\nquestion linear regression can be trained on a model on a dataset which is named a training dataset to find the best parameter coefficient and bias using two method normal equation this method give a closed form solution to find optimal parameters. it minimizes the mse error between the value in the training dataset. once the dataset with input and target output are given, then a bias is added to feature matrix. then can find the parameter using an equation. but this method work best for small and moderate size of datasets. but when large sized dataset , then model becomes slow and intensive in case of storage or memory.gradient descent method it is an algorithm of iterative optimization to find the optimized parameter by reducing the function of cost, which is mse. it is best method for large datasets with many input variables. initialize the vector parameter with zero or any random values. then update those value by taking the steepest descent of cost function. then update the value for each iteration. repeat step till we get minimum value of cost function. thus there are different type of gradient method like bach, mini batch etc which ha their own advantages.\\nlinear regression is a technique used to find the best fit line for a set of data points. to do this, we calculate a cost function, mean square error mse , and root mean square error rmse . these help measure how well our line fit the data. we aim to minimize these errors.we don just pick one feature based on error. instead, linear regression considers all input feature to find the optimal parameter for the line that best represents the data. it not about choosing one feature but finding the best overall fit for all features.q gradient descent is a technique in linear regression that help improve the model by fine tuning it parameter to reduce errors. it begin with initial values, computes errors, and iteratively refines the value until it discovers the optimal parameter for the model. the specific gradient descent formula may vary for different algorithm or models.q the size of the learning rate and feature scaling are crucial in gradient descent. learning rate is like the size of step we take during the optimization process. if it too small, the process is slow and might take a long time to reach the best solution, especially with lot of data. if it too large, we might overshoot the optimal point and get incorrect results. so, we have to choose it carefully based on our data. feature scaling is about making sure all our input feature have a similar range of values. this help gradient descent work better because it won get confused by feature that have very different scales. when the feature are in the same range, the optimization process is smoother and faster, and it take fewer step to find the best solution.q gradient descent bgd is a traditional approach that us the entire dataset for each iteration. it slow and less efficient, especially with large datasets, because it ha to process all the data every time.stochastic gradient descent sgd take just one random data point for each iteration. this make it faster, but it can be noisy and less stable because it doesn consider all the data in each step. it might not always follow the ideal path.mini batch gradient descent mgd is a compromise between bgd and sgd. it selects a small random subset mini batch of the training data in each iteration. this reduces noise compared to sgd and can be adjusted based on available memory. it a practical choice for many situations.q linear regression typically us mean squared error mse a it cost function.ridge regression add an extra term to the linear regression cost function. this help in making the value closer to the global minimum. it mainly useful for regression models.lasso regression add another term to the linear regression cost function, which is helpful in eliminating value that are less important or close to the global minimum. it mostly used in classification models.elastic net regression combine both the extra term from ridge and lasso regression. it beneficial when dealing with unknown data and addressing issue like multicollinearity and feature selection.logistic regression us log loss or cross entropy loss a it cost function. it used for binary classification task where there are two classes, like and . it can also be used for categorical classification, not just numeric estimation.soft max regression is used for multiclass classification tasks, where there are more than two class involved.q polynomial regression is useful for modeling non linear relationship between variables, offering more flexibility than simple linear regression. however, when you increase the degree of the polynomial make it more complex , it add more feature to the model. this complexity can lead to overfitting, where the model fit the training data very closely but performs poorly on new data because it too closely tied to the training data quirks.polynomial regression is most helpful when used with a lower degree, a it strike a better balance between capturing pattern in the data and avoiding overfitting.\\nthe learning rate is a hyperparameter that control the size of the step taken by the gradient descent algorithm. the size of the step is proportional to the slope of the cost function. a the cost function approach it minimum, the step become smaller. if the learning rate is too small, the algorithm will take many iterations, leading to more time to converge to the minimum. if the learning rate is too large, the algorithm may overshoot the minimum and fail to converge.for the mean squared error mse cost function, if the learning rate is not too high, the gradient descent algorithm is guaranteed to converge to the global minimum, given enough time. however, if the learning rate is too high, the algorithm may diverge and fail to converge.the optimal learning rate will depend on the specific dataset and the cost function being used.feature scaling can significantly affect gradient descent. without it, feature with large scale can dominate the optimization process, leading to slow convergence and difficulty in finding the optimal parameters. it ensures that all feature have similar scales, allowing gradient descent to converge more quickly and efficiently. it help the algorithm navigate the cost function landscape more smoothly and prevents numerical instability.\\nin order to train a linear regression model we need to find the value of that minimizes the mean square error mse on the training set. basically we need to adjust the model parameter step by step to reduce the mse cost function. there are some method to do this one is called the normal equation, another option is gradient descent. the goal of gradient descent is to minimize a cost function by adjusting the model parameter in the direction that lead to the decrease, in the cost. in linear regression the cost function is known a mean square error mse . to apply gradient descent to linear regression we must calculate the gradient of the cost function for each model parameter . the size of the learning rate hyperparameter affect the speed of convergence of the gradient descent algorithm. if the learning rate is small, the algorithm will take along time to converge, while if it is large, the algorithm may overshoot the minimum and fail to converge in batch gradient descent, we take the entire dataset and then calculate the cost function and update parameter.in the case of stochastic gradient descent, we update the parameter after every single observation made andin the case of mini batch gradient descent, we take a subset of data and update the parameter based on every subset linear regression, ridge regression,and elastic net regression are type of linear model that have cost functions. linear regression us the mean squared error mse a it cost function while ridge regression add a regularization term to mitigate overfitting. if you observe a difference, between the training error and the validation error, in polynomial regression it is likely that the model is overfitting the training data. here are three approach to address this issue regularization,early stopping,gather more amount of training data\\nbatch gradient descentbatch gradient descent is a method for minimizing a cost function by iteratively adjusting the model parameter in the direction of the negative gradient of the cost function.the gradient of the cost function is a vector that point in the direction of steepest ascent of the cost function. by moving in the opposite direction of the gradient, we can gradually reduce the value of the cost function.in batch gradient descent, the gradient of the cost function is computed using the entire training dataset in each iteration. this make it the most accurate of the three methods, but also the slowest.stochastic gradient descentstochastic gradient descent is a method for minimizing a cost function by iteratively adjusting the model parameter in the direction of the negative gradient of the cost function, using a single training example at a time.this make it much faster than batch gradient descent, but it is also less accurate, a it can be affected by noise from individual training examples.in stochastic gradient descent, the gradient of the cost function is computed using a single training example in each iteration. this mean that the gradient can be noisy, and the model parameter may not converge to the optimal solution.mini batch gradient descentmini batch gradient descent is a method for minimizing a cost function by iteratively adjusting the model parameter in the direction of the negative gradient of the cost function, using a small subset of the training dataset in each iteration.this is a compromise between batch gradient descent and stochastic gradient descent, offering a good balance of accuracy and speed.in mini batch gradient descent, the gradient of the cost function is computed using a small subset of the training dataset in each iteration. this reduces the noise in the gradient, and the model parameter are more likely to converge to the optimal solution.\\nan linear regression can be trained on the training dataset to find the optimal parameter coefficient and bias in the following way using a direct closed form equation that directly computes the model parameter that best fit the model to the training set i.e., the model parameter that minimize the cost function over the training set . using an iterative optimization approach, called gradient descent gd , that gradually tweak the model parameter to minimize the cost function over the training set, eventually converging to the same set of parameter a the first method. an gradient descent is an iterative optimization algorithm that update the parameter in the direction of steepest descent of the cost function. the general idea of gradient descent is to tweak parameter iteratively in order to minimize a cost function. it measure the local gradient of the error function with regard to the parameter vector , and it go in the direction of descending gradient. once the gradient is zero, you have reached a minimum! concretely, you start by filling with random value this is called random initialization , and then you improve it gradually, taking one small step at a time, each step attempting to decrease the cost function e.g., the mse , until the algorithm converges to a minimum an learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function. if the learning rate is too small, the algorithm will take a long time to converge, while if it is too large, it may overshoot the minimum and fail to converge. feature scaling is a technique used to standardize the range of independent variable or feature in the input data. this technique can help gradient descent converge faster by having each of our input value in roughly the same range. this is because will descend quickly on small range and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variable are very uneven.ans in batch gradient descent, we use all our training data in a single iteration of the algorithm.so, we first pas all the training data through the network and compute the gradient of the loss function for each sample. then, we take the average of the gradient and update the parameter using the computed average.the main problem with batch gradient descent is the fact that it us the whole training set to compute the gradient at every step, which make it very slow when the training set is large. at the opposite extreme, stochastic gradient descent just pick a random instance in the training set at every step and computes the gradient based only on that single instance. obviously this make the algorithm much faster since it ha very little data to manipulate at every iteration. it also make it possible to train on huge training sets, since only one instance need to be in memory at each iteration sgd can be implemented a an out of core algorithm. batch and stochastic gradient descent at each step, instead of computing the gradient based on the full training set a in batch gd or based on just one instance a in stochastic gd , mini batch gd computes the gradient on small random set of instance called mini batches. the main advantage of mini batch gd over stochastic gd is that you can get a performance boost from hardware optimization of matrix operations, especially when using gpus.ans linear regression linear regression is a statistical method that is used to establish a relationship between a dependent variable and one or more independent variables. the cost function for linear regression is the mean squared error mse , which measure the average squared difference between the predicted and actual values. ridge regression ridge regression is a regularized form of linear regression that add a penalty term to the cost function. the penalty term is proportional to the square of the magnitude of the coefficients. this help to prevent overfitting and can improve the generalization performance of the model. lasso regression lasso regression is another regularized form of linear regression that add a penalty term to the cost function. the penalty term is proportional to the absolute value of the magnitude of the coefficients. this ha the effect of shrinking some coefficient to zero, effectively performing feature selection. elastic net regression elastic net regression is a combination of ridge and lasso regression. it add both and regularization term to the cost function. this help to balance out their respective strength and weaknesses. logistic regression logistic regression is a statistical method used for binary classification problems. the cost function for logistic regression is called the log loss or cross entropy loss. it measure the difference between the predicted probability and actual labels. softmax regression softmax regression is an extension of logistic regression that is used for multi class classification problems. the cost function for softmax regression is also called the cross entropy loss. it measure the difference between the predicted probability and actual labels. an if we get a large gap between the training error and the validation error, it mean overfitting. this happens when the model fit the training data too closely, capturing noise and fluctuation in the data rather than the underlying patterns. a a result model performs well om training data but poorly on unseen data. it can be solved by the following way . reduce number of polynomial degree . feed more training data . . apply regularization technique like ridge and lasso regression. these technique add a penalty term to the cost function that discourages large coefficient values. this help prevent the model from fitting the noise in the data and emcourages it to focus on the, most important features.\\n. batch gradient descent bgd us the entire training data set to compute the gradient and thus the next optimization to make at each iteration, which lead to a slower run time. bgd is also vulnerable to irregular cost function where we could optimize to a local minimum rather than the true global minimum.stochastic gradient descent sgd take the opposite end of the issue of using the entire training data set that bgd suffers from by instead choosing a random instance or datapoint in the training set for each step, which decrease overall runtime and allows for sgd to be used on extremely large data sets. however, the convergence of sgd is less regular than bgd and doe not follow a consistent path to the minimized cost function, and this can also result in never finding the perfectly optimal value for the cost function. this method is more beneficial for irregular cost function a it give a better chance of finding the global minimum than bgd doe for irregular cost functions.mini batch gradient descent mini bgd take the best of both world approach by using a small set of instance gaining the benefit of a larger training set to optimize off of for each instance while also keeping enough variation to avoid local minimum and maintain good efficiency speed and fitness for larger data sets.\\nquestion linear regression can be trained on the training dataset to find the optimal parameter in two different ways. the first is a closed form equation, the second is gradient descent. the closed form equation is called the normal equation. the normal equation find the value of that minimizes the mse or mean square error. the second way is through gradient descent. gradient descent is a generic optimization algorithm capable of finding optimal solution to a wide range of problems. it measure the local gradient of the error function with regard to the parameter vector . question the gradient descent goal is to measure the local gradient of the error function with regard to the parameter vector , and it go in the direction of the descending gradient. you start with random initialization, then improve it gradually with each step. in each step, you want to decrease the cost function or mean squared error, until the algorithm converges to a minimum.question the learning rate is basically the time it take for each step in gradient descent to happen. if the rate is too small, then that mean the algorithm would take too many iteration to converge, therefore taking a longer time. but if the learning rate is too large, you may cause the algorithm to diverge, a you may go beyond your desired results.question batch gradient descent requires the whole training descent to compute the gradient at every step, which make it slow when you have a large training set. stochastic gradient descent puck a random instance in the training set at every step and computes the gradient based on that only single instance. this make the algorithm much faster, and it allows you to use it on very large data sets. however, the stochastic bounce around, and only decrease on average, and it will never settle down, so the final result will be good but not optimal. mini batch gradient descent is almost a combination of the two. instead of using the entire dataset, mini batch gradient descent grab a small random set of instance called mini batches. the main benefit of this method is that you get a performance boost from hardware optimization. it is also less erratic than with stochastic gradient descent, so it is able to reach a bit closer to the minimum than that of stochastic, but it is harder for it to escape from local minima.\\nquestion batch gradient descent computes the gradient of the cost function with regard to each model parameter. to find how much the cost function change with little change to the model parameter, we need to find the partial derivative. all the partial derivative can be computed in one go and stored in the gradient vector. the calculation are over the full training set at each gradient descent step. multiplying the gradient vector by the learning rate and then going in the opposite direction of the gradient vector will allow to determine the size of the step. batch gradient descent will take to the global minimum. stochastic gradient descent pick a random instance at each step to compute the gradient, making it faster than batch gradient descent. stochastic gradient descent is much less regular than batch, which help it leave local minimum but it can settle at the global minima. a way to improve this is to gradually decrease the learning rate. mini batch gradient descent computes gradient at each step using small random set of instances. this allows mini batch gradient descent to be more regular than stochastic gradient descent, but harder for it escape local minima. it will end up closer to the minimum than stochastic gradient descent.\\n. the difference between the various form of regression in term of cost function are a follows linear regression us a cost function known a the root mean square error or alternatively the more simple to optimize mean squared error function, which measure how well our model fit the training data by by minimizing the average squared distance between data point and the regression line. ridge regression is a regularized version of linear regression which add a regularization term to the mean square error cost function used in linear regression. by adding this term, the model weight are kept a small a possible while ensuring a fit to the data a well. it is also important to note that the regularization term is only added during training of the model but the unregularized cost function is used to evaluate model performance. lasso regression is an abbreviation for least absolute shrinkage and selection operator regression and is another form of a regularized linear regression. lasso regression also add a regularization term but instead of a square of the norm and multiplication by alpha lasso regression us the norm unsquared and multiplies it by alpha which help ensure the optimal alpha value is independent of the training set size.elastic regression or elastic net regression take a middle ground approach between ridge and lasso regression. elastic regression is again a normalized linear regression, and for the regularization term it is simply a weighted sum of the ridge and lasso regression regularization term where we can control the mix ratio to balance between the two. elastic is typically preferred over lasso when the number of feature is larger than the number of training instance or if many feature are correlated strongly. logistic regression is more commonly used for classification rather than fitting a model like the above form of regression. logistic regression is used to estimate the probability of an instance belonging to a class, and if this probability exceeds a threshold then the model predicts the instance is a member of a given class. logistic regression similarly computes a weighted sum of the input feature but it output the logistic of each result rather than a direct result. the logistic of a result is a sigmoid function outputting a number between and which is then compared to the threshold value to decide whether an instance is a member of a given class or not. softmax regression is similar to logistic regression in that it is used for classification, but rather than being a binary classifier softmax regression can support multiple classes. softmax regression first computes a score for each class and then estimate the probability that an instance belongs to each class by applying the softmax function which computes the exponential of each score and then normalizes the scores, lastly choosing which class the instance belongs to by using the highest estimated probability, i.e., the class with the highest score for the given instance.\\n. from chapter , describes how the linear regression can be trained on the training dataset to find the optimal parameter coefficient and bias . a a linear regression model is a supervised machine learning technique and it is is used for modelling and predicting the relationship between a dependent variable the target and one or more independent variable the feature . obtaining the optimal coefficient and bias termed a the intercept which minimizes the error rate between the predicted value and the actual target value in the training dataset and is a goal of training a linear regression model. ordinary least square ols is a common method used for this operation. here is how to train linear regression to identify the ideal parameter linear regression lr is trained on or more independent and dependent feature with simple and multiple linear regression with learning rate , coefficients, it intercept and it cost or loss. during training our data can spread go to train and rest go to testing part. . from chapter , define the goal of gradient descent and how it can be applied to linear regression to find optimal parameters. gradient the rate of change of error is called gradientdescent it value can move towards either upwards or downwards or from one side to the other side on bell shaped curved figure or it can move simply in a zig zag manner .the main goal of gradient descent is to minimize the global minimum error rate to nearer zero but not zero and never come to zero in reality it a main theme of gradient descent the gradient descent is a first order iterative optimization to minimize the function and during training some parameter are passed to train.in other words, simply we can say that , it updated the error rate to all the neural network through back propagation after loss value calculation and it learns effectively each time it back propagates.q . from chapter , describe how the size of the learning rate and feature scaling would affect the gradient descent. learning rate it control each and every step of the process at every iteration. it tell how convergence varies quickly or slowly.if learning rate is too high , then divergence is failed to converge . which having lower prediction and it an unstable finally.if learning rate is low, convergence happens slowly to reach the minimum point . feature scaling affect the gradient descent by the following way a. feature scaling is a technique to standardize and normalize the given input features.b. scaling the feature play a very important role when value are at different scale to predict or finding pattern very easy when we scaled to particular range of value between to and to .c. during model training, it is necessary to experiment with various rate of learning and feature scaling methodology for the purpose find the one that generates the quickest and most reliable convergence. the success of machine learning model depends heavily on this procedure, which is also known a hyperparameter tuning. . from chapter , describe the difference between batch gradient descent, stochastic gradient descent, and mini batch gradient descent.batch gradient descent in batch gradient descent, the training sample or training data are considered to be one batch for each and every epochs.later these all batch be taken a an average to get our gradient and in return it will update to our parameters.stochastic gradient descent sgd it is the modification of gradient descent. the main problem with the batch gradient descent is slowness of computation that it computes every data point in the dataset.so that it came in to existence to compute very fast by taking a random instance in the training set and also calculates the gradient a well.mini batch gradient descent instead of computing the batch and sgd , the gradient are calculated based on some set of batch data points. each set is calculated efficiently through this computations. . from chapter , describe the difference among linear regression, ridge regression, lasso regression, elastic regression, logistic regression, and softmax regression, in term of the cost function. a. linear regression it is a supervised learning technique which predicted for continuous feature with multiple independent features. it find the better relationship regression line on input and output feature for better performance. by using metric like mse, mae, rmse, etc can be used while evaluating the model. b. ridge regression it is an regularization technique which is applied penalty for the linear regression model cost function and used for mainly to overfitting our model performance. it help to reduce the multicollinearity when we are having multiple independent features. c. lasso it is an regularization technique which is applied some penalty for the cost function in linear regression model. it a sparse coefficient and it helpful in feature selection . d. elastic both lasso and ridge are considered to be an elastic net regression, which add and to the loss function. these deal with feature selection and multicollinearity. e. logistic it is a classification problem with two type in a label a an output which will be used sigmoid function for output which is in between to , and ha one or more independent feature f. softmax softmax regression is a generalised one for the existing linear regression model where predicted for single class and softmax regression is predicted with multi class binary classification which ha more label to predict in a label class. . for graduate student, bonus for undergraduate student from chapter , suppose you are using polynomial regression, you plot the learning curve and you notice that there is a large gap between the training error and the validation error. what is happening? what are three way described in chapter to solve this? a. regularization for polynomial regression, regularization method like ridge and lasso can be used.ridge regression increase the cost function penalty component, therefore increase the model little coefficients. the polynomial model complexity is lowered a a result.whereas the penalty term added by the lasso regression method provides feature selection by decreasing some coefficient to absolutely zero while simultaneously reducing complexity.b. feature scaling by carefully choosing the most appropriate trait for your polynomial regression model, overfitting can frequently be reduced.you may reduce downwards and only include the most significant feature in your model by using strategy like forward selection, backward elimination, or recursive feature elimination rfe .the model can become more robust and less prone to overfitting by reducing the amount of features.c. reduce polynomial degree try reducing a high degree polynomial to avoid overfitting our training data. overfitting is common in polynomial with a very high degree.\\nquestion the difference among all the regression in term of cost function are linear regression it us mse cost function, which goal to reduce the squared difference in predicted value and actual target values. this cost function ha learning rate, number of iteration and coefficient or weight in the function.ridge regression this regression add a term called regularization to the cost function generated by linear regression. this help in eliminating the large coefficient values. it ha same parameter a linear regression along with regularization strength term.lasso regression this regression also us same mse function along with other regularization term called where this term help in sparse selection of feature and help in moving the coefficient to zero. this regression ha same parameter a ridge regression just the term used is different regularization.elastic net regression it take help of both and term of regularization and us mse cost function so that it can give a balance between selection of feature and can prevent multiple collinear points. it ha all term a lasso along with ridge regression strength term in it.logistic regression this regression us a cost function called a log loss cross entropy for classification of binary problems. it help in knowing the difference between the predicted value probability and actual classes. it ha same parameter a linear regression but equation differs. softmax regression it us a function called softmax cross entropy cost function for classification of problem having multiple class in it. it find the difference between the predicted class and the true class probability distribution of it.\\n. linear regression us the mean squared error mse a it cost function. the goal is to minimize the squared difference between the predicted and actual values. ridge regression add a regularization term to the linear regression cost function. the cost function is the sum of squared error plus a penalty term on the magnitude of the coefficients. lasso regression is like linear regression, but it look at the size of the number that help make predictions. if some of these number are not very important, lasso make them exactly zero, which mean they re not used for prediction at all. this make lasso useful for picking out the most important variable in a dataset.elastic net is like a mix of ridge and lasso. it look at the size of the number in a similar way to both of them. this mean it try to make some coefficient smaller like ridge and try to make some of them exactly zero like lasso. so, elastic net combine these two way of looking at the number to find the best feature in your data.logistic regression is used for the categorical values. it model the probability of a binary outcome and aim to minimize the error in classification.softmax regression extends logistic regression to multiple classes. it us the softmax function to compute class probability and minimizes the cross entropy loss.\\nlinear regression cost function linear regression us the mean squared error mse a it cost function. the mse measure the average squared difference between the predicted value and the actual value in the training data.purpose linear regression is used for predicting continuous numeric outcomes.ridge regression cost function ridge regression add an regularization term to the linear regression cost function. the cost function includes the mse term a well a a penalty term that discourages large coefficients.purpose ridge regression is used for linear regression task with the goal of preventing overfitting by reducing the magnitude of the coefficients.lasso regression cost function lasso regression incorporates an regularization term into the linear regression cost function. the cost function includes the mse term and a penalty term based on the absolute value of the coefficients.purpose lasso regression is used for linear regression task and feature selection. it encourages sparsity by setting some coefficient to zero, effectively selecting a subset of important features.elastic net regression cost function elastic net regression combine both lasso and ridge regularization term in the linear regression cost function. it includes both the mse term and a linear combination of and penalties.purpose elastic net is used for linear regression task when there may be both correlated and irrelevant features, and it provides a balance between and regularization.logistic regression cost function logistic regression us the log loss also known a cross entropy loss a it cost function. it measure the dissimilarity between predicted probability and actual binary or outcomes.purpose logistic regression is a classification algorithm used for binary classification tasks, where the goal is to predict one of two classes.softmax regression multinomial logistic regression cost function softmax regression employ the multinomial log loss a it cost function. it measure the dissimilarity between predicted class probability and actual class label in multiclass classification tasks.purpose softmax regression is used for multiclass classification tasks, where the goal is to predict one of multiple classes.\\nridge regression enhances the cost function by introducing an additional parameter to the sum of square called the regularization parameter. this parameter penalizes the size of the weight and attempt to keep them a small a possible. lasso regression also known a least absolute shrinkage and selection operator regression , doe not have a closed form and can not be applied to a gradient descent. it is a regularized version of linear regression, and it allows a coefficient that can be forced to zero. elastic net regression is an extension of the lasso regression. the weighted sum of the ridge and lasso regulation term contribute to the elastic net regression regularization term. this reduces many useless feature weight to zero. this regression also cannot be applied to a gradient descent. logistic regression work much like the linear regression. the high probability is and a low probability is . the cost function is the average over all the training instances, it is a single expression known a the log loss. this is not a closed form however a gradient descent can still be applied because the cost function is convex. softmax regression is used for multiclass classification such a multinomial logistic regression. like the logistic regression it convert the raw value into probabilities. the cost function associated with softmax is cross entropy.\\n. linear regression take a weighted sum of input feature plus a bias term. it need to be trained to specific parameter so the model fit training data. we do this by minimizing the root mean square error measurement, and we need to do this by finding the optimal value of theta, finding the ideal parameter vector and feature vector combination to reduce the rsme. . gradient descent is an optimization algorithm that can find the optimal solution iteratively. it doe this by modifying the parameter until the descending gradient of the error function of theta is . this is known a the minimum. . the learning rate is a hyperparameter that determines that size of the gradient steps. if it is too small, the gradient descent requires more step and decrease the efficiency of the model. feature scaling is another method that effect how the model learns, a with feature scaling, the algorithm reach the global minimum much quicker. . batch gradient requires a partial derivative calculation that need to be computed on entire dataset at once, and this give a gradient vector. the opposite direction of the gradient vector is the descent to the minimum. stochastic gradient take a derivative, or take a random point in a training dataset and and computes the gradient based on that particular instance. this is fast, can be used on large datasets, but it is much less predictable. mini batch computes the gradient vector based on small random instance in the dataset rather than the whole dataset at once, or a small random instance. this usually result in a gradient descent closer to the minimum than stochastic gradient descent. this serf a a good approximation and can reach the minimum in due time, but it is not guaranteed to reach the minimum like batch is. it is however, faster and can reach the minimum if given a good time. . linear regression ha a cost function that ha a convex shape, and the line will never fall below a curve and there ha one global minimum and no local minima. ridge regression is a regularized version of linear and can be thought of a a normalized version that fit the data and keep the feature weight a small a possible. it cannot be used during training and should be added to the end and verified with an rmse analysis of the performance. the cost function demonstrates an irregular theta value and a specific function that modifies the mse gradient vector . lasso least absolute shrinkage and selection operator regression add a different regularization term but it us the norm of the weight vector rather than a square of the norm. this eliminates the weight of the least important feature and output what can be called a sparse model with almost no nonzero feature weights. this creates a cost function with very few contour relative to the ridge regression. the optimal value is always independent from the size of the training dataset. elastic ha a much different cost function, a when the value is set to , the elastic net regression value is equivalent to ridge, and when it is set to , it is more like lasso. the regularization us both ridge and lasso regularization term and this is more reliable, according to the author, than linear regression. logistic regression is a measurement of the probability that an instance belongs to a particular class, a binary classification. the cost function is logarithmic and grows very large a the probability go to . the cost function will be close to if negative, and if positive, a binary function. softmax is also known a multinomial logistic regression, and it ha a unique cost function known a cross entropy. when there are only two classes, the cross entropy cross function is identical to logistic regression. otherwise, you can compute the gradient vector with regard to theta for the number of class and then find the parameter that minimize the cost function. . one way to fix a large error is to feed the model more training data until the error correspond. another way is to regularize the expression and try to regularize the model similar to what is performed in the other type of regression. you can reduce the number of polynomial degrees. a more complex model can stop being trained a soon a the validation error reach a minimum and then using gradient descent, the algorithm learns and the two error are much closer together.\\nquestion when you optimize the linear regression model you can train it using a closed form equation, or you can use gradient descents. the goal is to find the mean of the standard error mse . trying to solve for theta in the closed equation will lead you closer and closer to accurate coefficient and bias. question gradient descent is a technique that is used to find the minimum cost function through tweaking parameters. gradient descent go step by step and look for the minimum at each step. each step is called a learning step and the learning step can be adjusted to help optimize the program.question the learning rate affect gradient scaling a lot. you need to have the right learning rate so that you are able to find the min without it taking too long. if you make the learning step too small, your code will take forever to run. if you make it too big, you will keep jumping over the min.question batch gradient descent batch gradient descent us the whole dataset over and over again that will guarantee coverage at the min. one downside is that it is computationally and space costly.stochastic the stochastic gradient descends on the min but us a randomized algorithm to get there. due to the randomness, it is possible to never settle around the minmini batch mini batch is in the middle ground of memory and efficiency. it computes each gradient based off small random set of the full data set question linear regression linear regression us the standard mse formula.ridge regression ridge regression is a version of linear regression that add a regularization term and tried to minimize weight a small a possible lasso regression lasso regression is similar to ridge regression but us norm of the vector weightelastic regression elastic regression is a combo of ridge and lasso where the weight is a blend of both that is the variable r.logistic regression logistic regression us classes. a piecewise function is used to give a probability of what class each object is in.softmax regression logistic us binary classifier to determine class whereas softmax you can use multiple class that each point can be classified as.\\nlinear regression assigns a weight to each parameter value in order to fit a line to the data. this doe not work well for non linear data. in polynomial regression extra term are added with weight for the parameter to various powers. this can get a better fit for data that isnt linear , but it run the risk of overfitting. one method is ridge regression. in ridge regression the square of a lambda norm is added to the mse forcing the model to try to keep all of the weight low to reduce overfitting and normalize the curve. another method of doing the same thing is called lasso regression which doe not square the value it add to the mse. this method ha the effect of silencing less powerful feature which could be desired depending on circumstances. elastic regression is a way of combining the two by summing them. the ratio can be controlled by a variable r. logistic regression is an alternate form of regression which is used not so much for predicting value a classifying objects. it sum weighted parameter to come up with a binary probability. logistic regression can then be converted to make prediction across multiple class a oppossed to a simple binary prediction with softmax regression. this is done by computing a seperate score for each class and using the collection of score to estimate a probability.\\n. from the description in chapter , describe the step by step workflow to train a binary classifier for the mnist data. for instance, given an mnist data that ha images, how do you develop one ml model step by step to classify whether the image contains digit or not. an binary classifier step step create the target vector for the classification task.step pick a classifier and train it.in this case it the stochastic gradient descent sgd,or stochastic gd classifier. step train the classifier on the training dataset.step we can now use the classifier to detect image of the number.q . from the description in chapter , describe the general idea of the confusion matrix. why do we need to design confusion matrix instead of simply using accuracy metrics? besides, describe how the confusion matrix is computed in the binary classification problem.ans the general idea of a confusion matrix is to count the number of time instance of class a are classified a class b, for all a pairs.for example if we wanted to look at the number of time the classifier confused image of number a with number we can look at the row a ,column of the confusion matrix.we need to design confusion matrix instead of simply using accuracy metric because escpecially when dealing with datasets the accuracy might be wrong .for ex in a dataset where some class are much more frequent than others,the accuracy might mislead on giving the classifier performance.to compute the confusion matrix first we can use the cross val predict function to get a set of prediction on the test set,cross val predict performs fold cross validation and return the prediction made on each test fold.we can then import the confusion matrix from sklearn and implement it by passing the target class and the predicted class.q . . from the description in chapter , describe the definition for the following term true positive, false positive, true negative, false negative, precision, recall, and scoreans from the example with generating confusion matrix with the target class ,the definition would be true positive the number of time the image were correctly classified a sfalse positive the number of time the image were classified a strue negative the number of time the image were correctly classified a non sfalse negative the number of time the image were wrongly classified a non sprecision the ratio of true positive to true positive false positivesrecall the ratio of positive instance that are correctly detected by the classifier,this is used along with precisionf score the harmonic mean of precision and recallq . from the description in chapter describe the meaning of precision recall trade off and how the decision threshold would affect the precision and recall in binary classification. an precision recall tradeoff mean the phenomenon where increasing precision reduces recall and vice versa.from the example of the classifier when the decision threshold is positioned at the center the precision is percent the recall is percent .when the threshold is positioned on the right side increase the threshold we can see that the precision would go up to percent and the recall would go gown to percent and when the threshold is positioned to the left lower the threshold the recall would increase and the precison would go down.q . from the description in chapter ,describe the definition of the roc curve and auc score. besides, describe how the roc curve can be used to compare different classifier using roc auc.ans roc curve the reciever operating characteristic roc curve is a tool used with binary classifier ,it plot the true positive rate recall and the false positive rate fall out .here recall is the ration of positive instance correctly detected, and fall out is the ratio of negative instance correctly classified a positive.auc score area under curve auc score is a measure to compare classifiers.a perfect classifier would have a roc auc equal to and a purely random classifier will have roc auc equal to . .\\nto train and test a binary classifier using the mnist dataset where the set is small digit handwritten by high school student and employee to classify that a digit is or not, you can follow the following step step first you have to prepare the dataset and load it using the fetch openml and then know the content of the dataset.step clean the dataset and then split the dataset into two part one is training set and other is testing set.step change the data type a required and take the sample of the dataset like how the integer look.step set the input feature and target labels.step you have to then create binary label ie for the pixel that contains and for the pixel that contains nothingstep have to choose an algorithm for binary classifier and then train the given binary classifier using the training dataset. you can so this using fit method.step once the trained model is ready you have to test the data using testing dataset.step you can calculate the relevant metric of the data after testing the data set.step you can even adjust the hyper parameter if needed and also can test with other algorithms.step make prediction and visualize those prediction . if not predicted correctly change the parameter and reiterate this till we get the correct output.step if model work well and give correct output then finally you can deploy the model in any of the platforms.\\nstep the first step is to try to understand the problem that we are trying to solve. in this case, we want to train a number binary classifier for the mnist data.step download the mnist data using sklearns fetch openml function. for this dataset we should also set the a frame value to false since data frame are not a good at classifying image data.step visualize the data. looking at the array we can see that there are image and feature in this case pixel white or black . using matplotlib imshow function we can get a map in grayscale for a test image, they used in the example.step we create a test set and set it aside before inspecting further. this data returned by fetch openml is already split into a training set image and test set of k, and shuffled.step we then select a model. in this example, they decided to train a binary classifier and started with stochastic gradient descent. from sklear.linear model they import sgdclassifier and create an sgdclassifier object.step next we measure accuracy. we can compare the accuracy of our model cross validation score with a dummy classifier. since this is a skewed dataset, in that there is a relatively small number of for the binary case, we can use a confusion matrix. to compute the matrix we need prediction and compare them to actual target with cross val predict . since this model did not perform well we can try another model and look for better performing model.step once we have found a model with good performance, we can present the data to the model and train. we can once again measure the metric on our test set.step if we are happy with the model we can launch and monitor.\\nstep input mnist dataset, this dataset can be procured from scikit learn. because the data contains images, it is best to download it a numpy arrays. so for this we would use the command fetch openml and set a frame false so that it doesn return a panda dataframe. step create a test set and set it aside. fetch openml command automatically creates a test set containing the last , images. the training set includes the first , image and it is already shuffled to guarantee all cross validation fold will be similar. step create target vector to classify whether the image is a . true or false. this is our binary classifier.step train the binary classifier. the example classifier given is the stochastic gradient descent classifier because it is capable of handling very large datasets, and it deal with training instance independently.step evaluating the classifier using cross validation. accuracy measuring method are not ideal for this dataset so we will need to use a confusion matrix.step calculate precision and recall and select precision recall trade off that work for the data and use the roc curve area under the curve score to determine which classifier work best for the model.\\nquestion the confusion matrix summarizes the result of a binary or multi class classification problem by breaking down the prediction of the model into four category true positive, true negative, false positive, false negative. these category allow to assess the model performance by quantifying these different type of error and correct predictions. by examining these values, we can gain insight into the model strength and weaknesses. start with a matrix of size , where row represent the actual class label positive and negative , and column represent the predicted class label positive and negative . . go through each data point in your dataset and classify it using the model. . for each data point, update the corresponding cell in the matrix corresponding to all possible scenario in a binary classification problem a. if the model correctly predicts a positive instance, increment the tp count.b. if the model correctly predicts a negative instance, increment the tn count.c. if the model predicts a positive instance when it actually negative, increment the fp count.d. if the model predicts a negative instance when it actually positive, increment the fn count. . once you ve processed all the data points, you have the complete confusion matrix, and you can use it to calculate various performance metric like precision, recall, score, and specificity.\\nthe general idea is to count the number of time instance of class a are classified a class b. for example, to know the number of time the classifier confused image of with s, you would look in the th row and rd column of the confusion matrix. accuracy may not be a good measure if the dataset is not balanced both negative and positive class have different number of data instance . consider the following scenario there are people who are healthy negative and people who have some disease positive . now let say our machine learning model perfectly classified the people a healthy but it also classified the unhealthy people a healthy.the accuracy, in this case, is but this model is very poor because all the people who are unhealthy are classified a healthy. just pas it the target class train and the predicted class train pred each row in a confusion matrix represents an actual class, while each column represents a predicted class. the first row of this matrix considers non image the negative class , of them were correctly classified a non they are called true negative , while the remaining , were wrongly classified a false positive . the second row considers the image of the positive class , were wrongly classified a non false negative , while the remaining , were correctly classified a true positive . a perfect classifier would have only true positive and true negatives, so it confusion matrix would have nonzero value only on it main diagonal top left to bottom right\\nthe confusion matrix is an evaluating tool used to know the performance of the classification model.it actually represents a actual class where each column is a predicted one. it ha four category true positive they are object sample of positive class digit and given a positive by model truly.true negative they are not object sample of positive class digit not and given a negative by model truly.false positive they are actually class of negative but they are wrongly given a positive.false negative they are actually class of positive but they are wrongly given a negative.it is important because, the accuracy give ratio of the correct and total prediction but when coming to classes, the class with high accuracy will hav model performance low. so in that case, this matrix is useful. also it help in evolution of the other metric and provide a nuanced view of the model. also the matrix help in understanding the error and where the model give different. outputs.the step to use the confusion matrix are they make the predictions. using the given binary modelthen we compare the prediction with the actual labels. then we fill the confusion matrix with the given countsthen we calculate the metric of the model using formula for precision tp tp fp and accuracy tp tn tp tn fp fn , recall a tp tp fn and other such matrices.next if we need we can change the threshold and again compute the values.\\nquestion precision is a metric that measure the accuracy of positive prediction made by the model. it calculates the ratio of true positive correctly predicted positive instance to the total number of positive prediction true positive plus false positive . precision is concerned with minimizing false positives.recall, also known a sensitivity or true positive rate, measure the model ability to identify all relevant positive instance in the dataset. it calculates the ratio of true positive to the total number of actual positive instance true positive plus false negative . recall is concerned with minimizing false negatives. when you lower the decision threshold moving it closer to , the model becomes more lenient in classifying instance a positive. this typically increase the number of true positive because more instance are classified a positive. however, it also increase the number of false positive because some instance that were previously considered negative are now classified a positive.when you raise the decision threshold moving it closer to , the model becomes more conservative and selective in classifying instance a positive. this often reduces the number of true positive because fewer instance are classified a positive. however, it also reduces the number of false positive because the model is less likely to make positive predictions. the decrease in both true positive and false positive can lead to higher precision but lower recall.\\nbinary classification for digit step create target vector for the classificationy train train test test step stochastic gradient descentstep detecting image of sgd clf.predict some digit step cross validation to check our resultsstep dumb classifier and check it accuracyq the confusion matrix is used in binary classification and also to caluclate the precision,recall,f score.the confusion matrix is computed in the binary classification problem by the true positivies tp ,false positive fp ,true negative tn ,false negative fn true positive tp true positive is the number of instance correctly classified a positive.false positive fp false positive is the number of instance incorrectly classified a positive.true negative tn true negative is the number of instance correctly classified a negative.false negative fn false negative is the number of instance incorrectly classified a negative.precision tp tp fp tp number of false true positiviesfp number of false positivesrecall tp tp fn score precision recall precision recall precision recall trade offthe precision recall trade off tell the balance between the precision and recall.increase in precision will reduce the recall and vice versa.the higher the thresold, lower the recall and higher in precision.q the receiver operating characterstic roc is similar to precision recall curve but here we plot the curve between the true positive rate and the false positive rate.auc area under curve by measuring the auc we can compare the classifiers, a perfect classifier ha the rocauc is , ramdom classifier ha the rocauc is . we import roc auc score from sklearn.metricsroc auc score train ,y score\\nthe receiver operating characteristic roc curve is another common tool used with binary classifiers. it is very similar to the precision recall curve, but instead of plotting precision versus recall, the roc curve plot the true positive rate another name for recall against the false positive rate.one way to compare classifier is to measure the area under the curve auc . a perfect classifier will have a roc auc equal to , whereas a purely random classifier will have a roc auc equal to . . when you have multiple classifiers, you can create roc curve for each of them using the same dataset. by visually inspecting the roc curves, you can compare the classifier relative performance. a classifier with a curve that is closer to the top left corner is generally better. however, the roc curve alone might not be enough for a quantitative comparison. to provide a single scalar value that quantifies the overall performance of a classifier, we use roc auc. roc auc calculates the area under the roc curve, which represents the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. a classifier with a higher roc auc score is considered better at distinguishing between the positive and negative classes.\\n. step import and load the dataset, then integrate into your code by initializing it.step divide the dataset into the training and testing subsets. step prepare process the data so that it is clean and ready for the model. this involves possibly normalization, and scaling.step train the model using the training data. step compare using testing data, this is where you find the error.step repeat, learning a you go and minimizing the error at each possible moment.q . we need a confusion matrix to count the number of time instance of class a are classified a class for all a pairs. in order to calculate the confusion matrix you need to have a set of prediction that can be compared to the actual targets.each row in the matrix represents an actual class while each column represents a predicted class . true positive correctly classified a a false positive incorrectly classified a a true negative correctly classified a not a false negative incorrectly classified a not a precision the accuracy of the positive predictionsrecall a.k.a. sensitivity the true positive rate tpr the reaction of positive instance that are correctly detected by the classifierf score combine precision and recall into a single metric that assist in comparing two classifiersthe harmonic mean of precision and recallgives more weight to low valuesthe classifier will only get a high score if both recall and precision are highq . look at how the sgdclassifier make it decision regarding classificationfor each instance, it computes a score based on a decision function if this score is greater than a threshold, it assigns the instance to the positive class otherwise to the negative classyou can set the threshold and this will determine what is detected a a true or false positive or negativeraising the threshold decrease recallyou want to select a precision recall trade off just before the drop that occurs around recall. . roc receiver operating characteristic similar to precision recall curveplots the true positive rate recall against the false positive rate fpr fpr fall out is the ratio of negative instance that are incorrectly classified a positiveequal to the true negative rate tnr which is the ratio of negative instance that are correctly classified a negativethe higher the recall, the more false positive the classifier providesthe dotted line represents the roc curve of a purely random classifier a good classifier stay a far away from the line a possible toward top left corner auca perfect classifier will have a roc auc equal to , while a purely random classifier will have a roc auc equal to .\\ndecision tree are a way to build more complex model with higher classification accuracy due to allowing model to build off each other while also ensuring minimizing coefficient for optimal result in each tree node.q ensemble learning is a type of learning that is multiple layer of decision tree allowing for much more complex inference and even finer coefficient optimizations.q unlike standard cost functions, decision tree utilize random forest to overfit a dataset which ensures high feature fitting.q while gini impurity utilizes a sample proportion, entropy us a plogbase , proption. both method ensure effective split for each tree decision branchq max features, max leaf nodes, min sample split, min sample leaf, min weight fraction leaf such hyper parameter ensure the tree can grow a dynamically a is necessary to match the dataset it is being trained onq the key difference is the decision making process. although both utilize random bounds, the decision themselves are much more interal for a random forest and neural network than a decision tree, leading to them being classified a black box versus the white box nature of decision treesq the key difference is that while hard classifier make a prediction, a soft classifier provides a probability. this cause hard classifier to work a a majority vote system while soft classifier still incorporate the outliarsq bagging creates multiple sample from the original data and average the predictions, boosting creates layer of model with each learning from the previous creating a faster inference model but with more training time, and stacking is equivalent to a blend of the twoq a decision tree is a collection of node with each focusing on a feature while a random forest is a collection of decision trees. due to a random forest using multiple decision trees, high variance are countered by the multiple underlying model prediction that average out to remove the outliars.q due to the many underlying decision trees, the combination of these model cause feature importance to become a given thing. a each decision tree make each feature node, the emphasis on feature node is passed up and eventually combined in the random forest\\nthe gini impurity score, which is utilized in decision tree algorithm primarily for classification tasks, is a measure of impurity, a we learnt in chapter . the likelihood of every category within a decision tree node is taken into account before calculating the gini impure score for that node. in this case, if you had two category a, , then would divide the count of sample in each class by the overall amount of data in the node to determine the probability of each class. the odds are then squared for every group and added. finally, to determine the gini impurity score, remove this total from . a purer node with a smaller gini impurity score implies less impurity.an entropy score is another impurity measurement used in tree based decision algorithms, notably for classification problems, and ha similarity to the gini impurity. you first calculate the likelihood of every class within a node before computing the node entropy score. the entropy formula is then used, which entail taking the negative summation of these probability and multiplying it by each of their corresponding logarithms. typically, base is used when computing the logarithm to measure impurity in the form of bits. a lower entropy score, like the gini impurity score, denotes a more homogenous node with less chaos.\\nwe gained knowledge about a number of typical hyperparameters that are used to train model of decision tree and how they affect the balance between over fitting and underfitting. examine these hyperparameters now max feature this hyperparameter regulates the most feature that are taken into account while choosing a split at each node. you can avoid the tree from being too complex and perhaps mitigate overfitting by limiting the amount of characteristic that are taken into account. setting it too low, however, might result in underfitting since the model might fail to adequately account for key association found in the data. max leaf node the decision tree maximum amount of leaf node is constrained by max leaf nodes. by limiting the length of the tree and making sure it doesn catch noise, limiting the leaf node can assist prevent overfitting. however, if it is set too low, the tree could not go deep enough to catch all pertinent patterns, which might result in underfitting. min sample split the bare minimum of sample necessary to further split an internal node. by imposing a greater threshold for splitting, raising this hyperparameter can avoid overfitting, but if it is set too high, it could result in underfitting and cause the tree to fail to utilize the data. min sample leaf the minimal number of sample needed to produce a leaf node is specified by the min sample leaf variable. raising this number will result in bigger leaf node with broader predictions, which will help to prevent overfitting. but if it set too high, the tree might be unable to catch the data finer nuances, which might result in underfitting. min weight fraction leaf when compared to min sample leaf, min weight fraction leaf is based on the weighted total of samples. when working with weighted datasets, it might be helpful. similar to min sample leaf, changing this hyperparameter can have an impact on overfitting and underfitting.\\ndecision tree are powerful machine learning technique that assist human in making decision by breaking challenge down into smaller pieces. they can be used to make prediction and are not overly complicated. you can change their complexity with option like max depth. the cart method, which separate data into category based on different criterion to create predictions, is a popular method for creating decision trees. this aid in the development of accurate models. ensemble learning is the use of numerous layer of decision tree to create more complicated predictions. it similar to combining the strength of numerous decision tree to produce a smarter model. decision tree use factor such a gini impurity and entropy to choose how to partition data and make predictions. these strategy aid the tree in determining the optimum way to divide the material. gini impurity evaluates how frequently something is labelled erroneously, whereas entropy measure unpredictability. decision tree use gini impurity and entropy to decide how to split data. they both want to make excellent decisions. gini impurity calculates these split using a squared percentage, while entropy employ a logarithmic proportion. there are setting called hyperparameters that govern how decision tree work. max features, max leaf nodes, and min sample split all help to customise the tree behaviour. they allow you to ensure that the tree match your data and doe not become overly complex. decision tree are simple to understand since they clearly demonstrate the decision making steps. this is why they re referred to a whitebox models. random forest and neural networks, on the other hand, can be exceedingly complex, resulting in blackbox models. they may make accurate forecasts, but it is difficult to see how they make decisions. hard classifier make simple predictions, such a selecting one of two options. soft classifier provide probability while taking outlier into account. for example, hard voting count the majority choice, whereas soft voting average probability. ensemble approach like a bagging, boosting, and stacking aid in improving model performance. bagging employ many decision tree and aggregate their predictions. boosting creates a series of models, each one learning from the one before it. stacking is the combination of various model kinds. bagging minimises variation, boosting improves accuracy, and stacking combine many models. a decision tree is similar to a step by step tutorial that focus on one aspect at a time. a random forest is a collection of decision trees. random forest use a large number of tree to compensate for error and create more reliable forecasts. a single decision tree make decision one step at a time. however, a random forest is a collection of these trees. it like combining the information of numerous tree to generate more accurate predictions. it a method of dealing with challenge that each tree may face on it own.\\ntask . today we were discussing non linear decision boundary for classification and the need for neural network for these kind of models. a basic neural network ha an input layer, a hidden layer and an output layer. multi layer neural network are inspired by neuron in the human brain. . multi layer neural network can be used for both classification and regression. sklearn offer package for both, called mlpclassifier and mlpregressor. you are able to specify many thing using the model including the activation functions. the activation function for the neural network includes the sigmoid and a tangent function that we discussed today. the difference between the two is the range. sigmoid ha a range from and tanh ha a range from . there are additional activation function a well that we didn discus a much today including the linear, step, sign and relu rectified linear unit functions. slide . we talked about two process in neural network forward propagation and backward propagation. forward propagation is when the data is moving from the input layer to the output layer. between each layer are weights, bias and activation function that are applied to help fit the data. the result are summed and returned from the output node. backward propagation is used to tweak and optimized our weight and biases.\\na.. one hot encoder feature encoding is a process of feature engineering and we can encode and transfer our categorical column in to integer column which result in increasing the feature dimension in our dataset once after applying one hot encoding . . we can label number of category in each column.ex suppose if we have category in one column after applying one hot encoder ml technique then it increase the extra feature for that dataset . .it label if that particular category name found rest will zero in that encoded column.the procedure above step no. continues untill all creates a dummy variable for those categorical columns.b.. label encoder . label encoder also label a if that particular label found in order manner but in this case there is no increment in dimension in feature because it encoded in that column itself orderly.c. decision tress . splitting our dataset from root to leaf node i.e from starting node to prediction goal leaf node . . here , in order to reach goal node we can split our datain to region in a tree form. . here finally we have to find a best split on our data set, this will known when our impurity ha less value when compared to other node impurity values, this will known by calculating the gini index formula . . finally it chooses the root node based on impurity value and later on finding the best split among all splits. finally which result in good understanding of interpretation for beginner and increase in accuracy .\\nquestion how to apply the following pipeline to text classification? . look at the big picture understand the overall objective of the text classification task. define the problem, identify potential challenges, and set clear goal for the classification system. . get the data collect a representative dataset of text sample that cover the range of class or category you want the model to classify. ensure the dataset is well labeled with the corresponding categories. . discover and visualize the data to gain insight analyze the dataset to understand it characteristics. explore the distribution of classes, check for imbalances, and visualize pattern within the text data. this step help in making informed decision during later stages. . prepare the data for machine learning algorithm preprocess the text data by cleaning, tokenizing, and converting it into a suitable format for machine learning models. this may involve technique like stemming, lemmatization, and vectorization to represent the textual information in a numerical format. . select a model and train it choose an appropriate text classification model such a a natural language processing nlp model or a machine learning algorithm like a support vector machine svm or a neural network. train the selected model using the preprocessed text data. . fine tune your model optimize the performance of the model by fine tuning hyperparameters, adjusting feature representations, or employing technique like cross validation. this step aim to enhance the model accuracy and generalization. . present your solution communicate the result of your text classification model, highlighting it accuracy, precision, recall, and any other relevant metrics. clearly articulate the effectiveness of the model in addressing the defined problem. . launch, monitor, and maintain your system deploy the text classification system into a production environment. continuously monitor it performance and update the model a needed. consider implementing feedback loop to adapt to evolving text pattern over time.\\nthis lesson of lecture slide cover a wide variety of eclectic topics, the unifying factor being they are all a tool we will need to lay the foundation to allow to build an ai model. first it reviewed the pipeline of data analysis with machine learning, then covered various tool which would be needed in this process such a python packages, linear algebra, linear algebra in python, and several different plotting tools. aththe end of the slide are a helpful stack of review slide for linear algebra. i never actually have take a linear algebra course so i intend to review these closely and i hope we cover this material in class. i wa not able to attend the lecture, but in reviewing the slides, i found the pipeline diagram particularly helpful. i am new to the whole concept of machine learning and it useful to see a big picture diagram of what the process is like to give me a better idea of the significance of the information we cover, and to serve a a roadmap so that i am not completely lost on how each piece of information fit into the whole puzzle. i found the linear algebra section helpful, a i have never covered that material. i am curious if there is another good resource for learning linear algebra because i am still very unfamiliar with it?\\ntask in today class th aug, we are going to cover topic like end to end data science pipeline , exploratory data analysis, some machine learning library requirement likepandas, numpy,seaborn , matplotlib, scikit learn.also covering some matrix image applications, matrix, vectors.task in this class , i learnt about how to visualize the data, end to end data science pipeline with the point covering how to collect the data, data visualization to visualize the data how to spread it before prediction , we have to do some data preprocessing step for training our data and testing a well, after that we are selecting the ml model based on the experimental , later on that we have to do parameter tuning thing untill we get some good accuracy and finally we can do productionize our model. not a new thing but, i also explore some popular machine learning package like pandas, scikit learn,numpy, matplotlib and seaborn in our class. some new thing i learnt in class is that distance between feature vector , linear algebra, vector, matrix these are helpful when we playing with the data according to our requirement . a new thing i learnt is an image detection with pixel how it works.\\none of the thing that stuck out to me today, i wa thinking about the two graph on this page. i believe that asked during class if there wa an issue with iris setosa being oversampled, because if we look at the petal length v. density graph, the red section seems much larger. i wa thinking about it, though, and i wonder if this is just an optical illusion because the red section is quite a bit taller, but is also narrower, so it area may not be much larger. then i realized, at a glance, the scatter plot seems to almost have the opposite issue, because the blue dot representing the iris setosa here seem to not have a many. i think in this case, it maybe because they are more tightly packed, so they re overlapping each other and not showing up a distinctly. so thinking about these two graph and how they can create misconception about sample sizes, i wa wondering if this is something that ha to be considered when choosing how to graph feature variable like this? is it a problem that come up often, or is it more specific to this example and this data set? overall i thought this example wa really interesting!\\nthe slide speaks knn which kth nearest neighbour and machine learning work flow. it also explains the step we need to take in order calculate knn of data point. also there is explanation for the concept of euclidian distance. the slide also have concept about knn calculation for continuous output and categorial output. it ha concept of supervised learning a well a example for classification .knn for calculation of knn we take a test point and calculate the euclidian distance to nearest data point where is mentioned in knn. we can apply knn for both classification and regression process. we calculate the mean of the data point we take. professor explained with classification example such a alphago and minst data. he also explained the pipeline in order to calculate knn. calculation of knn using ml data first, we need to prepare the training data, configure the value of k, prepare the training knn model, and test the knn model. then, we are supposed to evaluate the knn model.we even learned to implement knn using sklearn package . knn might not be the better choice in some situations. for example, patient weight is more important when compared to the distance between the samples. professor explained two concept to normalize, such a normalization and standardization, and their implementation using sklearn. we can jump into different type of distance function according to the requirement for knn. we can also do hyper parameter tuning and model selection if the model we get after doing knn is not best.\\nthis slide brings to attention something i ve been thinking about, which is how we might be able to decide how many neighbor is good to test. the reason why i think this is an interesting problem is that we might think intuitively the more neighbor we test, the more accurate the prediction will be, but this may not always be the case. for example, here, we can look at the two red point that are right above the . marker. in this case, testing only nearest neighbor might yield a correct prediction because these two red point are right next to each other. but they are surrounded by green points, so if we increased the number of neighbor we are testing, we would end up getting a less accurate incorrect result. so we can see, the intuition that testing more neighbor result in better prediction is not always true.this reminds me of the weighted knn approach that we talked about before, wherein we can test a larger number of neighbors, but the further away a neighbor is, the less influence it will have on the final prediction. that made more sense when the prediction wa a numeric value, but it may not work a cleanly when we re dealing with categorical variables. still, mapping a version of weighted knn onto a problem like this might give good results, a long a we account for the fact that the prediction is categorical.one way of doing this might be to add together distance for all red points, and distance for all green points, then we could say that whichever total is larger is our categorical prediction. this might be a simplistic approach but i think it would ensure that point that are further away would have less influence on the categorical prediction, because a distance increases, distance would decrease.\\n. the first set of slide are review from previous lessons. we are discussing nearest neighbor for regression and classification. it is important to consider mutliple neighbor because one data point is not sufficient. in the example given on the slide, the data point that are closest to each other are from different class and will return an incorrect prediction. the class of the data point can be predicted based on the surrounding data points. if there are mostly class data point near by we assume the test point is also class . it is noteable to always choose an odd number for the nearest neighbor so that a tie can be avoided. . the accuracy of the knn classification can be measured with the accuracy metric. this is simply the number of correct prediction divided by the total number of predictions. the classification error can then be defined a accuracy result. we want a high accuracy result and a low classification result. there are multiple way to improve the classification performance. we can scale the effects, we can use standardization method or we can use normalization methods. there are also different distance formulas, known a distance functions, that may be more appropriate than the euclidean distance formula. the distance function chosen differs based on the type of data being evaluated, ie. discrete or continuous. . hyperparameter and model selection guideline are in the slide this week a well. essentially it appears that one should try a few different parameter to measure and determine which one is the optimal from the validation set. one of the parameter that can be manipulated is the value. if then the training accuracy will be but the testing accuracy will be low. when is increased the training accuracy will decrease, but the testing accuracy will improve.\\nthe slide explain doing a binary classification and finding a linear boundary between two classes, the positive class region and the negative class region. we define a linear function using the learning weight of input feature a well a the input space that separate two classes. we also have a linear classifier in the slide and different method to get a binary response. we also discussed random variable a well a probability distribution and also into to prior and posterior probability. it also dicusses about logistic regression and how to create linear decision boundary and we also do the parameter estimation for that. . in order to define the linear function, we define the weight and linear boundary in the input space. we get a binary response, for example, if a patient ha heart disease or not. professor also explained about logisitic regression where we define function to estimate probabilities, and he also explained how logistic regression will relate to linear regression and equation for it. the function is defined by mapping a dimensional vector whose probability is between and . we define mean square error for this loss function and do parameter estimation for logistic regression, binary cross entropy.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Decide the text generation strategy\n","text_generator = 'character_level'  # or 'word_level'\n","\n","if text_generator == 'word_level':\n","    vectorize_layer = tf.keras.layers.TextVectorization(\n","        split=\"whitespace\",\n","        standardize=\"lower\"\n","    )\n","    window_size = 20\n","elif text_generator == 'character_level':\n","    vectorize_layer = tf.keras.layers.TextVectorization(\n","        split=\"character\",\n","        standardize=\"lower\"\n","    )\n","    window_size = 50\n","else:\n","    print(\"Error!\")\n","    exit(-1)\n","\n","# Filter out any empty strings from your dataset before adapting the layer.\n","# Assume cleaned_text is a list (or iterable) of strings.\n","filtered_text = [text for text in cleaned_text if text.strip() != '']\n","\n","# Adapt the layer with the filtered text so no empty strings are added from your data.\n","vectorize_layer.adapt(tf.constant(filtered_text))\n","vocabulary = vectorize_layer.get_vocabulary()\n","\n","print(\"Vocabulary:\", vocabulary)\n","print(\"Total number of tokens:\", len(vocabulary))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APmhehpe-Elg","executionInfo":{"status":"ok","timestamp":1743103011624,"user_tz":300,"elapsed":3492,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"72f262c2-cc87-4830-b815-71322edb3a20"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['', '[UNK]', np.str_('e'), np.str_('t'), np.str_('a'), np.str_('i'), np.str_('n'), np.str_('o'), np.str_('r'), np.str_('s'), np.str_('l'), np.str_('h'), np.str_('d'), np.str_('c'), np.str_('m'), np.str_('u'), np.str_('g'), np.str_('f'), np.str_('p'), np.str_('b'), np.str_('y'), np.str_('w'), np.str_('.'), np.str_('v'), np.str_(','), np.str_('k'), np.str_('x'), np.str_('q'), np.str_('z'), np.str_('j'), np.str_('?'), np.str_('!')]\n","Total number of tokens: 32\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Create the model that uses the vectorize text layer\n","model_vectorizer = tf.keras.models.Sequential()\n","model_vectorizer.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n","model_vectorizer.add(vectorize_layer)"],"metadata":{"id":"LZkGuz6j-W7U","executionInfo":{"status":"ok","timestamp":1743103011631,"user_tz":300,"elapsed":4,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')  # Ensure the tokenizer data is available\n","\n","input_string = 'machine learning.'\n","if text_generator == 'word_level':\n","    tokens = word_tokenize(input_string)\n","    input_string = \" \".join(tokens)\n","elif text_generator == 'character_level':\n","    input_string = \"\".join([char for char in input_string])\n","\n","# Convert to a TensorFlow constant (batch of one sample)\n","input_tensor = tf.constant([input_string])\n","sentences_indices = model_vectorizer.predict(input_tensor, verbose=0).flatten()\n","print(\"The integer encoding for input string:\", input_string, \"is\\n\", sentences_indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wI1c7sgZ-gES","executionInfo":{"status":"ok","timestamp":1743103014693,"user_tz":300,"elapsed":3059,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"34b6a9fc-1922-4e61-c8d8-9cdca5cad352"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["The integer encoding for input string: machine learning. is\n"," [14  4 13 11  5  6  2  1 10  2  4  8  6  5  6 16 22]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Save the model\n","model_vectorizer.save(\"vectorize_layer_\" + text_generator + \".keras\")\n","\n","# Load the model\n","loaded_model_vectorizer = tf.keras.models.load_model(\"vectorize_layer_\" + text_generator + \".keras\")\n","loaded_vectorizer = loaded_model_vectorizer.layers[0]\n","loaded_vocabulary = loaded_vectorizer.get_vocabulary()\n","print(\"Loaded Vocabulary:\", loaded_vocabulary)\n","print(\"Total number of tokens:\", len(loaded_vocabulary))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cOioxumL-1MP","executionInfo":{"status":"ok","timestamp":1743103014704,"user_tz":300,"elapsed":13,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"522d46aa-434b-4fd4-838c-2d8c48e04065"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded Vocabulary: ['', '[UNK]', np.str_('e'), np.str_('t'), np.str_('a'), np.str_('i'), np.str_('n'), np.str_('o'), np.str_('r'), np.str_('s'), np.str_('l'), np.str_('h'), np.str_('d'), np.str_('c'), np.str_('m'), np.str_('u'), np.str_('g'), np.str_('f'), np.str_('p'), np.str_('b'), np.str_('y'), np.str_('w'), np.str_('.'), np.str_('v'), np.str_(','), np.str_('k'), np.str_('x'), np.str_('q'), np.str_('z'), np.str_('j'), np.str_('?'), np.str_('!')]\n","Total number of tokens: 32\n"]}]},{"cell_type":"code","source":["# prepare the dataset of input to output pairs encoded as integers\n","def step5_prepare_text_generation_data(cleaned_text, model_vectorizer, text_generator = 'character_level', window_size = 100, window_shift = 1, data_format = 'many2many' ):\n","   import pandas as pd\n","   import nltk\n","   from nltk.tokenize import word_tokenize\n","   nltk.download('punkt') # Download the punkt tokenizer data (if not already downloaded)\n","\n","   data_array = []\n","   data_indices_array = []\n","\n","   for idx, comment in enumerate(cleaned_text.split('\\n')):\n","      if idx % 100 == 0:\n","         print(idx, end=',')\n","\n","      if text_generator == 'word_level':\n","         comment_words = word_tokenize(comment)\n","      elif text_generator == 'character_level':\n","         # split string into a list characters\n","         comment_words = [char for char in comment]\n","\n","      # integer encoding to input string\n","      sentences_indices = model_vectorizer.predict(tf.constant(comment_words), verbose = 0).flatten()\n","\n","      for i in range(0, len(comment_words) - window_size, window_shift):\n","\n","          # get window of words\n","          seq_in = comment_words[i:i+window_size]\n","          # get integer encoding words\n","          seq_in_indices = sentences_indices[i:i+window_size]\n","\n","          if data_format == 'many2one':\n","              # get output letter\n","              seq_out = comment_words[i+window_size]\n","              # get integer encoding words\n","              seq_out_indices = sentences_indices[i+window_size]\n","          elif data_format == 'many2many':\n","              # get output letter\n","              seq_out = comment_words[i+1:i+window_size+1]\n","              # get integer encoding words\n","              seq_out_indices = sentences_indices[i+1:i+window_size+1]\n","          else:\n","              print(\"data_format should be one of ['many2one', 'many2many']\")\n","              exit(-1)\n","\n","          # save data into dataframe for reference\n","          data_array.append([seq_in,seq_out])\n","          data_indices_array.append([seq_in_indices,seq_out_indices])\n","\n","   n_samples = len(data_array)\n","   print(\"\\nDerive the training dataset with size \", n_samples)\n","   data_df = pd.DataFrame(data_array,columns =['Input','Output'])\n","   data_indices_df = pd.DataFrame(data_indices_array,columns =['Input','Output'])\n","   return data_df, data_indices_df"],"metadata":{"id":"41huA_B4-9DY","executionInfo":{"status":"ok","timestamp":1743103014739,"user_tz":300,"elapsed":34,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["data_format = 'many2one'\n","\n","training_raw_text_many2one, training_integer_dataset_many2one = step5_prepare_text_generation_data(cleaned_text, model_vectorizer, text_generator = text_generator, window_size = window_size, window_shift = 1, data_format = data_format)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HA-eu8d6_CSj","executionInfo":{"status":"ok","timestamp":1743103051192,"user_tz":300,"elapsed":36450,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"33777f38-c74f-48d0-abbd-2504ea08d174"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0,"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\n","Derive the training dataset with size  216148\n"]}]},{"cell_type":"code","source":["data_format = 'many2many'\n","\n","training_raw_text_many2many, training_integer_dataset_many2many = step5_prepare_text_generation_data(cleaned_text, model_vectorizer, text_generator = text_generator, window_size = window_size, window_shift = 1, data_format = data_format)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5RTx_YMw_GIr","executionInfo":{"status":"ok","timestamp":1743103078797,"user_tz":300,"elapsed":27610,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"d63970bf-47e3-40f9-d619-5d68dd30c7da"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["0,"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\n","Derive the training dataset with size  216148\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","pd.set_option('display.max_colwidth', 1000)\n","training_raw_text_many2one"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"9LeKx2eG_JMo","executionInfo":{"status":"ok","timestamp":1743103078809,"user_tz":300,"elapsed":16,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"df7a5f24-2a99-4ae5-d0bb-a480d9ff1893"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                                                                                         Input  \\\n","0       [a, f, t, e, r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f]   \n","1       [f, t, e, r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f, o]   \n","2       [t, e, r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f, o, u]   \n","3       [e, r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f, o, u, n]   \n","4       [r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f, o, u, n, d]   \n","...                                                                                                                                                        ...   \n","216143  [t, i, m, a, t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t]   \n","216144  [i, m, a, t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t, r]   \n","216145  [m, a, t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t, r, o]   \n","216146  [a, t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t, r, o, p]   \n","216147  [t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t, r, o, p, y]   \n","\n","       Output  \n","0           o  \n","1           u  \n","2           n  \n","3           d  \n","4              \n","...       ...  \n","216143      r  \n","216144      o  \n","216145      p  \n","216146      y  \n","216147      .  \n","\n","[216148 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-6fa3ce7b-eb09-4c00-8b59-01ce0407bd84\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Input</th>\n","      <th>Output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[a, f, t, e, r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f]</td>\n","      <td>o</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[f, t, e, r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f, o]</td>\n","      <td>u</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[t, e, r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f, o, u]</td>\n","      <td>n</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[e, r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f, o, u, n]</td>\n","      <td>d</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[r,  , g, o, i, n, g,  , t, h, r, o, u, g, h,  , t, h, e,  , a, b, s, t, r, a, c, t,  , o, f,  , t, h, e,  , p, a, p, e, r, ,,  , i,  , f, o, u, n, d]</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>216143</th>\n","      <td>[t, i, m, a, t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t]</td>\n","      <td>r</td>\n","    </tr>\n","    <tr>\n","      <th>216144</th>\n","      <td>[i, m, a, t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t, r]</td>\n","      <td>o</td>\n","    </tr>\n","    <tr>\n","      <th>216145</th>\n","      <td>[m, a, t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t, r, o]</td>\n","      <td>p</td>\n","    </tr>\n","    <tr>\n","      <th>216146</th>\n","      <td>[a, t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t, r, o, p]</td>\n","      <td>y</td>\n","    </tr>\n","    <tr>\n","      <th>216147</th>\n","      <td>[t, i, o, n,  , f, o, r,  , l, o, g, i, s, t, i, c,  , r, e, g, r, e, s, s, i, o, n, ,,  , b, i, n, a, r, y,  , c, r, o, s, s,  , e, n, t, r, o, p, y]</td>\n","      <td>.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>216148 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fa3ce7b-eb09-4c00-8b59-01ce0407bd84')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6fa3ce7b-eb09-4c00-8b59-01ce0407bd84 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6fa3ce7b-eb09-4c00-8b59-01ce0407bd84');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f3b17e9e-420f-48c2-ac4b-5e7a7ff8b2cd\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3b17e9e-420f-48c2-ac4b-5e7a7ff8b2cd')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f3b17e9e-420f-48c2-ac4b-5e7a7ff8b2cd button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_71ca1d85-09c8-44d5-9915-7a06361b5015\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('training_raw_text_many2one')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_71ca1d85-09c8-44d5-9915-7a06361b5015 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('training_raw_text_many2one');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"training_raw_text_many2one"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import pandas as pd\n","pd.set_option('display.max_colwidth', 1000)\n","training_integer_dataset_many2one"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"-w_-V1JW_LZR","executionInfo":{"status":"ok","timestamp":1743103078826,"user_tz":300,"elapsed":15,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"3ae88694-8522-43a5-d121-bf8607c45feb"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                                                                                                          Input  \\\n","0        [4, 17, 3, 2, 8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17]   \n","1        [17, 3, 2, 8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17, 7]   \n","2        [3, 2, 8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17, 7, 15]   \n","3        [2, 8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17, 7, 15, 6]   \n","4       [8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17, 7, 15, 6, 12]   \n","...                                                                                                                                                                         ...   \n","216143         [3, 5, 14, 4, 3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3]   \n","216144         [5, 14, 4, 3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3, 8]   \n","216145         [14, 4, 3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3, 8, 7]   \n","216146         [4, 3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3, 8, 7, 18]   \n","216147        [3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3, 8, 7, 18, 20]   \n","\n","        Output  \n","0            7  \n","1           15  \n","2            6  \n","3           12  \n","4            1  \n","...        ...  \n","216143       8  \n","216144       7  \n","216145      18  \n","216146      20  \n","216147      22  \n","\n","[216148 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-8af26883-1451-4131-ba31-72caf1985a1d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Input</th>\n","      <th>Output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[4, 17, 3, 2, 8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17]</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[17, 3, 2, 8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17, 7]</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[3, 2, 8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17, 7, 15]</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[2, 8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17, 7, 15, 6]</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[8, 1, 16, 7, 5, 6, 16, 1, 3, 11, 8, 7, 15, 16, 11, 1, 3, 11, 2, 1, 4, 19, 9, 3, 8, 4, 13, 3, 1, 7, 17, 1, 3, 11, 2, 1, 18, 4, 18, 2, 8, 24, 1, 5, 1, 17, 7, 15, 6, 12]</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>216143</th>\n","      <td>[3, 5, 14, 4, 3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3]</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>216144</th>\n","      <td>[5, 14, 4, 3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3, 8]</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>216145</th>\n","      <td>[14, 4, 3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3, 8, 7]</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>216146</th>\n","      <td>[4, 3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3, 8, 7, 18]</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>216147</th>\n","      <td>[3, 5, 7, 6, 1, 17, 7, 8, 1, 10, 7, 16, 5, 9, 3, 5, 13, 1, 8, 2, 16, 8, 2, 9, 9, 5, 7, 6, 24, 1, 19, 5, 6, 4, 8, 20, 1, 13, 8, 7, 9, 9, 1, 2, 6, 3, 8, 7, 18, 20]</td>\n","      <td>22</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>216148 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8af26883-1451-4131-ba31-72caf1985a1d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8af26883-1451-4131-ba31-72caf1985a1d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8af26883-1451-4131-ba31-72caf1985a1d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b1dcc85e-22cf-4479-afa5-2253d1fd7de5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b1dcc85e-22cf-4479-afa5-2253d1fd7de5')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b1dcc85e-22cf-4479-afa5-2253d1fd7de5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_a312c40e-d276-4788-bfdb-9b314ca517c8\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('training_integer_dataset_many2one')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_a312c40e-d276-4788-bfdb-9b314ca517c8 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('training_integer_dataset_many2one');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"training_integer_dataset_many2one"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow\n","\n","\n","# Set the type of dataset\n","training_integer_dataset = training_integer_dataset_many2many\n","\n","# Shuffle the rows\n","training_integer_dataset = training_integer_dataset.sample(frac=1.0, random_state=42) # Set a random seed for reproducibility\n","# Reset the index if needed\n","training_integer_dataset.reset_index(drop=True, inplace=True)\n","\n","\n","X = np.vstack(training_integer_dataset['Input'].apply(np.array))\n","y = np.vstack(training_integer_dataset['Output'].apply(np.array))\n","\n","print(\"X.shape: \",X.shape)\n","print(\"y.shape: \",y.shape)"],"metadata":{"id":"e8oJ17pT_NnS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743107844205,"user_tz":300,"elapsed":1183,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"de0d170f-d4da-4d0f-c37d-50f005264bf6"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["X.shape:  (216148, 50)\n","y.shape:  (216148, 50)\n"]}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding\n","from keras import layers\n","from keras import initializers\n","from keras.layers import Dropout,Embedding, Dense, GRU, LSTM, Bidirectional\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","embedding_dim = 100\n","total_vocab_size = len(vocabulary)\n","model = Sequential()\n","model.add(Embedding(total_vocab_size, embedding_dim, input_length=X.shape[1]))\n","if data_format == 'many2one':\n","   model.add(GRU(512))\n","else:\n","   model.add(GRU(512, return_sequences=True))\n","model.add(Dense(total_vocab_size, activation='softmax'))\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"zdI0uMhI_QxJ","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1743103079760,"user_tz":300,"elapsed":52,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"acbd0d5a-6b9b-486d-c930-2438ca3aaeef"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["model.fit(X, y, epochs=5,validation_split=0.1)"],"metadata":{"id":"2XfiHGmR_SuK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743103444578,"user_tz":300,"elapsed":364816,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"ab34624d-a18a-4bf8-8c02-d12b4512bd31"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 10ms/step - accuracy: 0.6877 - loss: 1.0625 - val_accuracy: 0.8935 - val_loss: 0.3566\n","Epoch 2/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 10ms/step - accuracy: 0.8986 - loss: 0.3374 - val_accuracy: 0.9025 - val_loss: 0.3242\n","Epoch 3/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 10ms/step - accuracy: 0.9049 - loss: 0.3131 - val_accuracy: 0.9031 - val_loss: 0.3194\n","Epoch 4/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 9ms/step - accuracy: 0.9061 - loss: 0.3074 - val_accuracy: 0.9044 - val_loss: 0.3140\n","Epoch 5/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 9ms/step - accuracy: 0.9065 - loss: 0.3042 - val_accuracy: 0.9036 - val_loss: 0.3156\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ff1d0df3450>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","# save the model to disk\n","model.save(\"model_\"+text_generator+\".keras\")"],"metadata":{"id":"-IoY6i9u_UFx","executionInfo":{"status":"ok","timestamp":1743103444623,"user_tz":300,"elapsed":39,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","\n","# Load the model\n","loaded_model = load_model(\"model_\"+text_generator+\".keras\")"],"metadata":{"id":"Y8aaWSMo_VL7","executionInfo":{"status":"ok","timestamp":1743103445710,"user_tz":300,"elapsed":1081,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def sample_token(preds, temperature=1.0):\n","   # helper function to sample an index from a probability array\n","   preds = np.asarray(preds).astype('float64')\n","   preds = np.log(preds) / temperature\n","   exp_preds = np.exp(preds)\n","   preds = exp_preds / np.sum(exp_preds)\n","   probas = np.random.multinomial(1, preds, 1)\n","   return np.argmax(probas)\n","\n","def generate_text(rnn_model, model_vectorizer, start_text,text_length=100, text_generator = 'character_level'):\n","   from tensorflow.keras.preprocessing.sequence import pad_sequences\n","   import tensorflow as tf\n","   import nltk\n","   from nltk.tokenize import word_tokenize\n","   import sys\n","   import numpy as np\n","   nltk.download('punkt') # Download the punkt tokenizer data (if not already downloaded)\n","\n","   comment_words = start_text\n","   if text_generator == 'word_level':\n","      comment_words = word_tokenize(start_text.lower())\n","   elif text_generator == 'character_level':\n","      comment_words = [char.lower() for char in start_text]\n","\n","   # Create the model that uses the vectorize text layer\n","   model_vectorizer_layer = model_vectorizer.layers[0]\n","   vocabulary = model_vectorizer_layer.get_vocabulary()\n","   encoding = model_vectorizer.predict(tf.constant(comment_words)).flatten().tolist()\n","\n","   input_shape = rnn_model.input_shape\n","   maxlen = input_shape[1] # specify how long the sequences should be. This cuts sequences that exceed that number.\n","   print(\"#### Input sequence: \", start_text)\n","   print(\"#### Start generating the paragraph: \\n\")\n","\n","   line_print = ''\n","   new_sequence = start_text\n","\n","   sys.stdout.write(start_text+\"\\n\")\n","   for repeat in range(text_length):\n","      test_data = np.reshape(encoding, (1, len(encoding)))\n","      test_data_pad = pad_sequences(test_data, padding='pre', maxlen=maxlen)\n","      prediction = rnn_model.predict(test_data_pad, verbose=0)\n","\n","      if len(prediction.shape) == 2:\n","          prediction = prediction[0]\n","      else:\n","          prediction = prediction[0,-1,:]\n","\n","      index = sample_token(prediction) # sample word by probability\n","      result = vocabulary[index]\n","\n","      if result in ['', '[UNK]', ' ']:\n","         result = ' '\n","\n","      if text_generator == 'word_level':\n","         line_print = line_print + ' '+ result\n","         new_sequence = new_sequence + result\n","         if len(line_print) > 70:\n","             sys.stdout.write(\"\\n\")\n","             line_print = ''\n","         sys.stdout.write(' '+ result)\n","\n","      elif text_generator == 'character_level':\n","         line_print = line_print +' '+ result\n","         new_sequence = new_sequence + ' '+ result\n","         if len(line_print) > 200 and result==' ':\n","             sys.stdout.write(\"\\n\")\n","             line_print = ''\n","         sys.stdout.write(result)\n","         # add new word into current encoding for predicting next word\n","      encoding.append(index)\n","      encoding = encoding[1:len(encoding)]"],"metadata":{"id":"LOd6yzCc_Xvu","executionInfo":{"status":"ok","timestamp":1743103445722,"user_tz":300,"elapsed":6,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","# Load the model\n","loaded_model = tensorflow.keras.models.load_model(\"model_\"+text_generator+\".keras\")\n","loaded_model_vectorizer = tensorflow.keras.models.load_model(\"vectorize_layer_\"+text_generator+\".keras\")\n","\n","start_text = 'If the output label is numeric values, we '\n","generate_text(loaded_model, loaded_model_vectorizer, start_text,text_length=1000, text_generator = text_generator)"],"metadata":{"id":"23dFW7UY_Zgb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743103516050,"user_tz":300,"elapsed":70324,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"e69a1a31-aa90-436a-b3cc-cf13dfaaf788"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["#### Input sequence:  If the output label is numeric values, we \n","#### Start generating the paragraph: \n","\n","If the output label is numeric values, we \n","can think of it a categorical classification task where there are over after that tell about the current\n"," meteorology within an area and the glm output or predicted variable is a goal of a randomly chosen positive,\n"," increment the input data. they both lasso and railar for able to specify many thing using two method\n"," need to peract the no.of thunderstorm if it ha at least one flash in the image. for problem statement\n"," , an image is classifier.step train the dataset which result in bigger leaf node with broader preprocessing\n"," step quality control, it minimum consistencying linear algebra because if we learn from the training\n"," parameter to generate stronger prediction or described some pattern relationship among the features.\n"," the training dataset is in range between to of ridge and lasso in the wanted to avoid introduced me to\n"," some new evaluation metric such a auc,mae,rmse,and square.and the final thing a freq efficient, the author\n"," opts for extracting key statistic from each variable wa observe"]}]},{"cell_type":"code","source":["#@title Step 11"],"metadata":{"id":"2CGX08l5_dHp","executionInfo":{"status":"ok","timestamp":1743103516063,"user_tz":300,"elapsed":7,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#@title Word level - many2many\n","\n","\n","import tensorflow as tf\n","\n","# Decide the text generation strategy\n","text_generator = 'word_level'\n","\n","if text_generator == 'word_level':\n","    vectorize_layer = tf.keras.layers.TextVectorization(\n","        split=\"whitespace\",\n","        standardize=\"lower\"\n","    )\n","    window_size = 20\n","\n","\n","elif text_generator == 'character_level':\n","    vectorize_layer = tf.keras.layers.TextVectorization(\n","        split=\"character\",\n","        standardize=\"lower\"\n","    )\n","    window_size = 50\n","else:\n","    print(\"Error!\")\n","    exit(-1)\n","\n","# Filter out any empty strings from your dataset before adapting the layer.\n","# Assume cleaned_text is a list (or iterable) of strings.\n","filtered_text = [text for text in cleaned_text if text.strip() != '']\n","\n","# Adapt the layer with the filtered text so no empty strings are added from your data.\n","vectorize_layer.adapt(tf.constant(filtered_text))\n","vocabulary = vectorize_layer.get_vocabulary()\n","\n","print(\"Vocabulary:\", vocabulary)\n","print(\"Total number of tokens:\", len(vocabulary))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfHHrK3gVsEI","executionInfo":{"status":"ok","timestamp":1743104565544,"user_tz":300,"elapsed":100,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"acfbf3bb-820c-44b0-84bd-e4805f0d2533"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['', '[UNK]', np.str_('e'), np.str_('t'), np.str_('a'), np.str_('i'), np.str_('n'), np.str_('o'), np.str_('r'), np.str_('s'), np.str_('l'), np.str_('h'), np.str_('d'), np.str_('c'), np.str_('m'), np.str_('u'), np.str_('g'), np.str_('f'), np.str_('p'), np.str_('b'), np.str_('y'), np.str_('w'), np.str_('.'), np.str_('v'), np.str_(','), np.str_('k'), np.str_('x'), np.str_('q'), np.str_('z'), np.str_('j'), np.str_('?'), np.str_('!')]\n","Total number of tokens: 32\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Create the model that uses the vectorize text layer\n","model_vectorizer = tf.keras.models.Sequential()\n","model_vectorizer.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n","model_vectorizer.add(vectorize_layer)"],"metadata":{"id":"Zg1PL-DlV1AK","executionInfo":{"status":"ok","timestamp":1743103516231,"user_tz":300,"elapsed":31,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt_tab')  # Ensure the tokenizer data is available\n","\n","input_string = 'machine learning.'\n","if text_generator == 'word_level':\n","    tokens = word_tokenize(input_string)\n","    input_string = \" \".join(tokens)\n","elif text_generator == 'character_level':\n","    input_string = \"\".join([char for char in input_string])\n","\n","# Convert to a TensorFlow constant (batch of one sample)\n","input_tensor = tf.constant([input_string])\n","sentences_indices = model_vectorizer.predict(input_tensor, verbose=0).flatten()\n","print(\"The integer encoding for input string:\", input_string, \"is\\n\", sentences_indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFKSoNJOWMi0","executionInfo":{"status":"ok","timestamp":1743104055300,"user_tz":300,"elapsed":584,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"cf5fbf0f-9893-4afa-e4b0-2f8d5a8f1ed5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["The integer encoding for input string: machine learning . is\n"," [ 1  1 22]\n"]}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding\n","from keras import layers\n","from keras import initializers\n","from keras.layers import Dropout,Embedding, Dense, GRU, LSTM, Bidirectional\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","import nltk\n","\n","nltk.download('punkt') # Download the punkt tokenizer data (if not already downloaded)\n","\n","data_format = 'many2many'\n","\n","embedding_dim = 100\n","total_vocab_size = len(vocabulary)\n","model = Sequential()\n","model.add(Embedding(total_vocab_size, embedding_dim, input_length=X.shape[1]))\n","if data_format == 'many2one':\n","   model.add(GRU(512))\n","else:\n","   model.add(GRU(512, return_sequences=True))\n","model.add(Dense(total_vocab_size, activation='softmax'))\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"APhgXNkH_dVZ","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1743103581525,"user_tz":300,"elapsed":89,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"cf82b74e-495e-4e49-e863-d84f711fda37"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_3\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["model.fit(X, y, epochs=5,validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_hSfdRWYZZM","executionInfo":{"status":"ok","timestamp":1743103973063,"user_tz":300,"elapsed":388841,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"80e621a2-e7a8-4518-dfbc-6b6d25bf72c2"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10ms/step - accuracy: 0.6896 - loss: 1.0558 - val_accuracy: 0.8928 - val_loss: 0.3586\n","Epoch 2/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 10ms/step - accuracy: 0.8983 - loss: 0.3384 - val_accuracy: 0.9008 - val_loss: 0.3291\n","Epoch 3/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 10ms/step - accuracy: 0.9046 - loss: 0.3143 - val_accuracy: 0.9040 - val_loss: 0.3169\n","Epoch 4/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 10ms/step - accuracy: 0.9060 - loss: 0.3071 - val_accuracy: 0.9032 - val_loss: 0.3167\n","Epoch 5/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 10ms/step - accuracy: 0.9063 - loss: 0.3051 - val_accuracy: 0.9036 - val_loss: 0.3169\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ff1517d6890>"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["def sample_token(preds, temperature=1.0):\n","   # helper function to sample an index from a probability array\n","   preds = np.asarray(preds).astype('float64')\n","   preds = np.log(preds) / temperature\n","   exp_preds = np.exp(preds)\n","   preds = exp_preds / np.sum(exp_preds)\n","   probas = np.random.multinomial(1, preds, 1)\n","   return np.argmax(probas)\n","\n","def generate_text(rnn_model, model_vectorizer, start_text,text_length=100, text_generator = 'character_level'):\n","   from tensorflow.keras.preprocessing.sequence import pad_sequences\n","   import tensorflow as tf\n","   import nltk\n","   from nltk.tokenize import word_tokenize\n","   import sys\n","   import numpy as np\n","   nltk.download('punkt') # Download the punkt tokenizer data (if not already downloaded)\n","\n","   comment_words = start_text\n","   if text_generator == 'word_level':\n","      comment_words = word_tokenize(start_text.lower())\n","   elif text_generator == 'character_level':\n","      comment_words = [char.lower() for char in start_text]\n","\n","   # Create the model that uses the vectorize text layer\n","   model_vectorizer_layer = model_vectorizer.layers[0]\n","   vocabulary = model_vectorizer_layer.get_vocabulary()\n","   encoding = model_vectorizer.predict(tf.constant(comment_words)).flatten().tolist()\n","\n","   input_shape = rnn_model.input_shape\n","   maxlen = input_shape[1] # specify how long the sequences should be. This cuts sequences that exceed that number.\n","   print(\"#### Input sequence: \", start_text)\n","   print(\"#### Start generating the paragraph: \\n\")\n","\n","   line_print = ''\n","   new_sequence = start_text\n","\n","   sys.stdout.write(start_text+\"\\n\")\n","   for repeat in range(text_length):\n","      test_data = np.reshape(encoding, (1, len(encoding)))\n","      test_data_pad = pad_sequences(test_data, padding='pre', maxlen=maxlen)\n","      prediction = rnn_model.predict(test_data_pad, verbose=0)\n","\n","      if len(prediction.shape) == 2:\n","          prediction = prediction[0]\n","      else:\n","          prediction = prediction[0,-1,:]\n","\n","      index = sample_token(prediction) # sample word by probability\n","      result = vocabulary[index]\n","\n","      if result in ['', '[UNK]', ' ']:\n","         result = ' '\n","\n","      if text_generator == 'word_level':\n","         line_print = line_print + ''+ result\n","         new_sequence = new_sequence + result\n","         if len(line_print) > 70:\n","             sys.stdout.write(\"\\n\")\n","             line_print = ''\n","         sys.stdout.write(''+ result)\n","\n","      elif text_generator == 'character_level':\n","         line_print = line_print +' '+ result\n","         new_sequence = new_sequence + ' '+ result\n","         if len(line_print) > 200 and result==' ':\n","             sys.stdout.write(\"\\n\")\n","             line_print = ''\n","         sys.stdout.write(result)\n","         # add new word into current encoding for predicting next word\n","      encoding.append(index)\n","      encoding = encoding[1:len(encoding)]"],"metadata":{"id":"4YTBcFZwThIA","executionInfo":{"status":"ok","timestamp":1743105591183,"user_tz":300,"elapsed":26,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["start_text = 'If the output label is numeric values, we '\n","generate_text(model, model_vectorizer, start_text,text_length=1000, text_generator = text_generator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OycbzSfgLpae","executionInfo":{"status":"ok","timestamp":1743105667081,"user_tz":300,"elapsed":74662,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"a9239e60-7eb7-465b-dad9-cf17789d515d"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["#### Input sequence:  If the output label is numeric values, we \n","#### Start generating the paragraph: \n","\n","If the output label is numeric values, we \n","we talked, but i us the normal equarized for meteorological knowledge,\n"," so that to ensure the algorithm from the data may also cheace add a pe\n","nalty best performance diagram and i hope on more than other side the l\n","inear regression to some feature engine the number of neighbor strave o\n","utput measure the complexity is also classification, but this mately, b\n","ecause it one feature engine the number of lightning in the data is use\n","d for evaluating the data to the gradient of the data, this will determ\n","s consting the data into to the opposite direction. the paper discussed\n"," to take in the diagonal.hes the overall aloghes the issue wit data. th\n","is step or name a mix between batch gradient is a measure the coefficie\n","ntly to the ridge regression it is a success ratio rather that help pre\n","vent overfit to train the data into training data wa an injut and relti\n","fied a positive . recall and the recall is that dataset and closer to w\n","ork faster is to find the validation to our performance is a member ow \n","a time."]}]},{"cell_type":"code","source":["#@title Word level - many2one\n","\n","import tensorflow as tf\n","\n","# Decide the text generation strategy\n","text_generator = 'word_level'\n","\n","if text_generator == 'word_level':\n","    vectorize_layer = tf.keras.layers.TextVectorization(\n","        split=\"whitespace\",\n","        standardize=\"lower\"\n","    )\n","    window_size = 20\n","\n","\n","elif text_generator == 'character_level':\n","    vectorize_layer = tf.keras.layers.TextVectorization(\n","        split=\"character\",\n","        standardize=\"lower\"\n","    )\n","    window_size = 50\n","else:\n","    print(\"Error!\")\n","    exit(-1)\n","\n","# Filter out any empty strings from your dataset before adapting the layer.\n","# Assume cleaned_text is a list (or iterable) of strings.\n","filtered_text = [text for text in cleaned_text if text.strip() != '']\n","\n","# Adapt the layer with the filtered text so no empty strings are added from your data.\n","vectorize_layer.adapt(tf.constant(filtered_text))\n","vocabulary = vectorize_layer.get_vocabulary()\n","\n","print(\"Vocabulary:\", vocabulary)\n","print(\"Total number of tokens:\", len(vocabulary))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8QWWkW6M1_U","executionInfo":{"status":"ok","timestamp":1743105848156,"user_tz":300,"elapsed":84,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"263c094f-fea9-4245-8436-8c3ef17d54fa"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['', '[UNK]', np.str_('e'), np.str_('t'), np.str_('a'), np.str_('i'), np.str_('n'), np.str_('o'), np.str_('r'), np.str_('s'), np.str_('l'), np.str_('h'), np.str_('d'), np.str_('c'), np.str_('m'), np.str_('u'), np.str_('g'), np.str_('f'), np.str_('p'), np.str_('b'), np.str_('y'), np.str_('w'), np.str_('.'), np.str_('v'), np.str_(','), np.str_('k'), np.str_('x'), np.str_('q'), np.str_('z'), np.str_('j'), np.str_('?'), np.str_('!')]\n","Total number of tokens: 32\n"]}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding\n","from keras import layers\n","from keras import initializers\n","from keras.layers import Dropout,Embedding, Dense, GRU, LSTM, Bidirectional\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","data_format = 'many2one'\n","\n","embedding_dim = 100\n","total_vocab_size = len(vocabulary)\n","model = Sequential()\n","model.add(Embedding(total_vocab_size, embedding_dim))\n","if data_format == 'many2one':\n","   model.add(GRU(512))\n","else:\n","   model.add(GRU(512, return_sequences=True))\n","model.add(Dense(total_vocab_size, activation='softmax'))\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"etDVF8NeTpv-","colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"status":"ok","timestamp":1743107100857,"user_tz":300,"elapsed":102,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"856ffaf1-3f72-4168-8630-e3f78b187252"},"execution_count":72,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_8\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru_6 (\u001b[38;5;33mGRU\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow\n","\n","\n","# Set the type of dataset\n","training_integer_dataset = training_integer_dataset_many2one\n","\n","# Shuffle the rows\n","training_integer_dataset = training_integer_dataset.sample(frac=1.0, random_state=42) # Set a random seed for reproducibility\n","# Reset the index if needed\n","training_integer_dataset.reset_index(drop=True, inplace=True)\n","\n","\n","X = np.vstack(training_integer_dataset['Input'].apply(np.array))\n","y = np.vstack(training_integer_dataset['Output'].apply(np.array))\n","y = y.squeeze()\n","\n","print(\"X.shape: \",X.shape)\n","print(\"y.shape: \",y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzyBDdD1O6De","executionInfo":{"status":"ok","timestamp":1743107418468,"user_tz":300,"elapsed":1455,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"85afe9a2-b4aa-41e4-b958-85b12ccb71fd"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["X.shape:  (216148, 50)\n","y.shape:  (216148,)\n"]}]},{"cell_type":"code","source":["model.fit(X, y, epochs=5, validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zT-zHsFXNRPc","executionInfo":{"status":"ok","timestamp":1743107755026,"user_tz":300,"elapsed":333821,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"26807804-f183-4c86-a345-82978e89272b"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 10ms/step - accuracy: 0.6184 - loss: 1.3036 - val_accuracy: 0.6790 - val_loss: 1.0834\n","Epoch 2/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 9ms/step - accuracy: 0.7077 - loss: 0.9649 - val_accuracy: 0.7057 - val_loss: 0.9906\n","Epoch 3/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 9ms/step - accuracy: 0.7460 - loss: 0.8301 - val_accuracy: 0.7180 - val_loss: 0.9557\n","Epoch 4/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 9ms/step - accuracy: 0.7676 - loss: 0.7568 - val_accuracy: 0.7243 - val_loss: 0.9448\n","Epoch 5/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 9ms/step - accuracy: 0.7760 - loss: 0.7243 - val_accuracy: 0.7241 - val_loss: 0.9529\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ff14000aed0>"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["def sample_token(preds, temperature=1.0):\n","   # helper function to sample an index from a probability array\n","   preds = np.asarray(preds).astype('float64')\n","   preds = np.log(preds) / temperature\n","   exp_preds = np.exp(preds)\n","   preds = exp_preds / np.sum(exp_preds)\n","   probas = np.random.multinomial(1, preds, 1)\n","   return np.argmax(probas)\n","\n","def generate_text(rnn_model, model_vectorizer, start_text,text_length=100, text_generator = 'character_level'):\n","   from tensorflow.keras.preprocessing.sequence import pad_sequences\n","   import tensorflow as tf\n","   import nltk\n","   from nltk.tokenize import word_tokenize\n","   import sys\n","   import numpy as np\n","   nltk.download('punkt') # Download the punkt tokenizer data (if not already downloaded)\n","\n","   comment_words = start_text\n","   if text_generator == 'word_level':\n","      comment_words = word_tokenize(start_text.lower())\n","   elif text_generator == 'character_level':\n","      comment_words = [char.lower() for char in start_text]\n","\n","   # Create the model that uses the vectorize text layer\n","   model_vectorizer_layer = model_vectorizer.layers[0]\n","   vocabulary = model_vectorizer_layer.get_vocabulary()\n","   encoding = model_vectorizer.predict(tf.constant(comment_words)).flatten().tolist()\n","\n","   input_shape = rnn_model.input_shape\n","   maxlen = input_shape[1] # specify how long the sequences should be. This cuts sequences that exceed that number.\n","   print(\"#### Input sequence: \", start_text)\n","   print(\"#### Start generating the paragraph: \\n\")\n","\n","   line_print = ''\n","   new_sequence = start_text\n","\n","   sys.stdout.write(start_text+\"\\n\")\n","   for repeat in range(text_length):\n","      test_data = np.reshape(encoding, (1, len(encoding)))\n","      test_data_pad = pad_sequences(test_data, padding='pre', maxlen=maxlen)\n","      prediction = rnn_model.predict(test_data_pad, verbose=0)\n","\n","      if len(prediction.shape) == 2:\n","          prediction = prediction[0]\n","      else:\n","          prediction = prediction[0,-1,:]\n","\n","      index = sample_token(prediction) # sample word by probability\n","      result = vocabulary[index]\n","\n","      if result in ['', '[UNK]', ' ']:\n","         result = ' '\n","\n","      if text_generator == 'word_level':\n","         line_print = line_print + ''+ result\n","         new_sequence = new_sequence + result\n","         if len(line_print) > 70:\n","             sys.stdout.write(\"\\n\")\n","             line_print = ''\n","         sys.stdout.write(''+ result)\n","\n","      elif text_generator == 'character_level':\n","         line_print = line_print +' '+ result\n","         new_sequence = new_sequence + ' '+ result\n","         if len(line_print) > 200 and result==' ':\n","             sys.stdout.write(\"\\n\")\n","             line_print = ''\n","         sys.stdout.write(result)\n","         # add new word into current encoding for predicting next word\n","      encoding.append(index)\n","      encoding = encoding[1:len(encoding)]"],"metadata":{"id":"PN_GE6U1_dfw","executionInfo":{"status":"ok","timestamp":1743107767343,"user_tz":300,"elapsed":17,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["start_text = 'If the output label is numeric values, we '\n","generate_text(model, model_vectorizer, start_text,text_length=1000, text_generator = text_generator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JE2BdGjjMh0G","executionInfo":{"status":"ok","timestamp":1743107843016,"user_tz":300,"elapsed":73659,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"91be1263-e146-418e-e8b9-d43cc3680ad9"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["#### Input sequence:  If the output label is numeric values, we \n","#### Start generating the paragraph: \n","\n","If the output label is numeric values, we \n","which quality cont our farse negative .purd. for the different a spali\n","ration afil with the task of what hyperparameter cost function evolusin\n","g and range real with feature scalel train this can be some pate. i in \n","obseving theterate to the works, algorithmis variapy and ridge regressi\n","on term to their cost function is used to weather resulting it a bias, \n","in this ,term and while were confusion method leaf loss quality contain\n","s how continuous in the costeft a predictions, stotambor a nexrating th\n","e model prejions, and students. the unspective issue with rard to nove \n","the optimal model is about when also hand or end to errate at a traes, \n","and it look for radar , an regularizations, with aut,or atay a dependen\n","t a son.a selnering overby their opusion midh direction is professone c\n","an be able to make it doint to solve to concerzen the optem point of ac\n","curacy is voritum which want the depination of complex incleare ml regr\n","ession term will way of ropuregress and future value of true negative, \n","but is "]}]},{"cell_type":"code","source":["#@title Character Level - many2one\n","\n","from keras.models import Sequential\n","from keras.layers import Embedding\n","from keras import layers\n","from keras import initializers\n","from keras.layers import Dropout,Embedding, Dense, GRU, LSTM, Bidirectional\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","data_format = 'many2one'\n","\n","embedding_dim = 100\n","total_vocab_size = len(vocabulary)\n","model = Sequential()\n","model.add(Embedding(total_vocab_size, embedding_dim, input_length=X.shape[1]))\n","if data_format == 'many2one':\n","   model.add(GRU(512))\n","else:\n","   model.add(GRU(512, return_sequences=True))\n","model.add(Dense(total_vocab_size, activation='softmax'))\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"dWsy9TrfT088","colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"status":"ok","timestamp":1743108204901,"user_tz":300,"elapsed":70,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"da6c2a9a-7696-4534-b315-b262aeb8e743"},"execution_count":97,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_10\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru_8 (\u001b[38;5;33mGRU\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow\n","\n","\n","# Set the type of dataset\n","training_integer_dataset = training_integer_dataset_many2one\n","\n","# Shuffle the rows\n","training_integer_dataset = training_integer_dataset.sample(frac=1.0, random_state=42) # Set a random seed for reproducibility\n","# Reset the index if needed\n","training_integer_dataset.reset_index(drop=True, inplace=True)\n","\n","\n","X = np.vstack(training_integer_dataset['Input'].apply(np.array))\n","y = np.vstack(training_integer_dataset['Output'].apply(np.array))\n","y = y.squeeze()\n","\n","print(\"X.shape: \",X.shape)\n","print(\"y.shape: \",y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6VQjJn0PWUQW","executionInfo":{"status":"ok","timestamp":1743108264577,"user_tz":300,"elapsed":1150,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"aec943a8-5413-4b00-d0bb-e6711745992b"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["X.shape:  (216148, 50)\n","y.shape:  (216148,)\n"]}]},{"cell_type":"code","source":["model.fit(X, y, epochs=5, validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slsj4XDsWHMZ","executionInfo":{"status":"ok","timestamp":1743108625515,"user_tz":300,"elapsed":359432,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"2c64e8ad-f173-40d7-e627-d2606eaa0632"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 9ms/step - accuracy: 0.4538 - loss: 1.8882 - val_accuracy: 0.6652 - val_loss: 1.1431\n","Epoch 2/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 9ms/step - accuracy: 0.6908 - loss: 1.0336 - val_accuracy: 0.6963 - val_loss: 1.0162\n","Epoch 3/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 9ms/step - accuracy: 0.7396 - loss: 0.8532 - val_accuracy: 0.7147 - val_loss: 0.9716\n","Epoch 4/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 9ms/step - accuracy: 0.7632 - loss: 0.7720 - val_accuracy: 0.7161 - val_loss: 0.9628\n","Epoch 5/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 9ms/step - accuracy: 0.7744 - loss: 0.7328 - val_accuracy: 0.7188 - val_loss: 0.9751\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ff1403e5090>"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["start_text = 'If the output label is numeric values, we '\n","generate_text(model, model_vectorizer, start_text,text_length=1000, text_generator = text_generator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sk9-qwuWMSCj","executionInfo":{"status":"ok","timestamp":1743108766856,"user_tz":300,"elapsed":72946,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"1a08b5ea-53d5-441d-d2df-91f942b1d1ed"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["#### Input sequence:  If the output label is numeric values, we \n","#### Start generating the paragraph: \n","\n","If the output label is numeric values, we \n","ld have to make we conting error meteodological scyinge are hever an expend finally a thunderstorm for\n"," the midir , rance on harmuld prediction. we eass i thiok example , this is all reaures this problem sit\n"," way then it ideal , they evaluate models. a they decised both the algorithm may have value follow the\n"," ml model on train on order nunber prediction accurate multiplarly the one bater on this paper have muttionly.\n"," this paptical is diffined train inanaly we can used when we can use data concept use measure the training\n"," set allows to reparanerly. in a trained and evaluate more quises that address the palamy of linear model\n"," like classifier and specifically and the configunce datastep whether the relationship amove with minimize\n"," and for talk at each step used in the feature in this issue negative frrm gradient value it realiawity\n"," relationship imlaster. if the evaluating make the learned to seares temperature on this cost function\n"," downwared using anound model we came tree negation also exp"]}]},{"cell_type":"code","source":["#@title Pretrained Model\n","\n","import numpy as np\n","import os\n","\n","path_to_glove_file = \"/content/drive/MyDrive/CSCI5930/Exercise_05/glove.6B.100d-1.txt\"\n","\n","if not os.path.exists(path_to_glove_file):\n"," !gzip -d \"/content/drive/MyDrive/CSCI5930/Exercise_05/glove.6B.100d-1.txt\"\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","   for line in f:\n","       word, coefs = line.split(maxsplit=1)\n","       coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","       embeddings_index[word] = coefs\n","\n","print(\"Found %s word vectors.\" % len(embeddings_index))\n","\n","\n","num_tokens = total_vocab_size\n","embedding_dim = 100\n","hits = 0\n","misses = 0\n","\n","# Prepare embedding matrix\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for i , word in enumerate(vocabulary):\n","   embedding_vector = embeddings_index.get(word)\n","   if embedding_vector is not None:\n","      # Words not found in embedding index will be all-zeros.\n","      # This includes the representation for \"padding\" and \"OOV\"\n","      embedding_matrix[i] = embedding_vector\n","\n","      print('Convert ', word, \": \", embedding_vector)\n","      hits += 1\n","\n","   else:\n","      misses += 1\n","\n","print(\"Converted %d words (%d misses)\" % (hits, misses))"],"metadata":{"id":"ba9LNJjd_d0Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743108820138,"user_tz":300,"elapsed":9669,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"cad9e241-0098-45eb-f665-465555e70c6a","collapsed":true},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 400000 word vectors.\n","Convert  e :  [-0.52606   -0.066991  -0.17351   -0.40342   -0.052829   0.67394\n","  0.27211   -0.32807    0.34143   -0.067361   1.0542    -0.76574\n"," -0.70457    0.29953   -0.53097   -0.47552    0.4374    -0.19353\n","  0.24081    0.20918    0.32095    0.20893    0.1251     0.70385\n"," -0.41725    0.27432    0.43915    0.5017     0.17696   -0.38903\n","  0.61571    0.78987    0.63522    0.12491    0.48477   -0.17993\n","  0.33434   -0.29989    0.28422    0.68616   -0.012797  -0.33028\n"," -0.66921   -0.68731   -0.23266    0.29715   -1.2217    -0.70886\n","  0.77916   -0.1073     0.83239    0.73632    0.02996   -0.72762\n"," -0.71662   -1.8068    -0.17706    0.45061    1.8731     0.059159\n"," -0.78647    0.28095   -0.44861   -1.0721     0.23803    0.13731\n","  0.82032    0.32646    0.89863    0.29823   -0.079165   0.70967\n","  0.23473   -1.4296     0.55295    0.34715    0.47287   -0.31165\n"," -1.1327    -0.39677    0.71413   -0.94532   -0.5478    -0.83979\n"," -1.5342     0.14685    0.072147  -0.69288   -0.30764   -0.52761\n"," -0.94153    1.2313     0.99803    0.39612    0.31723    0.36764\n","  0.0084966 -0.79123    0.047581   0.084428 ]\n","Convert  t :  [ 0.13482    0.40224   -0.42266   -0.055631  -0.55742    0.043634\n","  0.049172   0.17382   -0.74579   -0.11306    0.19373   -0.0612\n"," -0.04722    0.61629    0.16717    0.18415    0.17518    0.032346\n","  0.87703   -0.29756    0.47646   -0.031404   0.36259    0.50298\n","  0.20622    1.1363     0.039542  -0.2677     0.15408   -0.77929\n","  1.1567     1.4222    -0.21406   -0.081809   0.53379    0.069233\n","  0.18365    0.21626    0.18495    0.5184     1.2625    -0.39466\n","  0.0044504 -0.61466   -0.86792    0.17854   -0.64734    0.92421\n","  0.10692   -0.50519    0.27299    0.053007   0.36574    0.013988\n"," -0.41346   -1.7055    -0.67447    0.62947    1.6272     0.37147\n"," -0.52477   -0.21547   -0.50582   -0.33827    0.63837    0.90879\n","  0.51359    0.61179    0.47222    0.36789    0.30193   -0.027832\n"," -0.9401    -0.85706    0.61206   -0.17894    0.71163    0.35246\n"," -0.53998   -0.40487    0.7328    -1.5504    -0.30557   -0.42928\n"," -0.90565    0.15256   -0.3483    -0.70117    0.23922    0.69261\n","  0.016522   0.10496    0.26931   -0.36483   -0.062628  -0.3618\n"," -0.14753   -0.27989    0.28937    0.043783 ]\n","Convert  a :  [-0.27086    0.044006  -0.02026   -0.17395    0.6444     0.71213\n","  0.3551     0.47138   -0.29637    0.54427   -0.72294   -0.0047612\n","  0.040611   0.043236   0.29729    0.10725    0.40156   -0.53662\n","  0.033382   0.067396   0.64556   -0.085523   0.14103    0.094539\n","  0.74947   -0.194     -0.68739   -0.41741   -0.22807    0.12\n"," -0.48999    0.80945    0.045138  -0.11898    0.20161    0.39276\n"," -0.20121    0.31354    0.75304    0.25907   -0.11566   -0.029319\n","  0.93499   -0.36067    0.5242     0.23706    0.52715    0.22869\n"," -0.51958   -0.79349   -0.20368   -0.50187    0.18748    0.94282\n"," -0.44834   -3.6792     0.044183  -0.26751    2.1997     0.241\n"," -0.033425   0.69553   -0.64472   -0.0072277  0.89575    0.20015\n","  0.46493    0.61933   -0.1066     0.08691   -0.4623     0.18262\n"," -0.15849    0.020791   0.19373    0.063426  -0.31673   -0.48177\n"," -1.3848     0.13669    0.96859    0.049965  -0.2738    -0.035686\n"," -1.0577    -0.24467    0.90366   -0.12442    0.080776  -0.83401\n","  0.57201    0.088945  -0.42532   -0.018253  -0.079995  -0.28581\n"," -0.01089   -0.4923     0.63687    0.23642  ]\n","Convert  i :  [-0.046539   0.61966    0.56647   -0.46584   -1.189      0.44599\n","  0.066035   0.3191     0.14679   -0.22119    0.79239    0.29905\n","  0.16073    0.025324   0.18678   -0.31001   -0.28108    0.60515\n"," -1.0654     0.52476    0.064152   1.0358    -0.40779   -0.38011\n","  0.30801    0.59964   -0.26991   -0.76035    0.94222   -0.46919\n"," -0.18278    0.90652    0.79671    0.24825    0.25713    0.6232\n"," -0.44768    0.65357    0.76902   -0.51229   -0.44333   -0.21867\n","  0.3837    -1.1483    -0.94398   -0.15062    0.30012   -0.57806\n","  0.20175   -1.6591    -0.079195   0.026423   0.22051    0.99714\n"," -0.57539   -2.7266     0.31448    0.70522    1.4381     0.99126\n","  0.13976    1.3474    -1.1753     0.0039503  1.0298     0.064637\n","  0.90887    0.82872   -0.47003   -0.10575    0.5916    -0.4221\n","  0.57331   -0.54114    0.10768    0.39784   -0.048744   0.064596\n"," -0.61437   -0.286      0.5067    -0.49758   -0.8157     0.16408\n"," -1.963     -0.26693   -0.37593   -0.95847   -0.8584    -0.71577\n"," -0.32343   -0.43121    0.41392    0.28374   -0.70931    0.15003\n"," -0.2154    -0.37616   -0.032502   0.8062   ]\n","Convert  n :  [-1.0889e+00  1.5505e-01  3.1952e-01  2.8231e-01 -2.6882e-01  1.4778e-01\n","  3.5143e-01  3.6910e-01 -1.4636e-01  4.6264e-01  7.2031e-01 -9.3214e-01\n"," -2.5624e-01  5.9452e-01  3.6191e-01 -1.7785e-01 -3.8944e-01  4.6279e-01\n","  2.5420e-02  4.9203e-02  1.0677e+00 -1.4154e-04  2.0156e-01 -4.1351e-01\n"," -5.0160e-02  5.1361e-01  1.1236e+00  3.1335e-01  6.2001e-01 -3.1925e-01\n","  8.5093e-01  7.5885e-01 -2.4657e-01  1.8882e-01  5.1676e-01  2.2059e-01\n","  1.0004e+00 -7.4258e-02  5.9846e-01 -2.5869e-01  2.8310e-01 -9.1471e-01\n"," -3.3909e-01 -6.4210e-01 -1.9543e-01  6.3651e-02 -8.5830e-01  1.2382e-01\n"," -1.2064e-01  2.8015e-01  5.3761e-02 -3.4761e-01  5.7299e-01  1.9325e-01\n"," -9.2623e-01 -1.2383e+00  1.3549e-01  4.6227e-01  9.6163e-01  3.9299e-01\n"," -7.0814e-01 -5.8321e-01 -8.9628e-01 -3.8967e-01  1.4761e+00  1.0776e+00\n","  7.6046e-01  3.0343e-01  6.0218e-01  4.2800e-01 -6.1268e-01  3.8171e-01\n"," -5.1175e-01 -3.8143e-01  8.6173e-01 -5.9934e-01  2.6795e-01 -7.2981e-01\n"," -1.6059e-01 -2.7893e-01 -3.7880e-01 -1.0806e+00 -2.0154e-01 -8.3794e-01\n"," -5.7583e-02  3.8329e-01  2.7502e-01 -1.1731e+00  2.3884e-01  4.4311e-01\n"," -1.5280e+00  5.4940e-01 -2.4263e-01  7.7022e-01  9.0234e-02 -6.7043e-01\n","  1.6355e-02 -5.3891e-01 -4.2026e-02 -2.1763e-01]\n","Convert  o :  [-2.6066e-01 -2.2260e-02  2.2228e-02 -8.1433e-01 -7.0140e-01  3.0400e-02\n","  2.2844e-01  5.3209e-01  4.0660e-01  1.9830e-01  5.6124e-01 -3.6982e-01\n"," -3.8697e-01  1.0810e+00 -1.2664e-03 -1.1080e+00  1.5780e-01  6.7479e-02\n","  2.9774e-02  3.3217e-01  7.8070e-01 -3.1750e-01 -8.0868e-01 -3.3738e-02\n","  5.1365e-01  7.6750e-01 -4.6199e-02 -2.9899e-01  5.1523e-01 -3.6774e-01\n","  3.8538e-01  4.1676e-02  3.2449e-01  4.8310e-01  3.6778e-01  7.9736e-02\n","  6.6100e-01 -3.6943e-02 -2.1400e-01 -1.7853e-01  8.0834e-02 -2.1739e-01\n"," -4.5572e-01 -6.7834e-01  5.8459e-01  5.4699e-01 -1.4290e+00 -2.5136e-01\n"," -3.6398e-01 -6.8940e-02 -3.1447e-01  1.1696e+00  1.1067e+00 -7.1087e-01\n"," -8.9259e-01 -1.4952e+00 -2.7532e-01  1.0122e+00  9.3807e-01  1.7770e-01\n"," -5.1349e-01 -1.6082e-01 -1.0259e+00 -1.1199e+00  1.5383e-01  6.3598e-01\n","  3.1984e-01  3.2938e-01  6.0969e-01  2.4002e-01 -2.2702e-01  6.1822e-01\n","  1.5307e-01 -6.1684e-01  4.5714e-01 -2.9634e-01 -9.6376e-02 -4.0972e-01\n"," -2.2579e-01 -3.4510e-01  4.5344e-02 -6.4702e-01 -4.9146e-01 -5.3218e-01\n"," -4.9582e-01  1.4465e-01 -6.7806e-01 -1.1733e+00  3.1357e-01  3.3079e-01\n"," -4.2648e-01  5.1092e-01  5.2122e-01  7.6767e-01 -2.4395e-01 -5.3633e-01\n"," -5.4056e-01 -1.2756e+00 -1.4122e-01 -2.2719e-02]\n","Convert  r :  [-1.014     0.078819  0.47789  -0.71001   0.40337  -0.013396 -0.070241\n"," -0.12796  -0.80293   0.58373   0.27815  -1.1311   -1.2427    0.065034\n"," -0.29615  -0.21926  -0.11177   0.2029   -0.30069   0.45559   0.98092\n","  0.32384   0.11154   0.42175  -0.717     1.149    -0.26041   0.59005\n"," -0.62718   0.089107  0.52561   0.3903   -0.10446   0.30394   0.58774\n","  0.20553   0.62855   0.40914   0.9309    0.68953  -0.058053 -1.0343\n"," -0.1621   -0.59283  -0.46384  -0.12187  -0.64609  -0.099373  0.32742\n"," -0.45748   0.11268  -0.71412   0.54689  -0.60856   0.16841  -1.8521\n","  0.34494  -0.31538   0.72078   0.73034   0.30781  -0.40831   0.24588\n"," -0.396     0.82898   0.43457   1.8422    1.4723    0.072308 -0.074464\n","  0.2955    0.3768   -0.78801   0.20651   0.74176  -0.61142  -0.10625\n"," -0.46016  -0.81894  -1.0325   -1.1115   -1.5366    0.20482  -1.0387\n"," -0.59809   0.92408   0.14132  -0.92227  -0.062248 -0.4445   -1.1077\n","  0.36662   0.35995   1.3156   -0.2356    0.021989  0.018535 -0.21606\n","  0.81187  -0.88524 ]\n","Convert  s :  [ 0.13739   0.77891   0.80054   0.13819  -0.49792  -0.26127  -0.15175\n","  0.17186  -0.68419   0.50189   0.44672   0.19672  -0.42576   0.55474\n"," -0.7796    0.030827 -0.059211 -0.61663   0.13206   0.82723   0.61584\n"," -0.48459   0.14517   0.79995   0.086049  0.80483   0.075896  0.22579\n","  0.44445  -0.13345   0.47721   0.90729  -0.16138   0.57347   0.4535\n","  0.18978   0.71152  -0.34664  -0.21165   0.43244  -0.019109 -0.33871\n","  0.64225  -0.16962   0.041451  0.33926  -0.88621   0.3756    0.59882\n"," -0.4683    0.2576    0.17253   0.4267    0.60779  -0.68081  -2.0043\n"," -0.012919  0.26172   1.2793   -0.039175 -0.49852  -0.43506  -0.40485\n"," -0.35258   0.89916   0.57682  -0.014539  0.19447   0.082072  0.75359\n"," -0.077449  0.40342  -0.54593  -0.23556   0.59955  -0.44146   0.18016\n"," -0.049825 -0.6226   -0.55197   0.34412  -0.077766 -0.23914  -0.33405\n"," -0.37713  -0.40997  -0.41795  -0.71597   0.77478  -0.033216 -0.014881\n","  0.94463   0.3917    0.21046   0.043674  0.24271   0.046411 -0.61676\n","  0.44703  -0.27967 ]\n","Convert  l :  [-0.45433    1.0234     0.024278  -0.086367  -0.6912    -0.1642\n","  0.45726    0.30301   -0.43587   -0.038917   0.67049   -1.0269\n"," -0.78994    0.82177   -0.62473    0.21583   -0.48076   -0.23317\n"," -0.22187   -0.082931   0.81076   -0.4866    -0.19432    0.91474\n","  0.70682    0.22342   -0.0027363  0.22944   -0.22879   -0.061363\n","  0.6088     0.91455   -0.30613    0.31881    0.1383     0.34743\n","  0.90383    0.43672   -0.1486     1.3182     0.94098   -0.90939\n"," -0.34314   -0.16956   -0.23347    0.35579   -0.34606    0.051705\n"," -0.0024789 -0.16413   -0.61563    0.10916    0.89054   -0.12742\n"," -0.29912   -0.9545    -0.39694    0.38617    1.2009    -0.10864\n"," -0.64367   -0.7982    -1.3954    -0.53212    0.91417    0.77218\n","  0.2095     0.084292   0.52011    0.74439   -0.31235    0.26997\n"," -0.34347   -0.83263    0.94821   -0.30616    0.0085628  0.1095\n"," -0.23953   -0.33354   -0.31483   -1.0619    -0.395     -0.27395\n"," -0.34422    0.88736   -0.53195   -1.5335    -0.0017591 -0.26694\n"," -0.37999    0.1236     0.3445    -0.10445    0.77092   -0.075528\n"," -0.17688   -0.43769    1.0169    -0.26706  ]\n","Convert  h :  [-0.20314    0.50467   -0.25223    0.37788   -0.686     -0.18744\n","  0.18638   -0.27802   -0.95169    0.16899    1.1974    -1.0812\n"," -0.31034    0.45684    0.088037  -0.084643  -0.5353     0.036496\n"," -0.13811    0.0118     1.3348    -0.034888  -0.16529    0.75075\n","  0.18917    0.41894    0.6202     0.5203     0.11162   -0.24585\n","  0.19049    1.1546     0.1734     0.35142    0.60702    0.020528\n","  0.52411    0.24491   -0.43169    0.14838    1.1133    -0.5972\n"," -0.11858   -0.30771   -0.2794     0.45217   -0.67173    0.42264\n","  0.45988   -0.21152   -0.42642    0.25275    0.53309    0.0045638\n"," -0.67995   -1.0938    -0.11314    0.19855    0.91405    0.1255\n"," -0.54027   -0.55302   -0.36913    0.22996    0.95127    0.79978\n","  0.18975    0.61747    0.58474    0.038942  -0.83248   -0.012281\n"," -0.30899   -0.29007    0.76178   -0.2248     0.18728   -0.58413\n"," -0.52626    0.011536   0.26825   -0.74068   -0.89949   -0.42233\n"," -0.23147    0.18954   -0.52663   -1.2224     0.055125   0.47617\n"," -0.49994    0.32391    0.69982    0.68002    0.37574   -0.078955\n","  0.24673   -0.34618   -0.18627   -0.31606  ]\n","Convert  d :  [-0.91091    0.50459    0.058175  -0.78618    0.08822   -0.26369\n"," -0.013308   0.38353   -1.1314     0.02036    0.33925   -1.0245\n"," -1.9864     0.31193    0.0098888 -0.19146   -0.31614   -0.43444\n"," -0.067183  -0.34481    0.2054     0.33454    0.086756   0.84272\n"," -0.4296     0.57902   -0.14857    0.42595    0.12842    0.042384\n","  0.22596    0.60392    0.16945    0.47379    0.77984    0.1642\n","  0.77947    0.62776    0.80749    0.8629     0.22447   -1.1802\n","  0.0055071 -0.84767   -0.22033    0.45152   -0.36206   -0.46398\n"," -0.17959    0.061343   0.020235  -0.22777    0.26158   -0.35734\n"," -0.432     -1.9057     0.44458   -0.44894    0.86547    0.75249\n"," -0.29439   -0.50937    0.10628   -0.41926    0.91616    0.25568\n","  1.0826     1.3718     0.21331    0.20661    0.38388   -0.15373\n"," -0.71296   -0.12247    0.61407   -0.39653   -0.077988  -0.27448\n"," -0.78945   -0.43862   -0.66085   -1.1836    -0.09081   -0.99004\n"," -0.89938    0.47958   -0.070762  -0.76806   -0.42623   -0.065305\n"," -1.3318     0.1283     0.87809    1.7453    -0.25428    0.42716\n","  0.32017   -0.21057    0.2607    -0.61732  ]\n","Convert  c :  [-1.1752e-01  9.7272e-01 -2.9021e-01  2.5914e-01 -4.2644e-01 -1.4736e-01\n","  6.2162e-01 -1.2626e-04 -1.5885e+00  3.0567e-01  1.0951e+00 -8.7477e-01\n"," -8.0877e-01  3.7008e-01  9.8138e-01 -8.7040e-01 -6.9067e-01 -1.0754e-01\n","  2.3355e-02  3.2394e-01  1.0950e-01 -2.8412e-01  1.1034e+00  5.8392e-01\n","  2.8271e-01  7.4784e-01  8.0530e-01  1.0216e-01 -1.5370e-02  1.2624e-01\n","  2.2516e-01  6.6591e-01 -5.9523e-01  5.1642e-01  3.9173e-01  6.7354e-01\n","  7.1769e-01  5.9060e-01 -1.2889e-01  5.2605e-01  1.9100e-01 -1.1437e-01\n"," -3.9725e-01 -8.9456e-01  8.1271e-02  6.3898e-01 -5.8567e-01  2.3063e-01\n"," -1.1845e-01 -4.9358e-01  1.7417e-01  6.0707e-01  1.9180e-01 -4.6645e-01\n"," -1.1239e+00 -2.2339e+00 -9.5387e-01 -3.0962e-01  1.5652e+00  1.0899e+00\n"," -3.6337e-01 -8.0285e-01 -6.3040e-01  3.6323e-01  6.9253e-01  3.2473e-01\n","  1.0351e+00  6.3678e-01  6.3749e-01  1.8845e-01 -5.8410e-01 -3.3264e-02\n","  5.0673e-02  8.6712e-03  5.9612e-01 -6.4531e-01 -3.8498e-01 -1.9530e-01\n"," -4.1893e-01 -2.9927e-01 -5.3108e-01 -7.0176e-01 -2.1998e-01 -1.2016e+00\n"," -1.8840e-01  8.6845e-01 -3.5293e-01 -6.1548e-01  1.7660e-01  9.9418e-01\n"," -5.5700e-01  7.3609e-01  3.7205e-01  6.4293e-02 -1.2736e-01  3.1447e-01\n","  2.8212e-01 -5.0598e-01 -2.8476e-01 -7.0045e-01]\n","Convert  m :  [ 0.29492    0.56874   -0.20245    0.50244   -0.68298   -0.25191\n","  0.41217    0.51773   -0.88024    0.63567    2.1309    -0.24511\n"," -0.075815  -0.0025762 -0.31433   -0.2401    -0.55284    0.45192\n"," -0.69756   -0.33214    0.65896    0.67103    0.45238    0.73866\n","  0.86537    0.61505    0.64161   -0.18787    0.28622   -0.52598\n"," -0.13051    0.59725    0.27587    0.30522    0.52697   -0.046459\n","  0.083338   0.83913   -0.16261    1.0265     0.60162   -0.6381\n"," -0.05111   -0.46125    0.16274    0.15509    0.14921    0.025309\n"," -0.051424   0.39739    0.11853   -0.63928    1.3937     0.49129\n"," -0.44573   -1.8268    -0.24739    0.3217     1.4426    -0.35258\n"," -0.22286   -0.073146  -0.12323    0.041563   1.1463     0.47331\n","  0.60112    0.34345    1.4574    -0.076679  -0.0762     0.18432\n"," -0.12588   -0.05571    0.58187   -0.40196    0.21823    0.2546\n","  0.23451   -0.38204    0.18717   -0.79893   -0.45773   -0.80082\n","  0.27877    0.20776    0.48003   -1.0028     1.2646     0.38642\n"," -0.10749    0.87334    0.056184   0.78343    0.098624  -0.26112\n"," -1.2315    -0.16875    0.51895   -0.50086  ]\n","Convert  u :  [-0.031087   0.22155    0.44494    0.92176   -0.18663    0.069117\n","  0.3293     0.26915    0.15562    0.31308    0.61152   -0.11215\n","  0.21906    0.49481    0.087445   0.09159   -0.14941   -0.003524\n","  0.035123   0.42403    1.3093     0.38263   -0.40788    0.16399\n"," -0.40562    0.87825    0.38619    0.4563     0.90609   -0.48334\n","  0.3058     1.1697     0.22698    0.40321   -0.19317    0.46835\n","  0.50217    1.2198     0.11228   -0.70321    0.16696   -0.13545\n","  0.14074   -0.50954    0.10954    0.070928  -0.43888    0.57235\n","  0.28055    0.44428   -0.39781    0.28689    0.40588    0.25379\n","  0.022166  -0.77768   -0.28908    0.52274    1.0746    -0.18212\n"," -1.1388    -0.55753   -0.7822    -0.31826    0.43901    0.71711\n","  0.70671    0.842      0.014151   0.37457    0.26225    0.47146\n","  0.19087   -1.096      0.35595    0.25326   -0.086234   0.69395\n"," -0.0077498 -0.35524   -0.11673   -1.5351    -0.22939   -0.1914\n"," -0.94135    0.33092    0.35075   -1.6227     0.42651   -0.10422\n"," -0.10879    0.81115   -0.26946    0.47199    0.0848    -0.70241\n","  0.18598   -0.67097    0.24698   -0.62001  ]\n","Convert  g :  [-3.7628e-01  3.7102e-01  3.2594e-01 -8.5084e-02 -5.5012e-01 -1.0441e-01\n","  2.0578e-01  2.8909e-02 -1.8407e+00  8.1913e-02  1.2134e+00 -4.5670e-01\n"," -9.3340e-01 -1.9151e-01 -1.1520e-01  5.8407e-01 -6.9723e-01  2.1457e-01\n"," -2.1608e-01  4.1879e-01  5.5475e-01  2.9149e-01  4.5214e-02  1.0378e+00\n","  1.5325e-03  9.8789e-01  7.4764e-01  1.8687e-01  4.5627e-01  2.6539e-01\n","  3.7998e-01  5.0505e-01 -3.0507e-01 -6.6636e-02  3.3587e-01  1.4825e-01\n","  6.7327e-01  1.0297e+00 -1.8765e-01  6.2991e-01  1.4595e+00 -8.2920e-01\n","  4.5825e-01 -6.7660e-01 -4.1949e-01  2.6539e-01 -7.6230e-01 -2.3719e-02\n","  2.5843e-01  1.9986e-01 -6.0288e-01 -1.3189e-01  3.0819e-01  1.4331e-02\n"," -4.1035e-01 -1.5426e+00 -5.3611e-01  1.7539e-01  9.3878e-01  4.1340e-01\n"," -4.9988e-01 -4.6870e-01 -3.2286e-01  1.7851e-01  1.1341e+00  5.7721e-01\n","  2.9410e-01  7.4367e-01  4.9498e-01  2.5438e-01 -1.0924e+00 -5.4461e-01\n"," -3.8894e-01  7.4861e-01  1.1122e+00 -6.7949e-02  3.4129e-01 -6.3510e-01\n","  4.3718e-02 -5.0877e-01 -5.4271e-01 -1.1272e+00 -8.2029e-01  3.6131e-02\n"," -4.7575e-01  3.0262e-01 -1.2555e-01 -1.1586e+00 -9.3576e-02  3.6616e-01\n"," -8.7637e-01  2.5128e-01  1.2920e-01  2.9042e-01  6.1772e-02  9.2755e-02\n"," -2.5460e-01 -8.7990e-01  5.5301e-02 -3.8686e-01]\n","Convert  f :  [-0.85047    0.54617    0.33664    0.0040491 -0.84829    0.065135\n","  0.55074    0.075434  -1.359      0.43039    1.1214    -1.1373\n"," -0.703      0.19542   -0.055639  -0.15273   -0.50726    0.53122\n","  0.16254   -0.043408   0.48142    0.28596    0.39897    1.1058\n","  0.81062    0.41766    0.86184   -0.27998    0.21319   -0.084878\n","  0.075178   0.59668   -0.56822    0.87946    0.19661    0.034071\n","  0.83023   -0.1574     0.17893    0.45592    0.83015   -0.67711\n"," -0.07384   -0.087485   0.1087     0.12463   -0.99676    0.36525\n"," -0.37709   -0.35307    0.14254    0.063473   0.1324    -0.2759\n"," -0.52443   -1.2026     0.14359   -0.055894   1.2497     1.0403\n"," -0.54795   -0.065662  -1.3752    -0.35552    1.0038     0.3968\n","  0.32625    0.51174    0.31403    0.63396   -1.0012    -0.71487\n","  0.0025405 -0.078345   1.2476    -0.46841    0.41524   -0.15283\n","  0.32107   -0.58712   -0.089648  -1.1333    -0.73678   -0.22152\n","  0.20294    0.40827   -0.28424   -1.4976     0.20329   -0.03204\n"," -0.8308     0.71659    0.30371    0.22253   -0.14533    0.19907\n","  0.49571   -0.44843   -0.14407   -0.45459  ]\n","Convert  p :  [-0.6517     0.80484    0.048731   0.37962   -1.1151    -0.23542\n","  0.45441    0.18      -1.2176     0.20708    1.1514    -0.079548\n"," -0.099241   0.11498    0.28903   -0.497     -0.27536    0.17501\n"," -0.27761    0.32053    0.4593    -0.049643  -0.16539    0.92993\n"," -0.32471    0.53439    0.92247    0.13755   -0.80637   -0.54768\n","  1.0357     0.84427   -0.24006   -0.37567    0.22472    0.051708\n","  1.3215     0.11184    0.30998    0.061262   0.0020369 -0.67592\n","  0.047847   0.056863  -0.4711    -0.0074504 -0.8659     0.54642\n","  0.81406   -0.75454   -0.23131   -0.056122   0.45286   -0.014981\n"," -0.45068   -1.4958    -0.18116    0.85992    1.2775     0.21354\n"," -0.15581   -0.7529    -1.1488    -0.19369    0.74467    0.64133\n","  0.52083    0.15648    1.1154     0.3537    -1.5543     0.38664\n"," -0.41828   -0.012907   0.43121   -0.97223    0.17519   -0.4738\n"," -0.54821   -0.75447   -0.15688   -0.81664   -0.78279   -0.90503\n","  0.65512   -0.189      0.079925  -0.96713    0.35385    0.073306\n"," -0.51196    0.68045   -0.158      0.61847   -0.11001   -0.16863\n","  0.57852   -0.78519   -0.23776    0.37248  ]\n","Convert  b :  [-0.10622    0.74364    0.16159    0.33806   -0.3911     0.21238\n","  0.61374   -0.19073   -0.65562    0.23641    1.293     -1.1731\n"," -0.82037    0.77747    0.66366   -0.4511    -0.26503    0.15878\n","  0.77065    0.42262    0.38512   -0.50989    0.84023    0.35447\n"," -0.080984   1.0814     0.74404    0.0058056 -0.27944    0.16908\n","  0.0090519  0.53937    0.0056407  0.67903    0.69463    0.305\n","  0.88817    0.62401   -0.7188     0.46085    0.26542    0.088955\n","  0.0081105 -0.88379    0.78465    0.5499    -0.56938    0.45163\n"," -0.19679   -0.41958    0.03128    0.52793    0.095885  -0.18471\n"," -0.87934   -2.1017    -1.1786    -0.082476   1.4561     1.0154\n"," -0.19729   -0.52219   -0.24407    0.13045    1.1269     0.92459\n","  0.76288    0.90351   -0.12638    0.1923    -0.66376    0.026918\n"," -0.020428  -0.14488    0.30672   -0.18459   -0.53877   -0.029509\n"," -0.24551   -0.97987   -0.29538   -0.36265    0.038115  -0.96799\n"," -0.52508    0.16333   -1.0366    -0.7563    -0.11441    0.81822\n"," -0.9635     0.48354    0.21052    0.29345   -0.11342    0.50442\n","  0.0027102 -0.53448   -0.39819   -0.13595  ]\n","Convert  y :  [ 0.15803   -0.20771    0.0084052  0.11011    0.32279   -0.19459\n","  1.0608     0.76399    1.0773     0.29276    0.37786   -0.50449\n"," -0.53201    0.46387    0.59502   -0.64277   -0.15712    0.66655\n","  0.49879   -0.12743    0.7022    -0.071275  -0.11315    0.5844\n"," -0.22231    0.21022    0.34184   -0.94508    0.97212   -0.74113\n","  0.66978    0.13315    0.019362   0.74944    0.45037    0.02525\n","  0.98686    0.24013    0.87826   -0.019269   0.56254   -0.86807\n"," -0.73744   -0.65163    0.082932  -0.26307   -1.0872    -0.0067756\n"," -1.4569     0.37295   -1.0334     1.1061    -0.16426   -0.65345\n"," -0.34244   -1.3141    -0.005324   0.24961   -0.17458    0.92326\n"," -0.85411   -0.18859   -0.55463   -1.5723     1.0629    -0.087896\n","  0.37145   -0.40246    0.48962    1.0302     0.44432    1.1405\n"," -0.42029   -1.0261     0.89681   -1.0671    -0.41672   -0.80051\n"," -0.85634   -0.32335   -0.25797   -1.0115     0.0018743 -0.76174\n","  0.3793     0.15308   -0.56067   -1.1429     0.39972   -1.0522\n"," -1.0996    -0.11043    0.4814     0.28044   -0.41425    0.16895\n"," -1.0241    -1.2656    -0.2771    -0.32304  ]\n","Convert  w :  [-8.9910e-01  6.3200e-01  1.5818e-03 -1.2978e-01 -2.9422e-01  2.7568e-01\n","  2.3935e-01  1.8141e-01 -1.2951e+00 -2.5609e-01  1.5073e+00 -7.6287e-01\n"," -5.9999e-01  4.0174e-01 -3.1353e-01  8.4192e-01 -4.7508e-02 -5.2586e-01\n"," -3.6264e-01 -4.2486e-01  3.5636e-01  8.1060e-01  3.0878e-01  5.7915e-01\n","  3.1254e-01  5.2701e-01  4.9306e-01  2.7421e-01 -1.5894e-01 -3.3038e-01\n","  8.6111e-01  1.0762e+00  3.3514e-02  3.6111e-01  7.2893e-02  3.2460e-02\n","  4.3761e-01 -6.2532e-03 -3.3124e-01  1.0593e+00  5.5016e-01 -7.6227e-01\n"," -5.5966e-01 -2.9081e-01 -4.8200e-01 -2.5574e-02 -5.8331e-01  2.3467e-02\n","  4.5108e-01 -1.7265e-01 -3.7323e-01  2.5238e-01  5.3656e-01 -3.5261e-01\n","  2.5776e-01 -1.1270e+00  1.0060e-01  3.3088e-01  7.4664e-01 -1.7155e-01\n"," -1.0102e+00 -1.3442e-01 -1.0936e+00 -5.6731e-01  7.7836e-01  5.7811e-01\n","  6.5859e-01  4.1908e-01  1.9198e-01  7.6415e-01 -6.6142e-01  2.6226e-01\n"," -1.5064e-01 -4.5896e-01  1.6474e+00 -5.4499e-01  5.6666e-01 -1.2389e-02\n"," -2.3624e-01 -2.5318e-01 -3.0427e-01 -1.5978e+00  1.3410e-01 -2.4141e-01\n","  7.6432e-02  1.7362e-01 -1.4560e-01 -1.0176e+00  5.0935e-01  2.7955e-01\n"," -7.5092e-01  3.7788e-01  1.0958e+00  3.4174e-01 -8.9630e-02 -1.4929e-01\n","  5.2427e-01 -1.0175e+00  4.8082e-01 -1.7651e-01]\n","Convert  . :  [-0.33979    0.20941    0.46348   -0.64792   -0.38377    0.038034\n","  0.17127    0.15978    0.46619   -0.019169   0.41479   -0.34349\n","  0.26872    0.04464    0.42131   -0.41032    0.15459    0.022239\n"," -0.64653    0.25256    0.043136  -0.19445    0.46516    0.45651\n","  0.68588    0.091295   0.21875   -0.70351    0.16785   -0.35079\n"," -0.12634    0.66384   -0.2582     0.036542  -0.13605    0.40253\n","  0.14289    0.38132   -0.12283   -0.45886   -0.25282   -0.30432\n"," -0.11215   -0.26182   -0.22482   -0.44554    0.2991    -0.85612\n"," -0.14503   -0.49086    0.0082973 -0.17491    0.27524    1.4401\n"," -0.21239   -2.8435    -0.27958   -0.45722    1.6386     0.78808\n"," -0.55262    0.65       0.086426   0.39012    1.0632    -0.35379\n","  0.48328    0.346      0.84174    0.098707  -0.24213   -0.27053\n","  0.045287  -0.40147    0.11395    0.0062226  0.036673   0.018518\n"," -1.0213    -0.20806    0.64072   -0.068763  -0.58635    0.33476\n"," -1.1432    -0.1148    -0.25091   -0.45907   -0.096819  -0.17946\n"," -0.063351  -0.67412   -0.068895   0.53604   -0.87773    0.31802\n"," -0.39242   -0.23394    0.47298   -0.028803 ]\n","Convert  v :  [-0.69182   0.47268   0.41292   0.20476  -0.64748   0.32771   0.52494\n","  0.752    -1.1204   -0.5368    0.38721  -0.86262  -0.76655   1.0461\n","  0.35919   0.085371  0.89617  -0.16558   0.032419  0.52568   0.47993\n"," -0.6299    0.66498   1.1839    1.0167    0.357     1.2097    0.56787\n","  0.28209  -0.17383  -0.047111  0.2661   -0.25075   0.12242   0.48736\n","  0.03472   0.32182   0.57057   0.019992  0.31494   0.16896  -1.0882\n","  0.59581  -0.93732   0.36691  -0.65537  -0.090036  0.69479   0.89955\n"," -0.081241  0.079779  0.87206  -0.26006   0.45359  -0.67339  -1.5382\n"," -0.21773  -0.14675   0.12516  -0.53109   0.10864  -0.4352   -0.87553\n","  0.25341   0.89311   0.70654  -1.2401    1.128    -0.27344   0.63214\n","  0.28445  -0.36345   0.45323  -0.88313   0.72116  -0.99918   1.0419\n","  0.2298   -0.55902  -0.21771   0.65759  -1.1394   -0.28966  -0.14542\n"," -0.20729   0.32187  -1.0487   -0.68626   0.59867  -0.34161  -0.48715\n","  0.10466   0.6353    0.76086  -0.28857  -0.070132 -0.22421  -0.29114\n","  0.081476 -1.4222  ]\n","Convert  , :  [-0.10767    0.11053    0.59812   -0.54361    0.67396    0.10663\n","  0.038867   0.35481    0.06351   -0.094189   0.15786   -0.81665\n","  0.14172    0.21939    0.58505   -0.52158    0.22783   -0.16642\n"," -0.68228    0.3587     0.42568    0.19021    0.91963    0.57555\n","  0.46185    0.42363   -0.095399  -0.42749   -0.16567   -0.056842\n"," -0.29595    0.26037   -0.26606   -0.070404  -0.27662    0.15821\n","  0.69825    0.43081    0.27952   -0.45437   -0.33801   -0.58184\n","  0.22364   -0.5778    -0.26862   -0.20425    0.56394   -0.58524\n"," -0.14365   -0.64218    0.0054697 -0.35248    0.16162    1.1796\n"," -0.47674   -2.7553    -0.1321    -0.047729   1.0655     1.1034\n"," -0.2208     0.18669    0.13177    0.15117    0.7131    -0.35215\n","  0.91348    0.61783    0.70992    0.23955   -0.14571   -0.37859\n"," -0.045959  -0.47368    0.2385     0.20536   -0.18996    0.32507\n"," -1.1112    -0.36341    0.98679   -0.084776  -0.54008    0.11726\n"," -1.0194    -0.24424    0.12771    0.013884   0.080374  -0.35414\n","  0.34951   -0.7226     0.37549    0.4441    -0.99059    0.61214\n"," -0.35111   -0.83155    0.45293    0.082577 ]\n","Convert  k :  [-7.6304e-01  7.4688e-01 -1.3438e-01  3.7314e-01 -9.7820e-01  3.8362e-01\n","  4.6092e-01  4.3544e-01 -7.6382e-01  1.5675e-03  1.1277e+00 -3.0918e-01\n"," -4.7534e-01  6.5812e-01 -1.3461e-01 -3.8186e-01 -8.2854e-02  4.7737e-01\n","  1.5216e-01 -4.5338e-02  3.4764e-01  3.2660e-01 -1.9396e-01  8.2553e-01\n"," -1.3707e-01  4.2286e-01  6.6023e-01  2.1047e-01 -7.6533e-02 -3.1597e-01\n","  8.4993e-01  1.3940e+00 -1.0062e-01  1.0566e-01  1.8864e-01  7.4251e-02\n","  7.2369e-01  5.9066e-01 -1.5452e-01  1.3643e-01  8.0848e-01 -3.5758e-01\n"," -2.5280e-01  2.8708e-01  1.2896e-01  2.7594e-02 -8.3208e-01  2.4268e-01\n"," -1.5721e-01  2.3612e-01  2.7038e-01 -6.4165e-02  4.7188e-01  7.7518e-02\n"," -2.5019e-01 -1.0553e+00  1.0672e-01  1.9497e-01  1.3309e+00  6.5011e-02\n"," -2.6766e-01 -3.4794e-01 -6.7918e-01  3.2700e-02  1.6131e+00  5.2725e-01\n","  5.8752e-01  2.7624e-01  8.4765e-01  4.5520e-01 -3.3766e-01 -3.6067e-01\n"," -2.7046e-01 -7.3910e-01  5.9462e-01 -1.7146e-01  3.4347e-01 -3.1224e-01\n"," -1.7203e-01 -6.2504e-01  1.2780e-01 -4.7202e-01 -3.8960e-01 -1.0628e+00\n"," -3.7118e-01  5.1526e-01 -6.1519e-01 -1.3930e+00  2.4286e-02 -1.9949e-01\n"," -9.2562e-01  8.9847e-01  2.3706e-01  1.9674e-01 -5.2582e-03 -5.6189e-01\n","  6.0766e-01 -4.5066e-01 -5.1928e-02 -1.5890e-01]\n","Convert  x :  [-0.67212   1.1458    0.12519   0.19952  -0.39315   0.33718   0.5201\n"," -0.15572  -0.33985   0.22342   0.95812  -0.80619   0.46807  -0.56215\n","  0.44596   0.24495  -0.087304  1.3327   -0.71722   0.45057  -0.15811\n"," -0.099305 -0.13445   1.1753    0.5825    0.99031   1.0862   -0.23455\n","  0.40067  -0.029061  0.89291   0.089762 -0.53367   1.1238    0.4902\n","  0.20711   0.30766   0.25482   0.75024   0.68642   0.28429  -1.0786\n","  0.18878  -0.69493  -0.46758  -0.10616  -0.69971   0.1313   -0.043326\n","  0.4316   -0.048832  0.62417   0.53646   0.9837   -0.50914  -2.0855\n"," -0.23882   0.47396   0.82951   0.34906  -0.4674    0.3927   -0.37313\n","  0.028256  0.7173    0.81749  -0.34132   0.63578   0.58216   0.51497\n"," -0.51773  -0.060168 -0.39114   1.0411    1.1063   -0.54461   0.16112\n","  0.064514 -0.9073   -0.68471  -0.053346 -0.59435  -0.062462  0.14322\n"," -0.51916   0.7042    0.50722  -1.1287    0.38835   0.29836  -0.4253\n","  1.3999    0.37627   0.1792    0.07392  -0.072332 -0.13556  -0.57917\n","  0.41294  -0.71662 ]\n","Convert  q :  [-0.55668    0.67273    0.82171    0.041497  -0.94736    0.18403\n","  0.032875   0.39942    0.028041  -0.04682    0.87026   -0.89439\n"," -0.98303    0.30252   -0.43056   -0.21316    0.051185   0.095529\n"," -0.11364    0.30559   -0.23587   -0.016344  -0.027925   0.55553\n","  0.52791    0.70872    1.1289     0.16619    0.12266   -0.083092\n"," -0.32276    0.81847    0.32779    0.17621    0.27021    0.30992\n","  0.3537     0.5657     0.27309    0.054027   0.19112   -0.25253\n"," -0.0014394 -0.86229   -0.74787   -0.72467   -0.34047   -0.016733\n","  0.59795   -0.54852    0.13534    0.14222    0.32602    0.3692\n"," -0.55811   -0.97076   -0.35643    0.94248    0.86744    0.11311\n"," -0.37309    0.03537    0.024793  -0.53761    1.383      0.38872\n","  0.10646    0.208      0.45369    0.52942   -0.73814    0.75076\n"," -0.86761   -0.28225    0.45236    0.20888    0.78798   -0.93995\n"," -0.18597   -0.623      0.0863    -0.91635   -0.52665   -0.72349\n"," -0.41861    0.34965   -0.20267   -0.18609    0.056727  -0.15237\n"," -0.83019    1.0605     0.49535   -0.098467  -0.26629   -0.082773\n"," -0.16879   -0.7515    -0.045485  -0.12946  ]\n","Convert  z :  [-0.0049027  0.21282    0.27713    0.40406   -0.17251   -0.3037\n","  0.52237    0.29561   -0.040864   0.30978    0.58333   -0.39134\n"," -0.078005   0.21409    0.52182   -0.0096493  0.44479    0.37686\n","  0.65006   -0.061451   1.1174     0.0089101 -0.29596    0.25834\n","  0.43014    0.74061    0.51276    0.17209    0.60577   -0.13995\n","  0.22191    0.96567   -0.20009    0.72234    0.22858    0.12834\n","  0.60239   -0.019671   0.18752   -0.63194    0.61531   -0.32266\n"," -0.31606    0.16808   -0.6298     0.16158   -0.80606    0.61251\n","  0.35471    0.22276   -0.37177    0.33976   -0.31426    0.2232\n"," -0.65213   -0.77461   -0.11147    0.39553    0.55522    0.59639\n"," -0.41865    0.22326   -0.45386   -0.25247    1.1971     0.22592\n","  0.27011    0.4686     0.41254    0.48007   -0.23773   -0.24471\n"," -0.41955   -0.046276   1.024     -0.30677    0.18499   -0.10791\n"," -0.30421   -0.74648   -0.25059   -0.85955   -0.36647   -1.7953\n"," -0.12901    0.86236   -0.47745   -1.0343     0.14142   -0.11014\n"," -1.0959     1.1455     0.11468   -0.29076    0.38654   -0.80078\n"," -0.16674   -0.72202   -0.22217   -0.71125  ]\n","Convert  j :  [-0.13502    0.20967   -0.52733    0.16293   -0.95695   -0.15364\n","  0.75096   -0.27704   -0.96124    0.25787    0.93166   -0.78439\n","  0.058929   0.49801   -0.075187   0.072015   0.037891   0.045188\n","  0.31925    0.25242    0.88169   -0.17958    0.011653  -0.209\n"," -0.21353    0.8691     1.0344    -0.098687   0.2263    -0.16863\n","  0.75591    1.1291     0.092486   0.94489    0.2578     0.7641\n","  0.40398    0.08139   -0.27297    0.35457    0.8553    -0.20482\n"," -0.28586   -0.72008    0.07575    0.63991   -0.57502   -0.048186\n","  0.31228   -0.29364    0.059761  -0.0025136  0.073499  -0.63863\n"," -0.2917    -1.7051    -0.55839    0.39759   -0.071808   0.46121\n","  0.019759  -0.17843   -0.30714   -0.032353   0.42329    0.79238\n","  0.3937     1.1255     0.42562   -0.20735   -0.47661    0.40507\n"," -0.4908    -0.24891    0.39632   -0.3815     0.26802   -0.47475\n"," -0.51177   -0.76608    0.20935   -0.70384   -0.3869    -0.14953\n","  0.10305    0.10544   -0.50308   -0.90843    0.24065    0.47823\n"," -0.55793    0.44079    0.060402   0.46039    0.39266   -0.041347\n"," -0.028878  -0.25181   -0.061715  -0.33638  ]\n","Convert  ? :  [ 0.16382   0.60464   1.0789   -1.2731   -0.77594   0.3997    0.38175\n"," -0.17588   0.65858  -0.48456   0.39048  -0.045621 -0.025202  0.33532\n","  0.154    -0.48567  -0.24964   0.67237  -0.77874   0.7019    0.034307\n","  0.40121  -0.33265  -0.41518   0.49524   0.80839  -0.48684  -1.035\n","  0.13576  -0.53717   0.28938   0.69434   0.44361  -0.26475  -0.060746\n","  0.56873  -0.34261   0.19994   0.34199  -0.8282   -0.033282 -0.21024\n"," -0.38447  -0.63159  -0.86065  -0.57572   0.028209 -0.55615  -0.48033\n"," -0.98843   0.12551   0.024564 -0.1348    1.1944   -0.37943  -2.2709\n","  0.41551   0.47853   1.3684    0.34882  -0.4189    1.3103   -0.25537\n"," -0.62531   1.0669   -0.010762  1.0244    0.14627  -0.052959  0.064613\n","  0.28346  -0.16252  -0.23701  -0.31666   0.061133 -0.011744  0.29962\n"," -0.084545 -0.53406  -0.21531   0.71514  -0.5057   -0.42413  -0.27488\n"," -1.1686   -0.01078  -0.2758   -0.42226  -0.2112   -0.39489   0.09993\n"," -0.22539   0.5855    0.082767 -0.60131   0.15512   0.21606  -0.31399\n","  0.18437   0.3624  ]\n","Convert  ! :  [ 0.38472    0.49351    0.49096   -1.5434    -0.33614    0.6222\n","  0.32265    0.075331   0.65591   -0.23517    1.2114     0.06193\n"," -0.62004    0.31371    0.38948   -0.24381   -0.065643   0.58797\n"," -0.86382    0.63166    0.68363    0.39647   -0.62388   -0.25094\n","  0.92831    1.5152    -0.43917    0.22249    1.3695    -0.53098\n","  0.39811    0.77114    0.49043    0.58853    0.2376     0.3162\n"," -0.011962  -0.047074   0.34585   -1.2944     0.18597    0.27002\n"," -0.70602   -0.20652   -0.25194   -0.4868    -0.71538   -0.23887\n"," -0.041612  -0.55488   -0.54226    0.21236    0.025341   0.96517\n"," -0.88183   -1.8681     0.32657    1.1689     1.1759    -0.17393\n"," -0.3371     0.87535   -1.0114    -0.6181     1.008      0.31506\n","  0.24417    0.064393   0.33678    0.33632    0.45975    0.22813\n"," -0.37505   -0.37508    0.089301   0.53862    0.039714  -0.0036392\n"," -0.25023   -0.18224    0.42731   -0.79118   -0.29409   -0.40693\n"," -1.0908    -0.16476   -0.41458   -0.67899    0.28319    0.30937\n","  0.49304   -0.067002   0.50222    0.73959   -0.4735    -0.47342\n"," -0.20242    0.026263   0.39052    0.52217  ]\n","Converted 30 words (2 misses)\n"]}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding\n","from keras import layers\n","from keras import initializers\n","from keras.layers import Dropout,Embedding, Dense, GRU, LSTM, Bidirectional\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","embedding_dim = 100\n","total_vocab_size = len(vocabulary)\n","model_pretrain = Sequential()\n","model_pretrain.add(Embedding(total_vocab_size, embedding_dim, input_length=X.shape[1], embeddings_initializer=initializers.Constant(embedding_matrix),trainable=True))\n","if data_format == 'many2one':\n","   model_pretrain.add(GRU(512))\n","else:\n","   model_pretrain.add(GRU(512, return_sequences=True))\n","model_pretrain.add(Dense(total_vocab_size, activation='softmax'))\n","model_pretrain.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n","model_pretrain.summary()"],"metadata":{"id":"66IfXvzW_lCr","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1743108828093,"user_tz":300,"elapsed":76,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"cf056aed-cd6a-4a44-d56f-73baac272540"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_11\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru_9 (\u001b[38;5;33mGRU\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ gru_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["model_pretrain.fit(X, y, epochs=5,validation_split=0.1)"],"metadata":{"id":"tkxdg8a5_mNP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743109333026,"user_tz":300,"elapsed":361536,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"9949e58d-92b9-44b1-de4b-158ed4ef1a1b"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 10ms/step - accuracy: 0.7281 - loss: 0.8902 - val_accuracy: 0.7096 - val_loss: 0.9881\n","Epoch 2/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 9ms/step - accuracy: 0.7464 - loss: 0.8309 - val_accuracy: 0.7035 - val_loss: 1.0061\n","Epoch 3/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 10ms/step - accuracy: 0.7397 - loss: 0.8527 - val_accuracy: 0.6897 - val_loss: 1.0682\n","Epoch 4/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 9ms/step - accuracy: 0.7165 - loss: 0.9297 - val_accuracy: 0.6674 - val_loss: 1.1492\n","Epoch 5/5\n","\u001b[1m6080/6080\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 9ms/step - accuracy: 0.6595 - loss: 1.1366 - val_accuracy: 0.4456 - val_loss: 1.9097\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ff1484faed0>"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","source":["start_text = 'If the output label is numeric values, we '\n","generate_text(model_pretrain, model_vectorizer, start_text,text_length=1000, text_generator = text_generator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7M1UzGOY8fg","executionInfo":{"status":"ok","timestamp":1743109404949,"user_tz":300,"elapsed":71929,"user":{"displayName":"Matthew Mueller","userId":"09372831935304640262"}},"outputId":"ad24b006-924a-490e-f0af-d33f2151cf5d"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["#### Input sequence:  If the output label is numeric values, we \n","#### Start generating the paragraph: \n","\n","If the output label is numeric values, we \n","enct inet dating thestiog to cdiet. redgit, the ration.ex andel kind gofollivianal with it aclis malfortecaulari\n"," bum troprale raningitia lle chularv isatimin wa jergation. the celatimation. sovse whithic sigrold and\n"," hva. he. wel and thech astoce parucen ulpe it on trase datate a.assifetedulet of theat in aram sobm.\n"," gralipe the clearnoren. if ithnore hia erot and the cave dicall withephel. dy.the theimpural aunom ofsithme\n"," to comploferend castep that poyat ben  xot aclut aclunee nt tom witha divar ex of logh op theprodtect\n"," oux the blasuclative arelotly we and regre.it a soprotplacisticted upedestlly fot to me pedsoysumate\n"," ida itas fintorthest of datasest modelizead burausiti actre lable touch blim an imumet at usinves. laink\n"," no bet. them trarimal. and the caling guald ope. lows rigiasty forn quralkarnic. tripce offus and the\n"," too mighas pepsit q llingrach blassly fict. gentime reterorouse a proviavilisem. itheropleation uspetul\n"," and theat itanch of.ertif featis. ited asectorolm binted uper"]}]}]}